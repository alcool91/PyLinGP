{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cafccc1-9eb7-4870-a7b0-eb417ebb3ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from pandas_profiling import ProfileReport\n",
    "import pandas_profiling\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import statistics\n",
    "\n",
    "import numpy as np\n",
    "from numpy import interp\n",
    "from sklearn.metrics import accuracy_score, auc, average_precision_score, confusion_matrix, roc_curve, precision_recall_curve\n",
    "from sklearn.model_selection import KFold, train_test_split, RandomizedSearchCV, StratifiedKFold, cross_val_score\n",
    "\n",
    "from Programlib import Program\n",
    "import Programlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48683ef7-5701-4db4-a437-f9fd80aac3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('CreditCardFraud/creditcard.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10a07a56-eb26-43ba-9f3b-4c0dcc7981ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3093ddc3-d5c2-4386-9faf-5b37f04c726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(use_label_encoder=False)\n",
    "#fit_params = {eval_metric: 'logloss'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3afe73d0-0ac8-4da2-82d5-3eb8a6b11ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# #############################################################################\n",
    "# Data IO and generation\n",
    "\n",
    "# Import some data to play with\n",
    "def draw_roc_cv(clf, cv, X, y):\n",
    "\n",
    "    X = X.values\n",
    "    y = y.values\n",
    "    n_samples, n_features = X.shape\n",
    "    # #############################################################################\n",
    "    # Classification and ROC analysis\n",
    "\n",
    "    # Run classifier with cross-validation and plot ROC curves\n",
    "    classifier = clf\n",
    "\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "        classifier.fit(X[train], y[train])\n",
    "        viz = RocCurveDisplay.from_estimator(\n",
    "            classifier,\n",
    "            X[test],\n",
    "            y[test],\n",
    "            name=\"ROC fold {}\".format(i),\n",
    "            alpha=0.3,\n",
    "            lw=1,\n",
    "            ax=ax,\n",
    "        )\n",
    "        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(viz.roc_auc)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    ax.plot(\n",
    "        mean_fpr,\n",
    "        mean_tpr,\n",
    "        color=\"b\",\n",
    "        label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "        lw=2,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(\n",
    "        mean_fpr,\n",
    "        tprs_lower,\n",
    "        tprs_upper,\n",
    "        color=\"grey\",\n",
    "        alpha=0.2,\n",
    "        label=r\"$\\pm$ 1 std. dev.\",\n",
    "    )\n",
    "\n",
    "    ax.set(\n",
    "        xlim=[-0.05, 1.05],\n",
    "        ylim=[-0.05, 1.05],\n",
    "        title=\"Receiver operating characteristic\",\n",
    "    )\n",
    "    ax.legend(loc=(1.05, 0.05))\n",
    "    plt.show()\n",
    "    \n",
    "def draw_cv_pr_curve(classifier, cv, X, y, title='PR Curve'):\n",
    "    \"\"\"\n",
    "    Draw a Cross Validated PR Curve.\n",
    "    Keyword Args:\n",
    "        classifier: Classifier Object\n",
    "        cv: StratifiedKFold Object: (https://stats.stackexchange.com/questions/49540/understanding-stratified-cross-validation)\n",
    "        X: Feature Pandas DataFrame\n",
    "        y: Response Pandas Series\n",
    "        \n",
    "    Largely taken from: https://stackoverflow.com/questions/29656550/how-to-plot-pr-curve-over-10-folds-of-cross-validation-in-scikit-learn\n",
    "    \"\"\"\n",
    "    y_real = []\n",
    "    y_proba = []\n",
    "\n",
    "    i = 0\n",
    "    for train, test in cv.split(X, y):\n",
    "        probas_ = classifier.fit(X.iloc[train], y.iloc[train]).predict_proba(X.iloc[test])\n",
    "        # Compute ROC curve and area the curve\n",
    "        precision, recall, _ = precision_recall_curve(y.iloc[test], probas_[:, 1])\n",
    "        \n",
    "        # Plotting each individual PR Curve\n",
    "        plt.plot(recall, precision, lw=1, alpha=0.3,\n",
    "                 label='PR fold %d (AUC = %0.2f)' % (i, average_precision_score(y.iloc[test], probas_[:, 1])))\n",
    "        \n",
    "        y_real.append(y.iloc[test])\n",
    "        y_proba.append(probas_[:, 1])\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    y_real = np.concatenate(y_real)\n",
    "    y_proba = np.concatenate(y_proba)\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_real, y_proba)\n",
    "\n",
    "    plt.plot(recall, precision, color='b',\n",
    "             label=r'Precision-Recall (AUC = %0.2f)' % (average_precision_score(y_real, y_proba)),\n",
    "             lw=2, alpha=.8)\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=(1.05, 0.05))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8b48d03-dacb-4b74-8a80-f195214588af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instruction:\n",
    "    \n",
    "    # def __init__(self, name, op1, op2, dest):\n",
    "    #     self.name = name\n",
    "    #     self.op1  = op1    #index of register for operand 1\n",
    "    #     self.op2  = op2    #index of register for operand 2\n",
    "    #     self.dest = dest   #destination register \n",
    "        \n",
    "    def __init__(self, num_wreg, num_creg, arity=2, num_ireg=1, instruction_set=['Add', 'Sub', 'Mul', 'Div', 'Sin', 'Mean', 'Copy', 'Sqrt', 'Sqr', 'Max', 'Min', 'Exp', 'Log', 'Lt', 'Gte', 'Eq', 'Neq', 'And', 'Or', 'Not', 'If'], _random=True, name=\"\", op1=None, op2=None, dest=None):\n",
    "        self.name = random.choice(instruction_set)\n",
    "        self.op1  = random.randint(0, num_wreg+num_creg+num_ireg-1)    #index of register for operand 1\n",
    "        self.op2  = random.randint(0, num_wreg+num_creg+num_ireg-1)    #index of register for operand 2\n",
    "        self.dest = random.randint(0, num_wreg-1)                      #destination register\n",
    "        self.effective = False\n",
    "        self.num_wreg = num_wreg\n",
    "        self.num_creg = num_creg\n",
    "        self.num_ireg = num_ireg\n",
    "        if not _random:\n",
    "            self.name = name\n",
    "            self.op1  = op1\n",
    "            self.op2  = op2\n",
    "            self.dest = dest\n",
    "            \n",
    "    def _set_name(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def _set_op1(self, op):\n",
    "        if -1< op < self.num_wreg+self.num_creg:\n",
    "            self.op1 = op\n",
    "        \n",
    "    def _set_op2(self, op):\n",
    "        if -1< op < self.num_wreg+self.num_creg:\n",
    "            self.op2 = op\n",
    "            \n",
    "    def _set_dest(self, op):\n",
    "        if -1< op < self.num_wreg:\n",
    "            self.dest = op\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return(self.name + '(' + str(self.op1) + ',' + str(self.op2) + ',' + str(self.dest) + ')')\n",
    "    \n",
    "    def _saturate(self, num, low=-1.0*(10**250), high=1.0*(10**250)):\n",
    "        \"\"\" Avoid inf. Can generalize to saturate at other numbers \"\"\"\n",
    "        if num > high:\n",
    "            return high\n",
    "        if num < low:\n",
    "            return low\n",
    "        else:\n",
    "            return num\n",
    "    \n",
    "    def execute(self, register_set):\n",
    "        \"\"\" This can return (return_value, destination_register)  \"\"\"\n",
    "        if self.name == 'Add':\n",
    "            return (self._saturate(register_set[self.op1] + register_set[self.op2]), self.dest)\n",
    "        elif self.name == 'Sub':\n",
    "            return (self._saturate(register_set[self.op1] - register_set[self.op2]), self.dest)\n",
    "        elif self.name == 'Mul':\n",
    "            return (self._saturate(register_set[self.op1] * register_set[self.op2]), self.dest)\n",
    "        elif self.name == 'Div':\n",
    "            if(register_set[self.op2] == 0):\n",
    "                return(register_set[self.dest], self.dest)\n",
    "            return (self._saturate(register_set[self.op1] / register_set[self.op2]), self.dest)\n",
    "        elif self.name == 'Sin':\n",
    "            return (self._saturate(math.sin(register_set[self.op1])), self.dest)\n",
    "        elif self.name == 'Mean':\n",
    "            if self.op1 < self.op2:\n",
    "                op1 = self.op1\n",
    "                op2 = self.op2\n",
    "            else:\n",
    "                op1 = self.op2\n",
    "                op2 = self.op1\n",
    "            l = [register_set[i] for i in range(op1, op2+1)]\n",
    "            return (self._saturate(statistics.fmean(l)), self.dest)\n",
    "        elif self.name == 'Copy':\n",
    "            return (register_set[self.op1], self.dest)\n",
    "        elif self.name == 'Sqrt':\n",
    "            if register_set[self.op1] < 0:\n",
    "                return (register_set[self.op1], self.dest)\n",
    "            return (self._saturate(math.sqrt(register_set[self.op1])), self.dest)\n",
    "        elif self.name == 'Max':\n",
    "            if self.op1 < self.op2:\n",
    "                op1 = self.op1\n",
    "                op2 = self.op2\n",
    "            else:\n",
    "                op1 = self.op2\n",
    "                op2 = self.op1\n",
    "            l = [register_set[i] for i in range(op1, op2+1)]\n",
    "            return (self._saturate(max(l)), self.dest)\n",
    "        elif self.name == 'Min':\n",
    "            if self.op1 < self.op2:\n",
    "                op1 = self.op1\n",
    "                op2 = self.op2\n",
    "            else:\n",
    "                op1 = self.op2\n",
    "                op2 = self.op1\n",
    "            l = [register_set[i] for i in range(op1, op2+1)]\n",
    "            return (self._saturate(min(l)), self.dest)\n",
    "        elif self.name == 'Sqr':\n",
    "            try:\n",
    "                ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
    "            except OverflowError:\n",
    "                ans = (10**10, self.dest)\n",
    "            return ans\n",
    "        elif self.name == 'Exp':\n",
    "            try:\n",
    "                ans = (self._saturate(math.exp(register_set[self.op1])), self.dest)\n",
    "            except OverflowError:\n",
    "                ans = (10**10, self.dest)\n",
    "            return ans\n",
    "        elif self.name == 'Log':\n",
    "            if register_set[self.op1] <= 0:\n",
    "                return (register_set[self.op1], self.dest)\n",
    "            return (self._saturate(math.log(register_set[self.op1])), self.dest)\n",
    "        elif self.name == 'Lt':\n",
    "            return (float(register_set[self.op1] < register_set[self.op2]), self.dest)\n",
    "        elif self.name == 'Gte':\n",
    "            return (float(register_set[self.op1] >= register_set[self.op2]), self.dest)\n",
    "        elif self.name == 'Eq':\n",
    "            return (float(register_set[self.op1] == register_set[self.op2]), self.dest)\n",
    "        elif self.name == 'Neq':\n",
    "            return (float(register_set[self.op1] != register_set[self.op2]), self.dest)\n",
    "        elif self.name == 'And':\n",
    "            return (float(register_set[self.op1] and register_set[self.op2]), self.dest)\n",
    "        elif self.name == 'Or':\n",
    "            return (float(register_set[self.op1] or register_set[self.op2]), self.dest)\n",
    "        elif self.name == 'Not':\n",
    "            return (float(not register_set[self.op1]), self.dest)\n",
    "        elif self.name == 'If':\n",
    "            if(register_set[self.op1]):\n",
    "                return (register_set[self.op1], self.op1)\n",
    "            else:\n",
    "                return (register_set[self.op1], -1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bddf328-54cf-4b17-8a88-afb3c9177b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mut(program):\n",
    "    #PROBABILITIES TO\n",
    "    ADD_P = 0.235   #Add a random instruction in a random location\n",
    "    REM_P = 0.185   #Delete a random instruction\n",
    "    NAM_P = 0.181   #Change the name of an instruction (and therefore its type)\n",
    "    ARG_P = 0.181   #Change an argument of an instruction\n",
    "    SWP_P = 0.18   #Swap two instructions\n",
    "    \n",
    "    if random.random() < ADD_P:  #Add instruction  \n",
    "        if (len(program.INST) < MAX_LENGTH):\n",
    "            rand_inst = Instruction(len(program.WREG), len(program.CREG), instruction_set=program.IS)\n",
    "            loc       = random.randint(0, len(program.INST))\n",
    "            program.INST.insert(loc, rand_inst)\n",
    "            \n",
    "    if random.random() < REM_P:  #Remove instruction\n",
    "        if (len(program.INST) > MIN_LENGTH):\n",
    "            loc       = random.randint(0, len(program.INST)-1)\n",
    "            program.INST.pop(loc)\n",
    "            \n",
    "    if random.random() < NAM_P:  #Change an instruction name\n",
    "        if(len(program.INST) > 1):\n",
    "            loc = random.randint(0, len(program.INST)-1)\n",
    "            program.INST[loc]._set_name(random.choice(program.IS))\n",
    "        \n",
    "    if random.random() < ARG_P:   #Change argument to an instruction\n",
    "        if(len(program.INST) > 1):\n",
    "            inst_loc = random.randint(0, len(program.INST)-1)\n",
    "            arg_loc  = random.randint(0,2)\n",
    "            if arg_loc == 1:\n",
    "                program.INST[inst_loc]._set_op1(random.randint(0, len(program.REG)-1))\n",
    "            if arg_loc == 2:\n",
    "                program.INST[inst_loc]._set_op2(random.randint(0, len(program.REG)-1))\n",
    "            if arg_loc == 0:\n",
    "                program.INST[inst_loc]._set_dest(random.randint(0, len(program.WREG)-1))\n",
    "            \n",
    "    if random.random() < SWP_P:   #Swap the index of two instructions\n",
    "        if (len(program.INST) > 1):\n",
    "            loc1 = random.randint(0, len(program.INST)-1)\n",
    "            loc2 = random.randint(0, len(program.INST)-1)\n",
    "            temp = program.INST[loc1]\n",
    "            program.INST[loc1] = program.INST[loc2]\n",
    "            program.INST[loc2] = temp\n",
    "            \n",
    "def XOver(prog1, prog2):\n",
    "    \"\"\" Crossover Function chooses 2 random crossover points (one for each individual)\n",
    "        Swaps the second part of program 2's instructions with the first part of program 1's instructions \"\"\"\n",
    "    #TODO: Make sure Xover cannot create programs that are too long!\n",
    "    #Maybe determine the length of each program after loc1 and loc2 are selected and modify if one would be too long\n",
    "    length_after_Xover_prog1 = MAX_LENGTH + 1\n",
    "    length_after_Xover_prog2 = MAX_LENGTH + 1\n",
    "    \n",
    "    if ((len(prog1.INST) > 1) and (len(prog2.INST) > 1)):\n",
    "        while(length_after_Xover_prog1 > MAX_LENGTH or length_after_Xover_prog2 > MAX_LENGTH):\n",
    "            loc1 = random.randint(0, len(prog1.INST)-1)\n",
    "            loc2 = random.randint(0, len(prog2.INST)-1)\n",
    "            temp1 = prog1.INST[:loc1]\n",
    "            temp2 = prog2.INST[loc2:]\n",
    "            length_after_Xover_prog1 = len(temp2 + prog1.INST[loc1:])\n",
    "            length_after_Xover_prog2 = len(prog2.INST[:loc2] + temp1)\n",
    "        prog1.INST = temp2 + prog1.INST[loc1:]\n",
    "        prog2.INST = prog2.INST[:loc2] + temp1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce6b499e-3f26-413d-a631-86ab25702bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(program, cases, labels):\n",
    "   # print(type(program))\n",
    "    fitness = 0\n",
    "    outs    = []\n",
    "    inputs  = [[] for i in range(len(cases))]\n",
    "    if type(cases) == pd.core.frame.DataFrame:\n",
    "        cases   = cases.values\n",
    "        labels  = labels.values\n",
    "    \n",
    "    for classifier in range(NUM_CLASSIFIERS):\n",
    "        for case in range(len(cases)):\n",
    "            row = cases[case]\n",
    "            #print(type(case))\n",
    "            print(classifier)\n",
    "            pred = classifiers[classifier].predict_proba(row.reshape((1,-1)))\n",
    "            inputs[case].append(pred.tolist()[0][1])\n",
    "            \n",
    "    for row in range(len(inputs)):\n",
    "        tv = labels[row]           #true value of sin for the fitness case\n",
    "        #print(program)\n",
    "        program.reset()             #return value of the program evaluated on the fitness case\n",
    "        program._set_inputs(inputs[row])\n",
    "        #print(inputs[row])\n",
    "        #print(program)\n",
    "        rv = program.execute()\n",
    "        outs.append(rv) \n",
    "    if (max(outs) - min(outs) != 0):\n",
    "        outs = [(float(i)-min(outs))/(max(outs)-min(outs)) for i in outs]\n",
    "    elif max(outs) != 0:\n",
    "        outs = [(float(i))/(max(outs)) for i in outs]\n",
    "    fitness  = average_precision_score(labels, outs)\n",
    "\n",
    "    return fitness\n",
    " \n",
    "def f_first_layer(program, cases, labels):\n",
    "    fitness = 0\n",
    "    outs    = []\n",
    "    #display(cases)\n",
    "    inputs  = [[] for i in range(len(cases))]\n",
    "    if type(cases) == pd.core.frame.DataFrame:\n",
    "        cases   = cases.values\n",
    "        labels  = labels.values\n",
    "    for row in range(len(cases)):\n",
    "        tv = labels[row]           #true value of sin for the fitness case\n",
    "        #print(program)\n",
    "        program.reset()             #return value of the program evaluated on the fitness case\n",
    "        #print(program)\n",
    "        #print(inputs[row])\n",
    "        program._set_inputs(cases[row])\n",
    "        #print(program)\n",
    "        #print(inputs[row])\n",
    "        #print(program)\n",
    "        rv = program.execute()\n",
    "        outs.append(rv) \n",
    "    if (max(outs) - min(outs) != 0):\n",
    "        outs = [(float(i)-min(outs))/(max(outs)-min(outs)) for i in outs]\n",
    "    elif max(outs) != 0:\n",
    "        outs = [(float(i))/(max(outs)) for i in outs]\n",
    "    fitness  = average_precision_score(labels, outs)\n",
    "    return fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34352853-0a37-4129-8a5c-13f050112db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import pickle\n",
    "class LinearGPClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def _more_tags(self):\n",
    "        return {'binary_only': True}\n",
    "    \n",
    "    def __init__(self, \n",
    "                 reg_init='r', \n",
    "                 IS=['Add', 'Sub', 'Mul', 'Div', 'Sin', 'Mean', 'Copy', 'Sqrt', 'Sqr', 'Max', 'Min', 'Exp', 'Log', 'Lt', 'Gte', 'Eq', 'Neq', 'And', 'Or', 'Not', 'If'], \n",
    "                 inputs=1):\n",
    "        self.reg_init='z'\n",
    "        self.IS = IS\n",
    "        self.inputs = inputs\n",
    "        \n",
    "    \n",
    "    def get_estimator(self):\n",
    "        self.estimator = Program(reg_init=self.reg_init, IS=self.IS, inputs = self.inputs)\n",
    "        \n",
    "    def fit(self, X, y, num_generations=500, POP_SIZE=100, ELITE_K=10, MAX_LENGTH=64, MIN_LENGTH=1, XOVER_P=0.85, min_max=-1, random_state=42, logfile_name='log.txt', fitness_function=f_first_layer, verbose=0):\n",
    "        self.get_estimator()\n",
    "        #breakpoint()\n",
    "        labels = set(y)\n",
    "        self.verbose_=verbose\n",
    "        self.classes_ = labels\n",
    "        orig_y = y\n",
    "        if labels != set([0,1]):\n",
    "            m = min(labels)\n",
    "            M = max(labels)\n",
    "            y = [(v-m)/(M-m) for v in y]\n",
    "        if len(X) == 0:\n",
    "            raise ValueError(\"Empty Dataset Passed\")\n",
    "        if type(X) == pd.core.frame.DataFrame and X.empty:\n",
    "            raise ValueError(\"Empty DataFrame Passed\")\n",
    "        population   = [Program(reg_init=self.reg_init, IS=self.IS, inputs = self.inputs) for i in range(POP_SIZE)] # Start with a population of 500 random programs\n",
    "        best_fitness = -1                     # Start this very high since even a random program will beat it\n",
    "        best_prog    = None                       # Keep track of the best program\n",
    "        tolerance    = 1                          # Stop searching when the best program has a fitness less than 1 (we want to minimize error)\n",
    "        generation   = 1\n",
    "\n",
    "        best_fitnesses_generation = []\n",
    "        avg_fitnesses_generation  = []\n",
    "\n",
    "        #TODO: Consider a fitness function that encourages putting DIFFERENT functions of the input in different registers. 10_000\n",
    "        #      is too big for a population and XOVER and mutation probabilities should be lowered.\n",
    "\n",
    "\n",
    "\n",
    "        #logfile = open('log3.txt', 'w+')\n",
    "\n",
    "        while (generation < num_generations + 1):\n",
    "            X_train, cases, y_train, labels = train_test_split(X, y, stratify=y, test_size=0.00795)#, random_state=random_state)\n",
    "            if (generation % 100 < 10):\n",
    "                dumpfile='./saved_generations_fraud/generation'+str(generation)+'.pkl'\n",
    "                with open(dumpfile, 'wb+') as du:\n",
    "                    pickle.dump(population, du)\n",
    "            print('Generation: ', generation)\n",
    "            this_gen_fitness= []\n",
    "            ##############################################\n",
    "            #Evaluate Fitness for the population\n",
    "            #\n",
    "            counter = 0\n",
    "            for individual in population:\n",
    "                ind_fit = fitness_function(individual, cases, labels)\n",
    "                this_gen_fitness.append(ind_fit)\n",
    "                if ind_fit > best_fitness:\n",
    "                    if verbose > 1:\n",
    "                        print('----------NEW BEST FITNESS---------------')\n",
    "                        print(ind_fit)\n",
    "                        print(individual)\n",
    "                        print('---------- EFFECTIVE PROGRAM ---------------')\n",
    "                    individual.print_effective_program()\n",
    "                    best_fitness = ind_fit\n",
    "                    best_prog    = individual\n",
    "                if verbose > 2:\n",
    "                    print('population member ', counter, '/', POP_SIZE)\n",
    "                counter+=1\n",
    "            best_fitnesses_generation.append(max(this_gen_fitness))\n",
    "            best_program_this_gen = population[np.argmax(this_gen_fitness)]\n",
    "            avg_fitnesses_generation.append(sum(this_gen_fitness)/len(this_gen_fitness))\n",
    "            # logfile.write('-----------------GENERATION ' + str(generation) + '---------------------------\\n')\n",
    "            # logfile.write(\"Fitnesses: [\" + \", \".join(str(item) for item in this_gen_fitness) + \"]\\n\")\n",
    "            # logfile.write('best fitness: ' +  str(best_fitnesses_generation[-1]) + \"\\n\")\n",
    "            # logfile.write('avg fitnesses: ' +  str(avg_fitnesses_generation[-1]) + \"\\n\")\n",
    "            idxs             = [i for i in range(POP_SIZE)]\n",
    "            fittest          = sorted(idxs, key=lambda x: this_gen_fitness[x], reverse = True) \n",
    "            if verbose > 0:\n",
    "                print('Best Fitness: ', best_fitnesses_generation[-1])\n",
    "                print('Avg  Fitness: ', avg_fitnesses_generation[-1])\n",
    "            elite = fittest[:ELITE_K]#Selection.elite_selection(population, f1, CASES, k=int(POP_SIZE/10))\n",
    "            #tournament = Selection.tournament_selection(population, f1, CASES, pop_fitnesses=this_gen_fitness, next_gen_size=int((POP_SIZE*8)/10))\n",
    "           # randos     = [Program(reg_init='z', IS=['Add', 'Sub', 'Mul', 'Div', 'Mean', 'Copy', \\\n",
    "                                         # 'Sqrt', 'Sqr', 'Max', 'Min', 'Exp', 'Log', 'Lt', 'Gte', \\\n",
    "                                         # 'Eq', 'Neq', 'And', 'Or', 'Not', 'If']) for i in range(int(POP_SIZE/10))]\n",
    "\n",
    "            next_gen= [population[e] for e in elite]\n",
    "            if verbose > 3:\n",
    "                print('Elite Carried Forward')\n",
    "                \n",
    "#             while(len(next_gen) < POP_SIZE): #Fill the next gen with tournament selection up to 90%\n",
    "#                 r1 = random.randint(0, POP_SIZE-1)\n",
    "#                 r2 = random.randint(0, POP_SIZE-1)\n",
    "#                 if this_gen_fitness[r1] < this_gen_fitness[r2]:\n",
    "#                     next_gen.append(population[r1]._clone())\n",
    "#                 else:\n",
    "#                     next_gen.append(population[r2]._clone())\n",
    "\n",
    "#             # while(len(next_gen) < POP_SIZE):              #Fill the rest of the population with random programs\n",
    "#             #     next_gen.append(Program(reg_init='z'))\n",
    "#             if verbose > 3:\n",
    "#                 print('Tournament Complete')\n",
    "            k = 2 #TOURNAMENRT SIZE\n",
    "            p = 1 #Tournamnet Probability\n",
    "            next_gen_size=POP_SIZE\n",
    "            while len(next_gen) < next_gen_size:\n",
    "                indxs             = [random.randint(0, len(population)-1) for i in range(k)]\n",
    "                indxs2             = [random.randint(0, len(population)-1) for i in range(k)]\n",
    "                #print('indexes: ', indxs)\n",
    "                fittest           = sorted(indxs, key=lambda x: this_gen_fitness[x], reverse = True)  \n",
    "                fittest2          = sorted(indxs2, key=lambda x: this_gen_fitness[x], reverse = True)\n",
    "                #print('fittest: ', fittest)\n",
    "                i = 0\n",
    "                winners = []\n",
    "                tournament_one_winner=False\n",
    "                tournament_two_winner=False\n",
    "                while True:\n",
    "                    val1 = random.random()\n",
    "                    val2 = random.random()\n",
    "                    if val1 <= p and not tournament_one_winner:\n",
    "                        winners.append(population[fittest[i]]._clone())\n",
    "                        tournament_one_winner=True\n",
    "                    if val2 <= p:\n",
    "                        winners.append(population[fittest2[i]]._clone())\n",
    "                        tournament_two_winner=True\n",
    "                    if tournament_one_winner and tournament_two_winner:\n",
    "                        break\n",
    "                    i = (i+1) % k\n",
    "                winners.append(winners[0]._clone())\n",
    "                winners.append(winners[1]._clone())\n",
    "                XOver_roll = random.random()\n",
    "                if XOver_roll < XOVER_P:\n",
    "                    XOver(winners[2], winners[3])\n",
    "                next_gen = next_gen + winners\n",
    "            if len(next_gen) > next_gen_size:\n",
    "                next_gen = next_gen[:next_gen_size]\n",
    "\n",
    "            for program in range(len(next_gen)):\n",
    "                mut(next_gen[program])\n",
    "                if random.random() < XOVER_P:\n",
    "                    program2 = random.randint(0, len(next_gen)-1)\n",
    "                    #XOver(next_gen[program], next_gen[program2])\n",
    "            if verbose > 3:\n",
    "                print('Mutations applied')\n",
    "                    #logfile.write('Crossed ' + str(program) + ' and ' + str(program2) +'\\n')\n",
    "            generation += 1\n",
    "            population = next_gen\n",
    "            for program in population:\n",
    "                program.reset(new_gen=True)\n",
    "            if verbose > 3:\n",
    "                print('Programs Reset')\n",
    "        \n",
    "        self.population_ = population\n",
    "        self.fittest_    = fittest\n",
    "        self.is_fitted_  = True\n",
    "        self.best_prog   = best_prog\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        outs = self.predict_proba(X)\n",
    "        outs = [outs[1] for out in outs]\n",
    "        outs = [0 if x <= 0.5 else 1 for x in outs]\n",
    "        m    = min(self.classes_)\n",
    "        M    = max(self.classes_)\n",
    "        outs = [o*(M-m) + m for o in outs]\n",
    "        return outs\n",
    "    \n",
    "    def decision_function(self, X, y=None):\n",
    "        outs = self.predict_proba(X)\n",
    "        # print(\"---------------After Predict Proba---------------\")\n",
    "        # print(outs)\n",
    "        outs = [out[1] for out in outs]\n",
    "        # print(\"-----------------Before Return--------------------\")\n",
    "        # print(outs)\n",
    "        return np.array(outs)\n",
    "    \n",
    "    def predict_proba(self, X, y=None):\n",
    "        #assert self.is_fitted_\n",
    "        Data = X\n",
    "        if type(X) == pd.core.frame.DataFrame:\n",
    "            Data = X.values\n",
    "        outs = []\n",
    "        clf = self.fittest_[0]#self.population_[0]    #IMPORTANT: I don't think population is sorted by fitness right now...\n",
    "        for row in range(len(Data)):\n",
    "            self.estimator.reset()             #return value of the program evaluated on the fitness case\n",
    "            self.estimator._set_inputs(Data[row])\n",
    "            out = self.estimator.execute()\n",
    "            outs.append(out) \n",
    "        m = min(outs)\n",
    "        M = max(outs)\n",
    "        s = M-m\n",
    "        if s == 0:\n",
    "            if self.verbose_ > 3:\n",
    "                print('No variation: ', outs[0])\n",
    "            outs = [o-m for o in outs]\n",
    "        else:\n",
    "            outs = [(o-m)/(s) for o in outs]\n",
    "        outs = [[1-v, v] for v in outs]\n",
    "        return np.array(outs)\n",
    "\n",
    "\n",
    "        #logfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2319a49a-0f83-4e10-a120-ba2a617290bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.00000000e+00, -1.35980713e+00, -7.27811733e-02,  2.53634674e+00,\n",
       "         1.37815522e+00, -3.38320770e-01,  4.62387778e-01,  2.39598554e-01,\n",
       "         9.86979013e-02,  3.63786970e-01,  9.07941720e-02, -5.51599533e-01,\n",
       "        -6.17800856e-01, -9.91389847e-01, -3.11169354e-01,  1.46817697e+00,\n",
       "        -4.70400525e-01,  2.07971242e-01,  2.57905802e-02,  4.03992960e-01,\n",
       "         2.51412098e-01, -1.83067779e-02,  2.77837576e-01, -1.10473910e-01,\n",
       "         6.69280749e-02,  1.28539358e-01, -1.89114844e-01,  1.33558377e-01,\n",
       "        -2.10530535e-02,  1.49620000e+02])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[X.values[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dd2861-4a62-4a16-8837-4bed677e3776",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers[0].predict_proba([X.values[0]])[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a4838f-bed7-40e6-b37c-df1cf09afb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSIFIERS = 4\n",
    "def get_random_samples(data, num, X, y):\n",
    "    cv = StratifiedKFold(n_splits=num)\n",
    "    splits = [] #[data.iloc[train], data.iloc[test]] for train, test in cv.split(X, y)\n",
    "    for train, test in cv.split(X,y):\n",
    "        #print(train[30000:30010])\n",
    "        splits.append([train,test])\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97f7d2-678d-43cb-9f71-f14e01ffe819",
   "metadata": {},
   "outputs": [],
   "source": [
    "spl = get_random_samples(data, NUM_CLASSIFIERS, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1b21081-a0ab-478b-add1-4b0fc555a6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  1\n",
      "----------NEW BEST FITNESS---------------\n",
      "0.0017660044150110375\n",
      "[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 162066.0, 0.0544105928181785, 0.785706434580365, 0.210227060844834, -0.609445244808125, 0.383924375742631, -1.06789966873896, 0.934487814612767, -0.136913829228006, 0.128665702097008, -0.378670745424556, -1.24188765931405, -0.482393260562802, -1.03856832359677, 0.317263522448578, -0.274645313382105, 6.66764911585938e-05, -0.400776270705783, -0.568671715568206, -0.0209360630377615, -0.103757642715292, -0.282845175969466, -0.686513649675817, 0.0632705604845528, -0.146642688550363, -0.506109867335658, 0.151054369513736, 0.24011346105828, 0.0935842002274566, 2.69]\n",
      "Max(7,0,3)\n",
      "Gte(9,4,0)\n",
      "Not(7,9,4)\n",
      "Min(0,9,6)\n",
      "Min(4,1,3)\n",
      "Sqrt(6,7,6)\n",
      "Exp(3,2,6)\n",
      "Div(10,1,2)\n",
      "If(5,3,6)\n",
      "\n",
      "---------- EFFECTIVE PROGRAM ---------------\n",
      "[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 162066.0, 0.0544105928181785, 0.785706434580365, 0.210227060844834, -0.609445244808125, 0.383924375742631, -1.06789966873896, 0.934487814612767, -0.136913829228006, 0.128665702097008, -0.378670745424556, -1.24188765931405, -0.482393260562802, -1.03856832359677, 0.317263522448578, -0.274645313382105, 6.66764911585938e-05, -0.400776270705783, -0.568671715568206, -0.0209360630377615, -0.103757642715292, -0.282845175969466, -0.686513649675817, 0.0632705604845528, -0.146642688550363, -0.506109867335658, 0.151054369513736, 0.24011346105828, 0.0935842002274566, 2.69]\n",
      "Gte(9,4,0)\n",
      "\n",
      "----------NEW BEST FITNESS---------------\n",
      "0.0019034653705289949\n",
      "[162066.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 162066.0, 0.0544105928181785, 0.785706434580365, 0.210227060844834, -0.609445244808125, 0.383924375742631, -1.06789966873896, 0.934487814612767, -0.136913829228006, 0.128665702097008, -0.378670745424556, -1.24188765931405, -0.482393260562802, -1.03856832359677, 0.317263522448578, -0.274645313382105, 6.66764911585938e-05, -0.400776270705783, -0.568671715568206, -0.0209360630377615, -0.103757642715292, -0.282845175969466, -0.686513649675817, 0.0632705604845528, -0.146642688550363, -0.506109867335658, 0.151054369513736, 0.24011346105828, 0.0935842002274566, 2.69]\n",
      "Log(6,3,4)\n",
      "Add(2,7,2)\n",
      "Not(10,0,4)\n",
      "Max(5,8,1)\n",
      "Div(11,9,0)\n",
      "Sin(3,8,3)\n",
      "\n",
      "---------- EFFECTIVE PROGRAM ---------------\n",
      "[162066.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 162066.0, 0.0544105928181785, 0.785706434580365, 0.210227060844834, -0.609445244808125, 0.383924375742631, -1.06789966873896, 0.934487814612767, -0.136913829228006, 0.128665702097008, -0.378670745424556, -1.24188765931405, -0.482393260562802, -1.03856832359677, 0.317263522448578, -0.274645313382105, 6.66764911585938e-05, -0.400776270705783, -0.568671715568206, -0.0209360630377615, -0.103757642715292, -0.282845175969466, -0.686513649675817, 0.0632705604845528, -0.146642688550363, -0.506109867335658, 0.151054369513736, 0.24011346105828, 0.0935842002274566, 2.69]\n",
      "Div(11,9,0)\n",
      "\n",
      "Best Fitness:  0.0019034653705289949\n",
      "Avg  Fitness:  0.0017715028532317581\n",
      "Generation:  2\n",
      "----------NEW BEST FITNESS---------------\n",
      "0.0021367270102892145\n",
      "[67859.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 67859.0, 1.30554236681399, -1.12141149990224, 1.621579150104, -0.163084661347321, -2.06525663252742, 0.10115093269873, -1.62340616842004, 0.280897016442069, 0.870875447364037, 0.291069792367429, -1.34578555249832, -0.658084512253041, -1.0391280242854, -1.18319745805861, -1.25557958955939, 0.610426704428336, 0.95066681677238, -1.2322605834906, 0.997362449984201, -0.0325995624797671, 0.128296643363977, 0.675433790466854, -0.0809637916395112, 0.434138666722082, 0.444179044842056, -0.0423635594467657, 0.0788006901399948, 0.0254369032838642, 6.6]\n",
      "Log(6,3,4)\n",
      "Add(2,7,2)\n",
      "Not(10,0,4)\n",
      "Max(5,8,1)\n",
      "Div(11,9,0)\n",
      "\n",
      "---------- EFFECTIVE PROGRAM ---------------\n",
      "[67859.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 67859.0, 1.30554236681399, -1.12141149990224, 1.621579150104, -0.163084661347321, -2.06525663252742, 0.10115093269873, -1.62340616842004, 0.280897016442069, 0.870875447364037, 0.291069792367429, -1.34578555249832, -0.658084512253041, -1.0391280242854, -1.18319745805861, -1.25557958955939, 0.610426704428336, 0.95066681677238, -1.2322605834906, 0.997362449984201, -0.0325995624797671, 0.128296643363977, 0.675433790466854, -0.0809637916395112, 0.434138666722082, 0.444179044842056, -0.0423635594467657, 0.0788006901399948, 0.0254369032838642, 6.6]\n",
      "Div(11,9,0)\n",
      "\n",
      "Best Fitness:  0.0021367270102892145\n",
      "Avg  Fitness:  0.001800605190570336\n",
      "Generation:  3\n",
      "----------NEW BEST FITNESS---------------\n",
      "0.002330993825691988\n",
      "[101304.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 101304.0, 1.95059987056144, -0.0519400942577975, -1.48752900899438, 0.64010914197112, 0.218985341006904, -1.09669081314598, 0.394188092489442, -0.39561930607216, 1.66786975259937, -0.271151012693021, 1.76027235569418, -1.55748761435088, 0.933117292602025, 2.21292457598706, -1.70895310486812, -0.645756220805548, 0.455772779690556, 0.158171588640285, 0.273842727610861, -0.24931890165203, -0.0758747328475964, 0.126789605855339, 0.012154410916428, 0.0345192710162314, 0.267024855768365, -0.287612231291499, -0.0770194833677063, -0.0758568529578598, 46.11]\n",
      "Add(2,7,2)\n",
      "Log(6,3,4)\n",
      "Not(10,0,4)\n",
      "Max(5,8,1)\n",
      "Div(11,9,0)\n",
      "\n",
      "---------- EFFECTIVE PROGRAM ---------------\n",
      "[101304.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 101304.0, 1.95059987056144, -0.0519400942577975, -1.48752900899438, 0.64010914197112, 0.218985341006904, -1.09669081314598, 0.394188092489442, -0.39561930607216, 1.66786975259937, -0.271151012693021, 1.76027235569418, -1.55748761435088, 0.933117292602025, 2.21292457598706, -1.70895310486812, -0.645756220805548, 0.455772779690556, 0.158171588640285, 0.273842727610861, -0.24931890165203, -0.0758747328475964, 0.126789605855339, 0.012154410916428, 0.0345192710162314, 0.267024855768365, -0.287612231291499, -0.0770194833677063, -0.0758568529578598, 46.11]\n",
      "Div(11,9,0)\n",
      "\n",
      "Best Fitness:  0.002330993825691988\n",
      "Avg  Fitness:  0.0019091350657168807\n",
      "Generation:  4\n",
      "Best Fitness:  0.0017660044150110375\n",
      "Avg  Fitness:  0.0016185262591009188\n",
      "Generation:  5\n",
      "Best Fitness:  0.0017660044150110375\n",
      "Avg  Fitness:  0.0017296492781320187\n",
      "Generation:  6\n",
      "Best Fitness:  0.0017660044150110375\n",
      "Avg  Fitness:  0.001756432586771353\n",
      "Generation:  7\n",
      "Best Fitness:  0.0017660044150110375\n",
      "Avg  Fitness:  0.001753113477109326\n",
      "Generation:  8\n",
      "Best Fitness:  0.0018359668463573998\n",
      "Avg  Fitness:  0.0017669372474289912\n",
      "Generation:  9\n",
      "----------NEW BEST FITNESS---------------\n",
      "0.002693755893913241\n",
      "[21072.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 105364.0, 2.08235269233341, 0.232952942743264, -1.96957746872728, 0.387435960792386, 0.77424962054109, -0.571305922855633, 0.0566862132293592, -0.261175111190163, 1.69720611882576, -0.671467885877798, 1.37231586569599, -2.13060603516913, 1.65599058070314, 0.85712880821834, -0.456964354685619, 0.670959197572043, 0.511898616311892, 1.61090949817109, 0.0373888600374695, -0.203252688460072, 0.0310253764453185, 0.471950967909077, -0.149351825464294, -1.16364840153247, 0.332286725616337, -0.096419468292734, -0.0354017865703463, -0.0575907442671113, 6.99]\n",
      "Copy(9,5,1)\n",
      "If(3,2,3)\n",
      "Eq(4,0,6)\n",
      "Mul(2,3,4)\n",
      "Mean(7,11,0)\n",
      "Sin(3,8,3)\n",
      "Sub(11,6,3)\n",
      "Gte(10,2,3)\n",
      "Gte(5,3,5)\n",
      "\n",
      "---------- EFFECTIVE PROGRAM ---------------\n",
      "[21072.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 105364.0, 2.08235269233341, 0.232952942743264, -1.96957746872728, 0.387435960792386, 0.77424962054109, -0.571305922855633, 0.0566862132293592, -0.261175111190163, 1.69720611882576, -0.671467885877798, 1.37231586569599, -2.13060603516913, 1.65599058070314, 0.85712880821834, -0.456964354685619, 0.670959197572043, 0.511898616311892, 1.61090949817109, 0.0373888600374695, -0.203252688460072, 0.0310253764453185, 0.471950967909077, -0.149351825464294, -1.16364840153247, 0.332286725616337, -0.096419468292734, -0.0354017865703463, -0.0575907442671113, 6.99]\n",
      "Mean(7,11,0)\n",
      "\n",
      "Best Fitness:  0.002693755893913241\n",
      "Avg  Fitness:  0.0018154844938858242\n",
      "Generation:  10\n",
      "Best Fitness:  0.0017660044150110375\n",
      "Avg  Fitness:  0.0017209928852754072\n",
      "Generation:  11\n",
      "----------NEW BEST FITNESS---------------\n",
      "0.004137291901664787\n",
      "[11613.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 58066.0, -0.379810623781098, 1.01883981353062, 1.17458425164379, -0.120079371078126, 0.249300030810275, -0.504238105220206, 0.547762875594865, 0.11900168713249, -0.573647539449294, -0.326628880430445, 1.13145362336497, 0.310450781684588, -0.273348941391593, -0.178645237685636, 0.258387781261421, 0.690405929735878, -0.339875877516689, 0.508445621200295, 0.325305340650498, 0.115578312169001, -0.236515668565166, -0.668743677275222, -0.0625709730897, -0.0578719069537913, -0.158804604130688, 0.0760649975798322, 0.238648205714549, 0.0844814266273642, 0.99]\n",
      "Mean(7,11,0)\n",
      "Sin(3,8,3)\n",
      "Sub(11,6,3)\n",
      "Gte(10,2,3)\n",
      "Gte(5,3,5)\n",
      "Mul(8,2,2)\n",
      "Mean(2,1,6)\n",
      "\n",
      "---------- EFFECTIVE PROGRAM ---------------\n",
      "[11613.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 58066.0, -0.379810623781098, 1.01883981353062, 1.17458425164379, -0.120079371078126, 0.249300030810275, -0.504238105220206, 0.547762875594865, 0.11900168713249, -0.573647539449294, -0.326628880430445, 1.13145362336497, 0.310450781684588, -0.273348941391593, -0.178645237685636, 0.258387781261421, 0.690405929735878, -0.339875877516689, 0.508445621200295, 0.325305340650498, 0.115578312169001, -0.236515668565166, -0.668743677275222, -0.0625709730897, -0.0578719069537913, -0.158804604130688, 0.0760649975798322, 0.238648205714549, 0.0844814266273642, 0.99]\n",
      "Mean(7,11,0)\n",
      "\n",
      "Best Fitness:  0.004137291901664787\n",
      "Avg  Fitness:  0.001829238747988473\n",
      "Generation:  12\n",
      "Best Fitness:  0.0017660044150110375\n",
      "Avg  Fitness:  0.001762794720231883\n",
      "Generation:  13\n",
      "Best Fitness:  0.0021193806154206833\n",
      "Avg  Fitness:  0.0017707160976831685\n",
      "Generation:  14\n",
      "Best Fitness:  0.0036353428731225336\n",
      "Avg  Fitness:  0.00179092892778586\n",
      "Generation:  15\n",
      "Best Fitness:  0.0017660044150110375\n",
      "Avg  Fitness:  0.0017553953898581784\n",
      "Generation:  16\n",
      "Best Fitness:  0.0017660044150110375\n",
      "Avg  Fitness:  0.00176600441501104\n",
      "Generation:  17\n",
      "Best Fitness:  0.0017660044150110375\n",
      "Avg  Fitness:  0.00176600441501104\n",
      "Generation:  18\n",
      "Best Fitness:  0.002913831341830278\n",
      "Avg  Fitness:  0.0017966131330595528\n",
      "Generation:  19\n",
      "Best Fitness:  0.0017660044150110375\n",
      "Avg  Fitness:  0.0017512938044800849\n",
      "Generation:  20\n",
      "Best Fitness:  0.0020670956492234373\n",
      "Avg  Fitness:  0.0017740335145900372\n",
      "Generation:  21\n",
      "----------NEW BEST FITNESS---------------\n",
      "0.004163463935248861\n",
      "[26608.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, -1.0, -1.0, 1.0, 0.5, 133043.0, -4.69690948954638, 2.8263364335758, -1.18272536294676, -1.26985745820137, -1.77238610019528, 0.116925136078131, -1.59762985192037, 0.333640810180401, 0.820260200234563, 0.346411135299001, -0.0296813914217533, 1.70356344677555, 0.545555971904821, 0.857218710772516, -0.40262247645419, 0.875831306185808, -0.156769160988312, 0.444173035379075, 0.136173332269562, -0.51628514216074, 1.44027375557551, -0.931747789418275, 0.336794861924296, -0.511857897347411, 0.0192265970657887, -0.309766412248278, -0.612300892956056, -0.286861408078099, 19.91]\n",
      "Not(6,1,5)\n",
      "Max(1,2,7)\n",
      "Gte(5,3,5)\n",
      "Eq(8,6,6)\n",
      "Min(7,2,6)\n",
      "Lt(9,6,6)\n",
      "Min(3,10,7)\n",
      "Gte(8,6,4)\n",
      "If(11,10,4)\n",
      "Mean(7,11,0)\n",
      "\n",
      "---------- EFFECTIVE PROGRAM ---------------\n",
      "[26608.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, -1.0, -1.0, 1.0, 0.5, 133043.0, -4.69690948954638, 2.8263364335758, -1.18272536294676, -1.26985745820137, -1.77238610019528, 0.116925136078131, -1.59762985192037, 0.333640810180401, 0.820260200234563, 0.346411135299001, -0.0296813914217533, 1.70356344677555, 0.545555971904821, 0.857218710772516, -0.40262247645419, 0.875831306185808, -0.156769160988312, 0.444173035379075, 0.136173332269562, -0.51628514216074, 1.44027375557551, -0.931747789418275, 0.336794861924296, -0.511857897347411, 0.0192265970657887, -0.309766412248278, -0.612300892956056, -0.286861408078099, 19.91]\n",
      "Gte(5,3,5)\n",
      "Lt(9,6,6)\n",
      "Min(3,10,7)\n",
      "Mean(7,11,0)\n",
      "\n",
      "Best Fitness:  0.004163463935248861\n",
      "Avg  Fitness:  0.0018938689227570571\n",
      "Generation:  22\n",
      "----------NEW BEST FITNESS---------------\n",
      "0.005822207776073376\n",
      "[-19050348529.0, 19050348529.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19050348529.0, -1.0, 1.0, 0.5, 138023.0, 1.87968811409826, -0.509541053711962, -1.76972768040964, 0.468141264119464, 2.07235424101407, 4.1661401153328, -0.85873042786812, 1.07733470458465, 1.02092486038104, -0.0025664061432383, -0.54175452176554, 0.745736479601007, -0.372132292538181, -0.0643760526609334, -0.793799649778635, -0.495657797524636, -0.133720049527652, -1.00147309642465, 0.0806411982872137, -0.172979818966188, -0.513759868287183, -1.34648756391304, 0.458950230174254, 0.617555794686475, -0.289793486814608, -1.08665721087646, 0.0699187096313232, -0.0330587431317043, 29.0]\n",
      "Div(7,8,3)\n",
      "Eq(8,6,6)\n",
      "Min(3,10,7)\n",
      "Sqr(11,1,1)\n",
      "Gte(9,8,6)\n",
      "Sub(1,3,7)\n",
      "Or(7,2,6)\n",
      "Not(6,1,5)\n",
      "Max(1,2,7)\n",
      "And(6,8,6)\n",
      "Sqrt(5,8,7)\n",
      "Eq(7,10,5)\n",
      "Min(7,6,6)\n",
      "Div(7,8,3)\n",
      "Eq(8,6,6)\n",
      "Lt(9,6,6)\n",
      "Min(7,2,6)\n",
      "Gte(8,6,4)\n",
      "Not(5,3,5)\n",
      "And(6,8,0)\n",
      "Max(1,3,7)\n",
      "Div(7,8,0)\n",
      "Mul(0,6,2)\n",
      "\n",
      "---------- EFFECTIVE PROGRAM ---------------\n",
      "[-19050348529.0, 19050348529.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19050348529.0, -1.0, 1.0, 0.5, 138023.0, 1.87968811409826, -0.509541053711962, -1.76972768040964, 0.468141264119464, 2.07235424101407, 4.1661401153328, -0.85873042786812, 1.07733470458465, 1.02092486038104, -0.0025664061432383, -0.54175452176554, 0.745736479601007, -0.372132292538181, -0.0643760526609334, -0.793799649778635, -0.495657797524636, -0.133720049527652, -1.00147309642465, 0.0806411982872137, -0.172979818966188, -0.513759868287183, -1.34648756391304, 0.458950230174254, 0.617555794686475, -0.289793486814608, -1.08665721087646, 0.0699187096313232, -0.0330587431317043, 29.0]\n",
      "Sqr(11,1,1)\n",
      "Max(1,3,7)\n",
      "Div(7,8,0)\n",
      "\n",
      "Best Fitness:  0.005822207776073376\n",
      "Avg  Fitness:  0.0018460378576427787\n",
      "Generation:  23\n",
      "Best Fitness:  0.005792870541476152\n",
      "Avg  Fitness:  0.001900679475015753\n",
      "Generation:  24\n",
      "----------NEW BEST FITNESS---------------\n",
      "0.006533259489867064\n",
      "[-4544782225.0, 4544782225.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4544782225.0, -1.0, 1.0, 0.5, 67415.0, 0.988990255372371, -0.0833770995406845, -0.0684722654077597, 0.33740003850452, 0.575819011683554, 1.18837862466487, -0.0891659802138827, 0.441962540564062, -0.348095002380523, -0.0490488131940988, 2.30212239083504, 1.13093703476243, -0.360639420335393, 0.787918556725939, 1.21104860992875, -1.05322362440847, 0.680006593644011, -1.8981436196318, -1.38179643204679, -0.184262711659734, 0.130949394848065, 0.438618010843259, 0.0283822099350026, -0.987667961427849, 0.204295879832743, 0.476140018769916, 0.007871726078367, -0.013708564199863, 37.92]\n",
      "Div(7,8,3)\n",
      "Eq(8,6,6)\n",
      "Min(3,10,7)\n",
      "Sqr(11,1,1)\n",
      "Gte(9,8,6)\n",
      "Sub(1,3,7)\n",
      "Or(7,2,6)\n",
      "Not(6,1,5)\n",
      "Max(1,2,7)\n",
      "And(6,8,6)\n",
      "Sqrt(5,8,7)\n",
      "Eq(7,10,5)\n",
      "Min(7,6,6)\n",
      "Div(7,8,3)\n",
      "Eq(8,6,6)\n",
      "Lt(9,6,6)\n",
      "Min(7,2,6)\n",
      "Gte(8,6,4)\n",
      "Not(5,3,5)\n",
      "And(6,8,0)\n",
      "Max(1,3,7)\n",
      "Div(7,8,0)\n",
      "Mul(0,6,2)\n",
      "\n",
      "---------- EFFECTIVE PROGRAM ---------------\n",
      "[-4544782225.0, 4544782225.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4544782225.0, -1.0, 1.0, 0.5, 67415.0, 0.988990255372371, -0.0833770995406845, -0.0684722654077597, 0.33740003850452, 0.575819011683554, 1.18837862466487, -0.0891659802138827, 0.441962540564062, -0.348095002380523, -0.0490488131940988, 2.30212239083504, 1.13093703476243, -0.360639420335393, 0.787918556725939, 1.21104860992875, -1.05322362440847, 0.680006593644011, -1.8981436196318, -1.38179643204679, -0.184262711659734, 0.130949394848065, 0.438618010843259, 0.0283822099350026, -0.987667961427849, 0.204295879832743, 0.476140018769916, 0.007871726078367, -0.013708564199863, 37.92]\n",
      "Sqr(11,1,1)\n",
      "Max(1,3,7)\n",
      "Div(7,8,0)\n",
      "\n",
      "Best Fitness:  0.006533259489867064\n",
      "Avg  Fitness:  0.0023367356515673118\n",
      "Generation:  25\n",
      "Best Fitness:  0.0051933117354026935\n",
      "Avg  Fitness:  0.0023111192306890693\n",
      "Generation:  26\n",
      "----------NEW BEST FITNESS---------------\n",
      "0.017868297575513673\n",
      "[-337.2002372478406, 337.2002372478406, 0.0, 0.0, 0.0, 0.0, 0.0, 337.2002372478406, -1.0, 1.0, 0.5, 113704.0, 2.06463361467252, -0.109082841217304, -1.59216539156812, -0.0170313490823156, 0.412354226899739, -0.391334476331781, 0.0343921415084711, -0.106783300175154, 0.562448775506596, 0.0789231988688255, 0.570291944410869, 0.700855239769401, -0.255994428631358, 0.707447366923738, 0.259606706590645, 0.0546483633708037, -0.920770597573804, 0.766209157516554, 0.249128261105702, -0.222314666521188, 0.288092322801353, 0.929397745360768, -0.0416994964093128, 0.341168219947816, 0.36243415620862, -0.44740001302763, -0.0026290227747617, -0.0615131743649812, 1.0]\n",
      "Div(7,8,3)\n",
      "Eq(8,6,6)\n",
      "Sqrt(11,1,1)\n",
      "Gte(9,8,6)\n",
      "Sub(1,3,7)\n",
      "Or(7,2,6)\n",
      "Not(6,1,5)\n",
      "Max(1,2,7)\n",
      "And(6,8,6)\n",
      "Sqr(1,1,7)\n",
      "Sqrt(5,8,7)\n",
      "Eq(7,10,5)\n",
      "Min(7,6,6)\n",
      "Div(7,8,3)\n",
      "Eq(8,6,6)\n",
      "Lt(9,6,6)\n",
      "Min(7,2,6)\n",
      "Gte(8,6,4)\n",
      "Not(5,3,5)\n",
      "And(6,8,0)\n",
      "Max(1,3,7)\n",
      "Div(7,8,0)\n",
      "Mul(0,6,2)\n",
      "\n",
      "---------- EFFECTIVE PROGRAM ---------------\n",
      "[-337.2002372478406, 337.2002372478406, 0.0, 0.0, 0.0, 0.0, 0.0, 337.2002372478406, -1.0, 1.0, 0.5, 113704.0, 2.06463361467252, -0.109082841217304, -1.59216539156812, -0.0170313490823156, 0.412354226899739, -0.391334476331781, 0.0343921415084711, -0.106783300175154, 0.562448775506596, 0.0789231988688255, 0.570291944410869, 0.700855239769401, -0.255994428631358, 0.707447366923738, 0.259606706590645, 0.0546483633708037, -0.920770597573804, 0.766209157516554, 0.249128261105702, -0.222314666521188, 0.288092322801353, 0.929397745360768, -0.0416994964093128, 0.341168219947816, 0.36243415620862, -0.44740001302763, -0.0026290227747617, -0.0615131743649812, 1.0]\n",
      "Sqrt(11,1,1)\n",
      "Max(1,3,7)\n",
      "Div(7,8,0)\n",
      "\n",
      "Best Fitness:  0.017868297575513673\n",
      "Avg  Fitness:  0.0020889379540457334\n",
      "Generation:  27\n",
      "Best Fitness:  0.008180854526031957\n",
      "Avg  Fitness:  0.0021645482269434914\n",
      "Generation:  28\n",
      "Best Fitness:  0.003095292168292021\n",
      "Avg  Fitness:  0.0018576400009983282\n",
      "Generation:  29\n",
      "Best Fitness:  0.00422496723353126\n",
      "Avg  Fitness:  0.002351494997830402\n",
      "Generation:  30\n",
      "Best Fitness:  0.003200448811536493\n",
      "Avg  Fitness:  0.0024513689995659003\n",
      "Generation:  31\n",
      "Best Fitness:  0.0028345729881313177\n",
      "Avg  Fitness:  0.0022132552290564677\n",
      "Generation:  32\n",
      "Best Fitness:  0.00259019866110938\n",
      "Avg  Fitness:  0.0021408521754655054\n",
      "Generation:  33\n",
      "Best Fitness:  0.0022184536091827277\n",
      "Avg  Fitness:  0.0019533158470475907\n",
      "Generation:  34\n",
      "Best Fitness:  0.006574307985182136\n",
      "Avg  Fitness:  0.002752623212524756\n",
      "Generation:  35\n",
      "Best Fitness:  0.011951490324565564\n",
      "Avg  Fitness:  0.004057031163386496\n",
      "Generation:  36\n",
      "Best Fitness:  0.0035456673071518107\n",
      "Avg  Fitness:  0.0021131215094659715\n",
      "Generation:  37\n",
      "Best Fitness:  0.00445644531726231\n",
      "Avg  Fitness:  0.0025302062958425836\n",
      "Generation:  38\n",
      "Best Fitness:  0.007143091657248732\n",
      "Avg  Fitness:  0.0037005726234036313\n",
      "Generation:  39\n",
      "Best Fitness:  0.003738507425938466\n",
      "Avg  Fitness:  0.0026117017315628924\n",
      "Generation:  40\n",
      "Best Fitness:  0.005371536484568919\n",
      "Avg  Fitness:  0.003926329827325614\n",
      "Generation:  41\n",
      "Best Fitness:  0.0025214959676485575\n",
      "Avg  Fitness:  0.001919038953587251\n",
      "Generation:  42\n",
      "Best Fitness:  0.004192132363036468\n",
      "Avg  Fitness:  0.003175030203207136\n",
      "Generation:  43\n",
      "Best Fitness:  0.006074764699368367\n",
      "Avg  Fitness:  0.004079659917955889\n",
      "Generation:  44\n",
      "Best Fitness:  0.002583779942935166\n",
      "Avg  Fitness:  0.002289700772021548\n",
      "Generation:  45\n",
      "Best Fitness:  0.003728458800013305\n",
      "Avg  Fitness:  0.0030923959528787303\n",
      "Generation:  46\n",
      "Best Fitness:  0.00730500120075784\n",
      "Avg  Fitness:  0.005577872411096355\n",
      "Generation:  47\n",
      "Best Fitness:  0.0034399342638633517\n",
      "Avg  Fitness:  0.0028399715781354416\n",
      "Generation:  48\n",
      "Best Fitness:  0.013622945769105527\n",
      "Avg  Fitness:  0.009730483533291069\n",
      "Generation:  49\n",
      "Best Fitness:  0.0023636909988955356\n",
      "Avg  Fitness:  0.002055865034197646\n",
      "Generation:  50\n",
      "Best Fitness:  0.002550722382067788\n",
      "Avg  Fitness:  0.002287628344824936\n",
      "Generation:  51\n",
      "Best Fitness:  0.007434963010753665\n",
      "Avg  Fitness:  0.005765826905294373\n",
      "Generation:  52\n",
      "Best Fitness:  0.0066994222847420555\n",
      "Avg  Fitness:  0.005178198118756791\n",
      "Generation:  53\n",
      "Best Fitness:  0.008082649524599545\n",
      "Avg  Fitness:  0.006235769580450757\n",
      "Generation:  54\n",
      "Best Fitness:  0.0017981966231943186\n",
      "Avg  Fitness:  0.0017905538076063898\n",
      "Generation:  55\n",
      "Best Fitness:  0.0040690433000501736\n",
      "Avg  Fitness:  0.0033711584045460715\n",
      "Generation:  56\n",
      "Best Fitness:  0.00267487665180582\n",
      "Avg  Fitness:  0.0024401052074052567\n",
      "Generation:  57\n",
      "Best Fitness:  0.013976189529303696\n",
      "Avg  Fitness:  0.010863539267706564\n",
      "Generation:  58\n",
      "Best Fitness:  0.006688703834182838\n",
      "Avg  Fitness:  0.00569939015247052\n",
      "Generation:  59\n",
      "Best Fitness:  0.0050106300355064895\n",
      "Avg  Fitness:  0.004240846996582936\n",
      "Generation:  60\n",
      "Best Fitness:  0.003428133095914922\n",
      "Avg  Fitness:  0.0016979769394033465\n",
      "Generation:  61\n",
      "Best Fitness:  0.0043482986572588415\n",
      "Avg  Fitness:  0.0018719315289232664\n",
      "Generation:  62\n",
      "Best Fitness:  0.013807540004572693\n",
      "Avg  Fitness:  0.004787205019281761\n",
      "Generation:  63\n",
      "Best Fitness:  0.00579242226115407\n",
      "Avg  Fitness:  0.00353298508751688\n",
      "Generation:  64\n",
      "Best Fitness:  0.005213888621297116\n",
      "Avg  Fitness:  0.004102970925532622\n",
      "Generation:  65\n",
      "Best Fitness:  0.007464800712210635\n",
      "Avg  Fitness:  0.006104353810800603\n",
      "Generation:  66\n",
      "Best Fitness:  0.003970052442934804\n",
      "Avg  Fitness:  0.0034955703240348765\n",
      "Generation:  67\n",
      "Best Fitness:  0.008437015334448551\n",
      "Avg  Fitness:  0.007286197749706501\n",
      "Generation:  68\n",
      "Best Fitness:  0.0024982007077130513\n",
      "Avg  Fitness:  0.002030749278791535\n",
      "Generation:  69\n",
      "Best Fitness:  0.005518595901838243\n",
      "Avg  Fitness:  0.002356264931580212\n",
      "Generation:  70\n",
      "Best Fitness:  0.0024493757380634872\n",
      "Avg  Fitness:  0.002157738703066394\n",
      "Generation:  71\n",
      "Best Fitness:  0.004214939784048405\n",
      "Avg  Fitness:  0.003469115102077401\n",
      "Generation:  72\n",
      "Best Fitness:  0.008515573346923266\n",
      "Avg  Fitness:  0.007318053135648274\n",
      "Generation:  73\n",
      "Best Fitness:  0.0031576571129216966\n",
      "Avg  Fitness:  0.0017131527787662493\n",
      "Generation:  74\n",
      "Best Fitness:  0.010257175076941585\n",
      "Avg  Fitness:  0.005014569450669407\n",
      "Generation:  75\n",
      "Best Fitness:  0.00670244758306224\n",
      "Avg  Fitness:  0.004405986431474285\n",
      "Generation:  76\n",
      "Best Fitness:  0.01217657270328461\n",
      "Avg  Fitness:  0.008252176429306668\n",
      "Generation:  77\n",
      "Best Fitness:  0.006042027043759379\n",
      "Avg  Fitness:  0.0025986473646180002\n",
      "Generation:  78\n",
      "Best Fitness:  0.006048036158523149\n",
      "Avg  Fitness:  0.004181186783613892\n",
      "Generation:  79\n",
      "Best Fitness:  0.009514275139312712\n",
      "Avg  Fitness:  0.007741017487837783\n",
      "Generation:  80\n",
      "Best Fitness:  0.014631946193680506\n",
      "Avg  Fitness:  0.011366656585825015\n",
      "Generation:  81\n",
      "Best Fitness:  0.005437254068510911\n",
      "Avg  Fitness:  0.004751954133190938\n",
      "Generation:  82\n",
      "Best Fitness:  0.01692153164274615\n",
      "Avg  Fitness:  0.013058240320780243\n",
      "Generation:  83\n",
      "Best Fitness:  0.003055819239362537\n",
      "Avg  Fitness:  0.0017382014925653956\n",
      "Generation:  84\n",
      "Best Fitness:  0.004033732581516483\n",
      "Avg  Fitness:  0.0021066268094307563\n",
      "Generation:  85\n",
      "Best Fitness:  0.008467610294863254\n",
      "Avg  Fitness:  0.00516701684799537\n",
      "Generation:  86\n",
      "Best Fitness:  0.003645279255422921\n",
      "Avg  Fitness:  0.00185991462524914\n",
      "Generation:  87\n",
      "Best Fitness:  0.004079898243739208\n",
      "Avg  Fitness:  0.0025319824825047383\n",
      "Generation:  88\n",
      "Best Fitness:  0.006493630307126785\n",
      "Avg  Fitness:  0.003967192842685355\n",
      "Generation:  89\n",
      "Best Fitness:  0.00335212807298432\n",
      "Avg  Fitness:  0.0026969161713568408\n",
      "Generation:  90\n",
      "Best Fitness:  0.006133056133056133\n",
      "Avg  Fitness:  0.005084963720725315\n",
      "Generation:  91\n",
      "Best Fitness:  0.0022029528481754073\n",
      "Avg  Fitness:  0.002150519036195683\n",
      "Generation:  92\n",
      "Best Fitness:  0.006509864664213962\n",
      "Avg  Fitness:  0.006067104374288346\n",
      "Generation:  93\n",
      "Best Fitness:  0.004788759104238292\n",
      "Avg  Fitness:  0.003373365159031569\n",
      "Generation:  94\n",
      "Best Fitness:  0.0035115543756920687\n",
      "Avg  Fitness:  0.0032909086369972756\n",
      "Generation:  95\n",
      "Best Fitness:  0.005054316881485786\n",
      "Avg  Fitness:  0.004456771306571117\n",
      "Generation:  96\n",
      "Best Fitness:  0.005927394447533529\n",
      "Avg  Fitness:  0.005112366951929221\n",
      "Generation:  97\n",
      "Best Fitness:  0.005045466762784839\n",
      "Avg  Fitness:  0.004418119589628983\n",
      "Generation:  98\n",
      "Best Fitness:  0.008354052025277456\n",
      "Avg  Fitness:  0.0070301914085674315\n",
      "Generation:  99\n",
      "Best Fitness:  0.0025783596141387114\n",
      "Avg  Fitness:  0.0020236892556901903\n",
      "Generation:  100\n",
      "Best Fitness:  0.002421954566851958\n",
      "Avg  Fitness:  0.0020345856588426135\n",
      "Generation:  101\n",
      "Best Fitness:  0.01430326000526978\n",
      "Avg  Fitness:  0.011286250255895202\n",
      "Generation:  102\n",
      "Best Fitness:  0.0048600953634290595\n",
      "Avg  Fitness:  0.004244273367576082\n",
      "Generation:  103\n",
      "Best Fitness:  0.004398062721176989\n",
      "Avg  Fitness:  0.004021637713883848\n",
      "Generation:  104\n",
      "Best Fitness:  0.01090541165587419\n",
      "Avg  Fitness:  0.00971189226626423\n",
      "Generation:  105\n",
      "Best Fitness:  0.01160104278699194\n",
      "Avg  Fitness:  0.010683105872273713\n",
      "Generation:  106\n",
      "Best Fitness:  0.0018447189638544888\n",
      "Avg  Fitness:  0.0018352732179932765\n",
      "Generation:  107\n",
      "Best Fitness:  0.005791271265926539\n",
      "Avg  Fitness:  0.0049862178957434415\n",
      "Generation:  108\n",
      "Best Fitness:  0.004972193973028811\n",
      "Avg  Fitness:  0.00454470203195978\n",
      "Generation:  109\n",
      "Best Fitness:  0.0017699920163509603\n",
      "Avg  Fitness:  0.0017695666722080353\n",
      "Generation:  110\n",
      "Best Fitness:  0.0023065734233865104\n",
      "Avg  Fitness:  0.0022200823820464335\n",
      "Generation:  111\n",
      "Best Fitness:  0.006813533603897508\n",
      "Avg  Fitness:  0.006049830904397265\n",
      "Generation:  112\n",
      "Best Fitness:  0.002262792088355382\n",
      "Avg  Fitness:  0.002196564132406615\n",
      "Generation:  113\n",
      "Best Fitness:  0.007816790558148792\n",
      "Avg  Fitness:  0.00684866477524674\n",
      "Generation:  114\n",
      "Best Fitness:  0.0017660044150110375\n",
      "Avg  Fitness:  0.0017233011874801295\n",
      "Generation:  115\n",
      "Best Fitness:  0.0024858277369428266\n",
      "Avg  Fitness:  0.0022092605046418914\n",
      "Generation:  116\n",
      "Best Fitness:  0.003095272868567702\n",
      "Avg  Fitness:  0.0028109433214484757\n",
      "Generation:  117\n",
      "Best Fitness:  0.008868439642593874\n",
      "Avg  Fitness:  0.0081192195436077\n",
      "Generation:  118\n",
      "Best Fitness:  0.0035785340275182815\n",
      "Avg  Fitness:  0.0033480135051740458\n",
      "Generation:  119\n",
      "Best Fitness:  0.015228321599671203\n",
      "Avg  Fitness:  0.012708839530102125\n",
      "Generation:  120\n",
      "Best Fitness:  0.005265180443047082\n",
      "Avg  Fitness:  0.004751300001880315\n",
      "Generation:  121\n",
      "Best Fitness:  0.003937146347414125\n",
      "Avg  Fitness:  0.0018673536661590717\n",
      "Generation:  122\n",
      "Best Fitness:  0.004950420762278852\n",
      "Avg  Fitness:  0.004347504349719172\n",
      "Generation:  123\n",
      "Best Fitness:  0.00855120592392262\n",
      "Avg  Fitness:  0.007096191077086081\n",
      "Generation:  124\n",
      "Best Fitness:  0.010246699265684176\n",
      "Avg  Fitness:  0.00866363622689186\n",
      "Generation:  125\n",
      "Best Fitness:  0.014376926587679664\n",
      "Avg  Fitness:  0.01082702898997474\n",
      "Generation:  126\n",
      "Best Fitness:  0.007395737596088563\n",
      "Avg  Fitness:  0.006573656492472336\n",
      "Generation:  127\n",
      "Best Fitness:  0.0070026457949621335\n",
      "Avg  Fitness:  0.006368840503245557\n",
      "Generation:  128\n",
      "Best Fitness:  0.008359226296680107\n",
      "Avg  Fitness:  0.0015568849290768317\n",
      "Generation:  129\n",
      "Best Fitness:  0.00918107514075243\n",
      "Avg  Fitness:  0.006866857513707173\n",
      "Generation:  130\n",
      "Best Fitness:  0.004974399727636678\n",
      "Avg  Fitness:  0.004556198788928048\n",
      "Generation:  131\n",
      "Best Fitness:  0.0024252738871784196\n",
      "Avg  Fitness:  0.0023989031082917245\n",
      "Generation:  132\n",
      "Best Fitness:  0.002203021078366864\n",
      "Avg  Fitness:  0.001986530909954905\n",
      "Generation:  133\n",
      "Best Fitness:  0.005853510701155822\n",
      "Avg  Fitness:  0.005152945656292252\n",
      "Generation:  134\n",
      "Best Fitness:  0.011986792721139262\n",
      "Avg  Fitness:  0.011169129656648997\n",
      "Generation:  135\n",
      "Best Fitness:  0.005880906850436941\n",
      "Avg  Fitness:  0.005496849289797194\n",
      "Generation:  136\n",
      "Best Fitness:  0.007411139143034418\n",
      "Avg  Fitness:  0.006959635795075236\n",
      "Generation:  137\n",
      "Best Fitness:  0.0025757187858155874\n",
      "Avg  Fitness:  0.002477629550405086\n",
      "Generation:  138\n",
      "Best Fitness:  0.009398419546510255\n",
      "Avg  Fitness:  0.005731028910382324\n",
      "Generation:  139\n",
      "----------NEW BEST FITNESS---------------\n",
      "0.01842953742031266\n",
      "[26888128576.0, 26888128576.0, -26888128576.0, 0.0, 0.0, 1.0, 1.0, 26888128576.0, -1.0, 1.0, 0.5, 163976.0, 2.19237310149524, -1.57607936701388, -0.746201209985696, -1.81092413815091, -1.09134009611085, 0.302718305838423, -1.42940457989905, 0.0781042882876768, -1.29959761226812, 1.64287538011153, 0.278976874743307, 0.105428565373847, 1.21642821658432, -0.57434030410115, -0.612736264393756, 0.135750622505261, -0.161642350471123, 0.494335064709574, 0.3811873002146, -0.243676444501356, -0.366637092058996, -0.71403281935127, 0.365682475494201, 0.0388813762170136, -0.530521472333624, -0.504671517327896, 0.0246056425658002, -0.0353646282432005, 57.25]\n",
      "Gte(5,3,0)\n",
      "Div(7,8,0)\n",
      "If(7,9,5)\n",
      "Max(1,3,7)\n",
      "Div(7,8,0)\n",
      "If(7,9,5)\n",
      "Sub(6,6,5)\n",
      "Not(6,1,5)\n",
      "Eq(8,6,6)\n",
      "Gte(8,6,4)\n",
      "Neq(0,6,2)\n",
      "Or(10,11,5)\n",
      "Gte(8,6,3)\n",
      "Gte(8,6,4)\n",
      "Sqrt(5,6,7)\n",
      "Neq(0,6,2)\n",
      "Or(10,11,5)\n",
      "And(6,8,0)\n",
      "Gte(2,10,6)\n",
      "Log(7,2,6)\n",
      "Not(6,1,5)\n",
      "Sqr(11,1,1)\n",
      "Sqr(1,3,7)\n",
      "If(7,9,5)\n",
      "Gte(8,6,3)\n",
      "Max(1,3,7)\n",
      "Div(7,8,0)\n",
      "If(7,9,5)\n",
      "Not(0,4,1)\n",
      "Add(11,1,3)\n",
      "Sqrt(7,8,3)\n",
      "And(4,5,1)\n",
      "And(4,5,1)\n",
      "Or(10,11,2)\n",
      "Min(7,6,6)\n",
      "Sqr(11,1,1)\n",
      "Add(8,6,6)\n",
      "Sin(8,10,7)\n",
      "Gte(5,6,4)\n",
      "Sqrt(5,8,7)\n",
      "Eq(8,6,6)\n",
      "Or(10,11,2)\n",
      "Not(6,1,5)\n",
      "Eq(8,6,6)\n",
      "Gte(8,6,4)\n",
      "Sqrt(5,6,7)\n",
      "Neq(0,6,2)\n",
      "Or(10,11,5)\n",
      "Copy(7,6,6)\n",
      "Or(11,8,2)\n",
      "Not(5,3,5)\n",
      "And(6,8,0)\n",
      "Max(1,3,7)\n",
      "Div(7,8,0)\n",
      "Sub(6,6,5)\n",
      "Neq(1,11,4)\n",
      "Gte(8,6,4)\n",
      "Sqrt(5,6,7)\n",
      "Div(7,8,3)\n",
      "Mul(0,6,2)\n",
      "Or(8,6,1)\n",
      "Sqr(11,1,1)\n",
      "Sqr(1,3,7)\n",
      "If(7,9,5)\n",
      "Add(8,6,3)\n",
      "Max(1,3,7)\n",
      "Div(7,8,0)\n",
      "If(7,9,5)\n",
      "Copy(7,6,6)\n",
      "Or(11,8,2)\n",
      "Eq(7,10,5)\n",
      "Log(1,4,4)\n",
      "Max(1,3,2)\n",
      "Gte(8,6,4)\n",
      "Not(6,1,5)\n",
      "Div(7,8,3)\n",
      "Add(5,3,0)\n",
      "Div(7,8,0)\n",
      "If(7,9,5)\n",
      "Sub(6,6,5)\n",
      "Neq(1,11,4)\n",
      "And(8,7,0)\n",
      "Add(11,1,3)\n",
      "Eq(8,10,6)\n",
      "Eq(8,6,6)\n",
      "And(4,5,1)\n",
      "Neq(1,11,4)\n",
      "Min(7,6,6)\n",
      "Mean(1,7,7)\n",
      "Sqr(11,1,1)\n",
      "Eq(8,6,6)\n",
      "If(7,9,5)\n",
      "Div(7,8,3)\n",
      "Max(1,3,7)\n",
      "\n",
      "---------- EFFECTIVE PROGRAM ---------------\n",
      "[26888128576.0, 26888128576.0, -26888128576.0, 0.0, 0.0, 1.0, 1.0, 26888128576.0, -1.0, 1.0, 0.5, 163976.0, 2.19237310149524, -1.57607936701388, -0.746201209985696, -1.81092413815091, -1.09134009611085, 0.302718305838423, -1.42940457989905, 0.0781042882876768, -1.29959761226812, 1.64287538011153, 0.278976874743307, 0.105428565373847, 1.21642821658432, -0.57434030410115, -0.612736264393756, 0.135750622505261, -0.161642350471123, 0.494335064709574, 0.3811873002146, -0.243676444501356, -0.366637092058996, -0.71403281935127, 0.365682475494201, 0.0388813762170136, -0.530521472333624, -0.504671517327896, 0.0246056425658002, -0.0353646282432005, 57.25]\n",
      "Sqr(11,1,1)\n",
      "Eq(8,6,6)\n",
      "Not(6,1,5)\n",
      "Sqrt(5,6,7)\n",
      "Copy(7,6,6)\n",
      "Or(11,8,2)\n",
      "Max(1,3,7)\n",
      "Div(7,8,0)\n",
      "Mul(0,6,2)\n",
      "Sqr(11,1,1)\n",
      "Max(1,3,7)\n",
      "And(8,7,0)\n",
      "\n",
      "Best Fitness:  0.01842953742031266\n",
      "Avg  Fitness:  0.0019507128603395837\n",
      "Generation:  140\n",
      "Best Fitness:  0.005882919711773522\n",
      "Avg  Fitness:  0.004988111106353205\n",
      "Generation:  141\n",
      "Best Fitness:  0.004496580119544052\n",
      "Avg  Fitness:  0.004310055269688218\n",
      "Generation:  142\n",
      "Best Fitness:  0.002220884419140581\n",
      "Avg  Fitness:  0.002172363885366764\n",
      "Generation:  143\n",
      "Best Fitness:  0.00627015304228057\n",
      "Avg  Fitness:  0.005849765837068745\n",
      "Generation:  144\n",
      "Best Fitness:  0.004065982880511285\n",
      "Avg  Fitness:  0.003820651844191265\n",
      "Generation:  145\n",
      "Best Fitness:  0.005196305357409841\n",
      "Avg  Fitness:  0.004876719327269695\n",
      "Generation:  146\n",
      "Best Fitness:  0.0065228237821411215\n",
      "Avg  Fitness:  0.006205702490999123\n",
      "Generation:  147\n",
      "Best Fitness:  0.01014814613681922\n",
      "Avg  Fitness:  0.00925405101982636\n",
      "Generation:  148\n",
      "Best Fitness:  0.009289209627669073\n",
      "Avg  Fitness:  0.008295016764684275\n",
      "Generation:  149\n",
      "Best Fitness:  0.0025556584698042638\n",
      "Avg  Fitness:  0.002481957424690232\n",
      "Generation:  150\n",
      "Best Fitness:  0.002808753026126019\n",
      "Avg  Fitness:  0.0019945226033306375\n",
      "Generation:  151\n",
      "Best Fitness:  0.002632754001617048\n",
      "Avg  Fitness:  0.00253558467807435\n",
      "Generation:  152\n",
      "Best Fitness:  0.0033542264099857134\n",
      "Avg  Fitness:  0.0031001108907897606\n",
      "Generation:  153\n",
      "Best Fitness:  0.0018062060086353453\n",
      "Avg  Fitness:  0.001801917838648754\n",
      "Generation:  154\n",
      "Best Fitness:  0.00545384998089102\n",
      "Avg  Fitness:  0.002056429589772077\n",
      "Generation:  155\n",
      "Best Fitness:  0.0089284913577364\n",
      "Avg  Fitness:  0.007781868221579122\n",
      "Generation:  156\n",
      "Best Fitness:  0.004028224476197785\n",
      "Avg  Fitness:  0.0015745048357503693\n",
      "Generation:  157\n",
      "Best Fitness:  0.004583114311041123\n",
      "Avg  Fitness:  0.001909083333330707\n",
      "Generation:  158\n",
      "Best Fitness:  0.009143129404308071\n",
      "Avg  Fitness:  0.005597977984111397\n",
      "Generation:  159\n",
      "----------NEW BEST FITNESS---------------\n",
      "0.08514033330152891\n",
      "[-1466660209.0, 1466660209.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1466660209.0, -1.0, 1.0, 0.5, 38297.0, -0.754473640254745, 0.0970394383555368, 3.02400079096313, 0.58065259890373, -0.450725619688691, 1.08535178204537, -0.0113879844992903, 0.444268697601195, 1.01968687835275, -0.725029611565515, 0.341624690881941, 0.74588430929121, -1.49901627653652, -0.89922977051169, -2.70033045071742, -0.658828021993552, 0.168041806859612, -0.251938494925083, -0.40369856141632, -0.152932546232325, -0.0416549290041342, 0.365083445636467, -0.137683141839483, 0.208950657187162, -0.125351811617755, -0.522423910770157, 0.0134724145586479, -0.0993709580991375, 32.8]\n",
      "Not(6,1,5)\n",
      "Gte(8,6,4)\n",
      "Neq(0,6,2)\n",
      "Or(10,11,5)\n",
      "Gte(8,6,3)\n",
      "Gte(8,6,4)\n",
      "Sqrt(5,6,7)\n",
      "Neq(0,6,2)\n",
      "Or(10,11,5)\n",
      "And(6,8,0)\n",
      "Gte(2,10,6)\n",
      "Log(7,2,6)\n",
      "Not(6,1,5)\n",
      "Sqr(11,1,1)\n",
      "Sqrt(10,6,7)\n",
      "Sqr(1,3,7)\n",
      "If(7,9,5)\n",
      "Gte(8,6,3)\n",
      "Max(1,3,7)\n",
      "Div(7,8,0)\n",
      "And(4,5,1)\n",
      "Not(0,4,1)\n",
      "Add(11,1,3)\n",
      "Div(7,8,0)\n",
      "Gte(10,11,2)\n",
      "Or(11,8,2)\n",
      "Or(11,8,2)\n",
      "Div(7,8,0)\n",
      "Eq(8,6,6)\n",
      "Sqrt(5,8,7)\n",
      "Sqrt(5,6,7)\n",
      "Neq(0,6,2)\n",
      "Or(10,11,5)\n",
      "Copy(7,6,6)\n",
      "Or(11,8,2)\n",
      "Gte(8,6,3)\n",
      "Lt(7,10,1)\n",
      "Max(1,3,7)\n",
      "Div(7,8,0)\n",
      "Add(11,1,3)\n",
      "Not(0,4,1)\n",
      "Log(11,1,3)\n",
      "Div(7,8,0)\n",
      "Max(1,3,7)\n",
      "Sqr(11,1,3)\n",
      "If(7,9,5)\n",
      "Sub(6,6,5)\n",
      "Sqr(11,1,1)\n",
      "If(7,9,5)\n",
      "Gte(8,6,3)\n",
      "Or(10,11,2)\n",
      "Div(7,8,0)\n",
      "If(7,9,5)\n",
      "Not(0,4,1)\n",
      "Max(11,1,3)\n",
      "Eq(8,10,6)\n",
      "If(7,9,5)\n",
      "Sqrt(10,6,1)\n",
      "Sub(4,5,1)\n",
      "Or(10,11,2)\n",
      "Min(7,6,6)\n",
      "Sqr(11,1,1)\n",
      "If(7,9,5)\n",
      "Sin(8,10,7)\n",
      "Gte(5,6,4)\n",
      "Gte(8,6,4)\n",
      "Eq(8,6,6)\n",
      "Sub(1,3,7)\n",
      "Log(1,4,4)\n",
      "Not(0,4,1)\n",
      "Max(1,2,3)\n",
      "Eq(4,10,6)\n",
      "And(4,5,1)\n",
      "And(4,5,1)\n",
      "Neq(0,6,2)\n",
      "Eq(8,6,6)\n",
      "Gte(8,6,4)\n",
      "Gte(8,6,4)\n",
      "Sqrt(5,6,7)\n",
      "Neq(0,6,2)\n",
      "Log(7,2,6)\n",
      "Not(6,1,5)\n",
      "Sqr(11,1,1)\n",
      "Sqrt(10,6,7)\n",
      "Sqr(1,3,7)\n",
      "If(7,9,5)\n",
      "Gte(8,6,3)\n",
      "Max(1,3,7)\n",
      "Div(7,8,0)\n",
      "If(7,9,5)\n",
      "Not(0,4,1)\n",
      "Add(11,1,3)\n",
      "Sin(8,10,7)\n",
      "And(4,5,1)\n",
      "And(4,5,1)\n",
      "Or(10,11,2)\n",
      "\n",
      "---------- EFFECTIVE PROGRAM ---------------\n",
      "[-1466660209.0, 1466660209.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1466660209.0, -1.0, 1.0, 0.5, 38297.0, -0.754473640254745, 0.0970394383555368, 3.02400079096313, 0.58065259890373, -0.450725619688691, 1.08535178204537, -0.0113879844992903, 0.444268697601195, 1.01968687835275, -0.725029611565515, 0.341624690881941, 0.74588430929121, -1.49901627653652, -0.89922977051169, -2.70033045071742, -0.658828021993552, 0.168041806859612, -0.251938494925083, -0.40369856141632, -0.152932546232325, -0.0416549290041342, 0.365083445636467, -0.137683141839483, 0.208950657187162, -0.125351811617755, -0.522423910770157, 0.0134724145586479, -0.0993709580991375, 32.8]\n",
      "Sqrt(5,6,7)\n",
      "Or(11,8,2)\n",
      "Lt(7,10,1)\n",
      "Max(1,3,7)\n",
      "Div(7,8,0)\n",
      "Not(0,4,1)\n",
      "Max(1,3,7)\n",
      "Div(7,8,0)\n",
      "Eq(8,6,6)\n",
      "Neq(0,6,2)\n",
      "Sqr(11,1,1)\n",
      "Max(1,3,7)\n",
      "Div(7,8,0)\n",
      "\n",
      "Best Fitness:  0.08514033330152891\n",
      "Avg  Fitness:  0.07291991559182928\n",
      "Generation:  160\n",
      "Best Fitness:  0.003356306073315464\n",
      "Avg  Fitness:  0.003186673896429661\n",
      "Generation:  161\n",
      "Best Fitness:  0.005496544737642492\n",
      "Avg  Fitness:  0.005148360974196891\n",
      "Generation:  162\n",
      "Best Fitness:  0.007323536355258247\n",
      "Avg  Fitness:  0.006953034225908428\n",
      "Generation:  163\n",
      "Best Fitness:  0.0028375560650720642\n",
      "Avg  Fitness:  0.002766119288401334\n",
      "Generation:  164\n",
      "Best Fitness:  0.0017660044150110375\n",
      "Avg  Fitness:  0.0015699087718570473\n",
      "Generation:  165\n",
      "Best Fitness:  0.0023750621130711924\n",
      "Avg  Fitness:  0.0022532713836452296\n",
      "Generation:  166\n",
      "Best Fitness:  0.0024471140435020225\n",
      "Avg  Fitness:  0.002329055041230255\n",
      "Generation:  167\n",
      "Best Fitness:  0.011511640347026255\n",
      "Avg  Fitness:  0.009946242801121831\n",
      "Generation:  168\n",
      "Best Fitness:  0.018366194456097325\n",
      "Avg  Fitness:  0.016145493450396217\n",
      "Generation:  169\n",
      "Best Fitness:  0.004660319126479369\n",
      "Avg  Fitness:  0.004202273615286626\n",
      "Generation:  170\n",
      "Best Fitness:  0.0029512332275995995\n",
      "Avg  Fitness:  0.0016736406861655331\n",
      "Generation:  171\n",
      "Best Fitness:  0.0044817929768987755\n",
      "Avg  Fitness:  0.003709879580300466\n",
      "Generation:  172\n",
      "Best Fitness:  0.003126076223386757\n",
      "Avg  Fitness:  0.0029146965555481006\n",
      "Generation:  173\n",
      "Best Fitness:  0.02030494007873567\n",
      "Avg  Fitness:  0.01782553127726488\n",
      "Generation:  174\n",
      "Best Fitness:  0.006003741186488302\n",
      "Avg  Fitness:  0.005611489154641774\n",
      "Generation:  175\n",
      "Best Fitness:  0.009771220994034695\n",
      "Avg  Fitness:  0.0017786389032932092\n",
      "Generation:  176\n",
      "Best Fitness:  0.012630706783906535\n",
      "Avg  Fitness:  0.009718414418980388\n",
      "Generation:  177\n",
      "Best Fitness:  0.0024163097469288557\n",
      "Avg  Fitness:  0.0021439300496371503\n",
      "Generation:  178\n",
      "Best Fitness:  0.0028515253520905778\n",
      "Avg  Fitness:  0.0025942545133937054\n",
      "Generation:  179\n",
      "Best Fitness:  0.0044858258706450506\n",
      "Avg  Fitness:  0.004155642768858897\n",
      "Generation:  180\n",
      "Best Fitness:  0.002393465631942807\n",
      "Avg  Fitness:  0.0023181702859109954\n",
      "Generation:  181\n",
      "Best Fitness:  0.0018701006861084675\n",
      "Avg  Fitness:  0.0018562211832954744\n",
      "Generation:  182\n",
      "Best Fitness:  0.004322427756575693\n",
      "Avg  Fitness:  0.0041179138892505195\n",
      "Generation:  183\n",
      "Best Fitness:  0.004234041491199821\n",
      "Avg  Fitness:  0.0018450030960488761\n",
      "Generation:  184\n",
      "Best Fitness:  0.002804055008708198\n",
      "Avg  Fitness:  0.0017990996814242148\n",
      "Generation:  185\n",
      "Best Fitness:  0.0024868717490416324\n",
      "Avg  Fitness:  0.0021913216292295014\n",
      "Generation:  186\n",
      "Best Fitness:  0.004868206199738913\n",
      "Avg  Fitness:  0.0017078978965817404\n",
      "Generation:  187\n",
      "Best Fitness:  0.0032738586439692236\n",
      "Avg  Fitness:  0.002231564825905167\n",
      "Generation:  188\n",
      "Best Fitness:  0.0035307357822626445\n",
      "Avg  Fitness:  0.0026667982080198715\n",
      "Generation:  189\n",
      "Best Fitness:  0.008219195155775445\n",
      "Avg  Fitness:  0.0047101447238542034\n",
      "Generation:  190\n",
      "Best Fitness:  0.006255553474799202\n",
      "Avg  Fitness:  0.0025284894130974287\n",
      "Generation:  191\n",
      "Best Fitness:  0.004738606899668869\n",
      "Avg  Fitness:  0.0035089271625720073\n",
      "Generation:  192\n",
      "Best Fitness:  0.003692071008418269\n",
      "Avg  Fitness:  0.00323931786836527\n",
      "Generation:  193\n",
      "Best Fitness:  0.007506091022635946\n",
      "Avg  Fitness:  0.006664211653517621\n",
      "Generation:  194\n",
      "Best Fitness:  0.011757303240203989\n",
      "Avg  Fitness:  0.009618359656015452\n",
      "Generation:  195\n",
      "Best Fitness:  0.0022477904083086303\n",
      "Avg  Fitness:  0.002164280836137051\n",
      "Generation:  196\n",
      "Best Fitness:  0.005068864942943795\n",
      "Avg  Fitness:  0.004581727075044347\n",
      "Generation:  197\n",
      "Best Fitness:  0.0034574301282340753\n",
      "Avg  Fitness:  0.002091545009070292\n",
      "Generation:  198\n",
      "Best Fitness:  0.0025973901684882665\n",
      "Avg  Fitness:  0.0019650540556484086\n",
      "Generation:  199\n",
      "Best Fitness:  0.0037539414824798076\n",
      "Avg  Fitness:  0.003264870510648884\n",
      "Generation:  200\n",
      "Best Fitness:  0.01144447607495606\n",
      "Avg  Fitness:  0.010131756463707712\n",
      "Generation:  201\n",
      "Best Fitness:  0.00183646046339334\n",
      "Avg  Fitness:  0.001827066323609032\n",
      "Generation:  202\n",
      "Best Fitness:  0.010473710057854298\n",
      "Avg  Fitness:  0.008964374413094793\n",
      "Generation:  203\n",
      "Best Fitness:  0.005465842837661515\n",
      "Avg  Fitness:  0.004873868690037435\n",
      "Generation:  204\n",
      "Best Fitness:  0.0017660044150110375\n",
      "Avg  Fitness:  0.0014184590208478815\n",
      "Generation:  205\n",
      "Best Fitness:  0.0023629295138248027\n",
      "Avg  Fitness:  0.0022037494874744654\n",
      "Generation:  206\n",
      "Best Fitness:  0.006923054963879609\n",
      "Avg  Fitness:  0.00609792687606064\n",
      "Generation:  207\n",
      "Best Fitness:  0.0025869423302603027\n",
      "Avg  Fitness:  0.002466538102690409\n",
      "Generation:  208\n",
      "Best Fitness:  0.0022401830333337964\n",
      "Avg  Fitness:  0.002164314454402155\n",
      "Generation:  209\n",
      "Best Fitness:  0.0021028768303960606\n",
      "Avg  Fitness:  0.002066943772754992\n",
      "Generation:  210\n",
      "Best Fitness:  0.0023748839290843693\n",
      "Avg  Fitness:  0.0023099367809165453\n",
      "Generation:  211\n",
      "Best Fitness:  0.005625375324791208\n",
      "Avg  Fitness:  0.0052137090944146475\n",
      "Generation:  212\n",
      "Best Fitness:  0.0024335649123619855\n",
      "Avg  Fitness:  0.0023712592659425667\n",
      "Generation:  213\n",
      "Best Fitness:  0.009412813090352419\n",
      "Avg  Fitness:  0.008495196049311458\n",
      "Generation:  214\n",
      "Best Fitness:  0.0019651587605666494\n",
      "Avg  Fitness:  0.001935970266737892\n",
      "Generation:  215\n",
      "Best Fitness:  0.002796505565866383\n",
      "Avg  Fitness:  0.0027140654737979578\n",
      "Generation:  216\n",
      "Best Fitness:  0.0038849312675323616\n",
      "Avg  Fitness:  0.003403736296546338\n",
      "Generation:  217\n",
      "Best Fitness:  0.0054405077144108254\n",
      "Avg  Fitness:  0.004852587186506862\n",
      "Generation:  218\n",
      "Best Fitness:  0.002886172761711891\n",
      "Avg  Fitness:  0.0027517525601077866\n",
      "Generation:  219\n",
      "Best Fitness:  0.004951148927374714\n",
      "Avg  Fitness:  0.004568931585891071\n",
      "Generation:  220\n",
      "Best Fitness:  0.0022744814228992335\n",
      "Avg  Fitness:  0.002179242361845029\n",
      "Generation:  221\n",
      "Best Fitness:  0.010890208689288444\n",
      "Avg  Fitness:  0.009915809843650527\n",
      "Generation:  222\n",
      "Best Fitness:  0.005197704423903371\n",
      "Avg  Fitness:  0.004785900422836295\n",
      "Generation:  223\n",
      "Best Fitness:  0.0018724895124074207\n",
      "Avg  Fitness:  0.0018582914994212392\n",
      "Generation:  224\n",
      "Best Fitness:  0.0069422461553808905\n",
      "Avg  Fitness:  0.006122383966567105\n",
      "Generation:  225\n",
      "Best Fitness:  0.009865770837827584\n",
      "Avg  Fitness:  0.006997342525106817\n",
      "Generation:  226\n",
      "Best Fitness:  0.026446862743812777\n",
      "Avg  Fitness:  0.021531717598737777\n",
      "Generation:  227\n",
      "Best Fitness:  0.00670082831958351\n",
      "Avg  Fitness:  0.0057764442557169254\n",
      "Generation:  228\n",
      "Best Fitness:  0.004116442400150848\n",
      "Avg  Fitness:  0.003674715387343825\n",
      "Generation:  229\n",
      "Best Fitness:  0.004035575235101044\n",
      "Avg  Fitness:  0.0037632267366902424\n",
      "Generation:  230\n",
      "Best Fitness:  0.0018937506016742265\n",
      "Avg  Fitness:  0.001878421059274644\n",
      "Generation:  231\n",
      "Best Fitness:  0.0022084516162988155\n",
      "Avg  Fitness:  0.0020197932948971538\n",
      "Generation:  232\n",
      "Best Fitness:  0.005644386963610309\n",
      "Avg  Fitness:  0.005239487833433375\n",
      "Generation:  233\n",
      "Best Fitness:  0.0017660044150110375\n",
      "Avg  Fitness:  0.0017531630134018177\n",
      "Generation:  234\n",
      "Best Fitness:  0.005409285949036732\n",
      "Avg  Fitness:  0.0043842137688616485\n",
      "Generation:  235\n",
      "Best Fitness:  0.008447532392506396\n",
      "Avg  Fitness:  0.00782073154980629\n",
      "Generation:  236\n",
      "Best Fitness:  0.004541485251641165\n",
      "Avg  Fitness:  0.0019749534496684845\n",
      "Generation:  237\n",
      "Best Fitness:  0.0030296071969284854\n",
      "Avg  Fitness:  0.0028397072479200407\n",
      "Generation:  238\n",
      "Best Fitness:  0.0018926432997890927\n",
      "Avg  Fitness:  0.0018740695966883122\n",
      "Generation:  239\n",
      "Best Fitness:  0.0017660044150110375\n",
      "Avg  Fitness:  0.0015742088651733354\n",
      "Generation:  240\n",
      "Best Fitness:  0.002431028114198776\n",
      "Avg  Fitness:  0.002218220530458699\n",
      "Generation:  241\n",
      "Best Fitness:  0.0021917774408958387\n",
      "Avg  Fitness:  0.0021236537567542687\n",
      "Generation:  242\n",
      "Best Fitness:  0.0037339596829250752\n",
      "Avg  Fitness:  0.0035995773562748383\n",
      "Generation:  243\n",
      "Best Fitness:  0.002789172730095595\n",
      "Avg  Fitness:  0.0027452530741144815\n",
      "Generation:  244\n",
      "Best Fitness:  0.0020602251239084683\n",
      "Avg  Fitness:  0.002032764524411373\n",
      "Generation:  245\n",
      "Best Fitness:  0.014667573380412776\n",
      "Avg  Fitness:  0.012771293141048953\n",
      "Generation:  246\n",
      "Best Fitness:  0.004120415634037108\n",
      "Avg  Fitness:  0.003680925539818909\n",
      "Generation:  247\n",
      "Best Fitness:  0.005120882315441482\n",
      "Avg  Fitness:  0.00471829696738983\n",
      "Generation:  248\n",
      "Best Fitness:  0.005922581395104957\n",
      "Avg  Fitness:  0.005312950104691174\n",
      "Generation:  249\n",
      "Best Fitness:  0.0036305543356189396\n",
      "Avg  Fitness:  0.0034316690107540934\n",
      "Generation:  250\n",
      "Best Fitness:  0.0027965741911841965\n",
      "Avg  Fitness:  0.002700387678741373\n",
      "Generation:  251\n",
      "Best Fitness:  0.004485752368887132\n",
      "Avg  Fitness:  0.0042319092265253625\n",
      "Generation:  252\n",
      "Best Fitness:  0.0021156498958662356\n",
      "Avg  Fitness:  0.0020597066189294057\n",
      "Generation:  253\n",
      "Best Fitness:  0.012120644333479588\n",
      "Avg  Fitness:  0.010457924110622786\n",
      "Generation:  254\n",
      "Best Fitness:  0.0025848506016621755\n",
      "Avg  Fitness:  0.002541178805040779\n",
      "Generation:  255\n",
      "Best Fitness:  0.0018967979848829306\n",
      "Avg  Fitness:  0.0018776149279683867\n",
      "Generation:  256\n",
      "Best Fitness:  0.002113027135649761\n",
      "Avg  Fitness:  0.0020806383483901454\n",
      "Generation:  257\n",
      "Best Fitness:  0.0025476828218207502\n",
      "Avg  Fitness:  0.002472229488565722\n",
      "Generation:  258\n",
      "Best Fitness:  0.007333162501566235\n",
      "Avg  Fitness:  0.006887789854641821\n",
      "Generation:  259\n",
      "Best Fitness:  0.001914626009012631\n",
      "Avg  Fitness:  0.0018987730389857925\n",
      "Generation:  260\n",
      "Best Fitness:  0.0027690509534621043\n",
      "Avg  Fitness:  0.0018539357207091151\n",
      "Generation:  261\n",
      "Best Fitness:  0.010069061304235828\n",
      "Avg  Fitness:  0.008589613673597465\n",
      "Generation:  262\n",
      "Best Fitness:  0.008878196365678019\n",
      "Avg  Fitness:  0.008397086230813013\n",
      "Generation:  263\n",
      "Best Fitness:  0.0018733746702961863\n",
      "Avg  Fitness:  0.0018633534464695745\n",
      "Generation:  264\n",
      "Best Fitness:  0.002239886943402346\n",
      "Avg  Fitness:  0.002176702606283509\n",
      "Generation:  265\n",
      "Best Fitness:  0.0032301351432016904\n",
      "Avg  Fitness:  0.0032106238006562957\n",
      "Generation:  266\n",
      "Best Fitness:  0.007382011398512865\n",
      "Avg  Fitness:  0.006794646399954541\n",
      "Generation:  267\n",
      "Best Fitness:  0.0026706188786493933\n",
      "Avg  Fitness:  0.0025258805644672545\n",
      "Generation:  268\n",
      "Best Fitness:  0.0022748034359676107\n",
      "Avg  Fitness:  0.0022171809452202103\n",
      "Generation:  269\n",
      "Best Fitness:  0.005649158545558063\n",
      "Avg  Fitness:  0.005183180049892425\n",
      "Generation:  270\n",
      "Best Fitness:  0.003486523506614832\n",
      "Avg  Fitness:  0.0019453474155880811\n",
      "Generation:  271\n",
      "Best Fitness:  0.0021296410739662965\n",
      "Avg  Fitness:  0.0020668730806177094\n",
      "Generation:  272\n",
      "Best Fitness:  0.004925622756723933\n",
      "Avg  Fitness:  0.004111665782590032\n",
      "Generation:  273\n",
      "Best Fitness:  0.002682844718316489\n",
      "Avg  Fitness:  0.0025762021216335037\n",
      "Generation:  274\n",
      "Best Fitness:  0.0067281763372075976\n",
      "Avg  Fitness:  0.0060003911219520915\n",
      "Generation:  275\n",
      "Best Fitness:  0.002602322861185277\n",
      "Avg  Fitness:  0.00253541738549134\n",
      "Generation:  276\n",
      "Best Fitness:  0.0023872273089443237\n",
      "Avg  Fitness:  0.0023209635335914415\n",
      "Generation:  277\n",
      "Best Fitness:  0.002874747711835088\n",
      "Avg  Fitness:  0.0027269152722585515\n",
      "Generation:  278\n",
      "Best Fitness:  0.002613618115215389\n",
      "Avg  Fitness:  0.0025181347224206094\n",
      "Generation:  279\n",
      "Best Fitness:  0.010659716615868701\n",
      "Avg  Fitness:  0.01006680246914486\n",
      "Generation:  280\n",
      "Best Fitness:  0.013944433773207261\n",
      "Avg  Fitness:  0.012320643192114449\n",
      "Generation:  281\n",
      "Best Fitness:  0.008989284926490879\n",
      "Avg  Fitness:  0.007922718163318854\n",
      "Generation:  282\n",
      "Best Fitness:  0.0019331084398469562\n",
      "Avg  Fitness:  0.001915284010531123\n",
      "Generation:  283\n",
      "Best Fitness:  0.005008326340310354\n",
      "Avg  Fitness:  0.004576016750270439\n",
      "Generation:  284\n",
      "Best Fitness:  0.0026170978281100273\n",
      "Avg  Fitness:  0.002519931503075882\n",
      "Generation:  285\n",
      "Best Fitness:  0.003451043926446286\n",
      "Avg  Fitness:  0.003061104609866791\n",
      "Generation:  286\n",
      "Best Fitness:  0.005333445269461996\n",
      "Avg  Fitness:  0.00427776693430933\n",
      "Generation:  287\n",
      "Best Fitness:  0.0028145693159212864\n",
      "Avg  Fitness:  0.0019634692725421362\n",
      "Generation:  288\n",
      "Best Fitness:  0.0026139570464285998\n",
      "Avg  Fitness:  0.0017889249781560578\n",
      "Generation:  289\n",
      "Best Fitness:  0.003364564991904727\n",
      "Avg  Fitness:  0.002681929698641335\n",
      "Generation:  290\n",
      "Best Fitness:  0.005688751158293581\n",
      "Avg  Fitness:  0.0044799704753407206\n",
      "Generation:  291\n",
      "Best Fitness:  0.006818868455930789\n",
      "Avg  Fitness:  0.005740924127201236\n",
      "Generation:  292\n",
      "Best Fitness:  0.003927831258809408\n",
      "Avg  Fitness:  0.003581938963801671\n",
      "Generation:  293\n",
      "Best Fitness:  0.0024366286471089117\n",
      "Avg  Fitness:  0.0023726539368006654\n",
      "Generation:  294\n",
      "Best Fitness:  0.008620648135359558\n",
      "Avg  Fitness:  0.007362309647038526\n",
      "Generation:  295\n",
      "Best Fitness:  0.003888742561701297\n",
      "Avg  Fitness:  0.0035271286710320663\n",
      "Generation:  296\n",
      "Best Fitness:  0.006353444257758731\n",
      "Avg  Fitness:  0.005555716886324811\n",
      "Generation:  297\n",
      "Best Fitness:  0.002716248811731809\n",
      "Avg  Fitness:  0.002658456425095536\n",
      "Generation:  298\n",
      "Best Fitness:  0.0026921967894780003\n",
      "Avg  Fitness:  0.002578962390678463\n",
      "Generation:  299\n",
      "Best Fitness:  0.004020108115009562\n",
      "Avg  Fitness:  0.003990053399009582\n",
      "Generation:  300\n",
      "Best Fitness:  0.002745070800534876\n",
      "Avg  Fitness:  0.00173440684589728\n",
      "Generation:  301\n",
      "Best Fitness:  0.004369576116785626\n",
      "Avg  Fitness:  0.003580193226558121\n",
      "Generation:  302\n",
      "Best Fitness:  0.0024048602837954654\n",
      "Avg  Fitness:  0.002318211968756953\n",
      "Generation:  303\n",
      "Best Fitness:  0.002582628917694304\n",
      "Avg  Fitness:  0.0018831755270302052\n",
      "Generation:  304\n",
      "Best Fitness:  0.007866835208553857\n",
      "Avg  Fitness:  0.007446534110003827\n",
      "Generation:  305\n",
      "Best Fitness:  0.007914434859822522\n",
      "Avg  Fitness:  0.0070792041666627075\n",
      "Generation:  306\n",
      "Best Fitness:  0.0029665347819276145\n",
      "Avg  Fitness:  0.0028704923525742923\n",
      "Generation:  307\n",
      "Best Fitness:  0.00394757342794919\n",
      "Avg  Fitness:  0.003843042214931163\n",
      "Generation:  308\n",
      "Best Fitness:  0.008071718440672183\n",
      "Avg  Fitness:  0.007399978044052074\n",
      "Generation:  309\n",
      "Best Fitness:  0.008239106899327116\n",
      "Avg  Fitness:  0.007282742263203026\n",
      "Generation:  310\n",
      "Best Fitness:  0.0027253262778751527\n",
      "Avg  Fitness:  0.0025844168979216307\n",
      "Generation:  311\n",
      "Best Fitness:  0.017408836048112164\n",
      "Avg  Fitness:  0.016158564879932998\n",
      "Generation:  312\n",
      "Best Fitness:  0.005961276128453992\n",
      "Avg  Fitness:  0.005345969610482353\n",
      "Generation:  313\n",
      "Best Fitness:  0.005461589097023937\n",
      "Avg  Fitness:  0.004919570010328709\n",
      "Generation:  314\n",
      "Best Fitness:  0.002639776114293828\n",
      "Avg  Fitness:  0.002511622931732356\n",
      "Generation:  315\n",
      "Best Fitness:  0.0018365428876178512\n",
      "Avg  Fitness:  0.00182243519309649\n",
      "Generation:  316\n",
      "Best Fitness:  0.007507141151383164\n",
      "Avg  Fitness:  0.006814727426024843\n",
      "Generation:  317\n",
      "Best Fitness:  0.009969912210891343\n",
      "Avg  Fitness:  0.0019118938391127086\n",
      "Generation:  318\n",
      "Best Fitness:  0.005387314635900259\n",
      "Avg  Fitness:  0.0050750782491644645\n",
      "Generation:  319\n",
      "Best Fitness:  0.003826922648933381\n",
      "Avg  Fitness:  0.0035144610306781805\n",
      "Generation:  320\n",
      "Best Fitness:  0.004871795256152889\n",
      "Avg  Fitness:  0.00429205803297022\n",
      "Generation:  321\n",
      "Best Fitness:  0.03303170107854933\n",
      "Avg  Fitness:  0.029279034762175915\n",
      "Generation:  322\n",
      "Best Fitness:  0.00524220814520394\n",
      "Avg  Fitness:  0.00482528446379562\n",
      "Generation:  323\n",
      "Best Fitness:  0.008696714179049731\n",
      "Avg  Fitness:  0.007859805449958247\n",
      "Generation:  324\n",
      "Best Fitness:  0.0033834381047100105\n",
      "Avg  Fitness:  0.0032971749745927294\n",
      "Generation:  325\n",
      "Best Fitness:  0.005163187868504575\n",
      "Avg  Fitness:  0.004786096693341989\n",
      "Generation:  326\n",
      "Best Fitness:  0.00415646844835354\n",
      "Avg  Fitness:  0.003907817288380801\n",
      "Generation:  327\n",
      "Best Fitness:  0.006105847405615433\n",
      "Avg  Fitness:  0.005585066246742901\n",
      "Generation:  328\n",
      "Best Fitness:  0.0023843907785391916\n",
      "Avg  Fitness:  0.0023101844149158134\n",
      "Generation:  329\n",
      "Best Fitness:  0.006100007344415304\n",
      "Avg  Fitness:  0.0026897650430273185\n",
      "Generation:  330\n",
      "Best Fitness:  0.003080820374395428\n",
      "Avg  Fitness:  0.002882674627603902\n",
      "Generation:  331\n",
      "Best Fitness:  0.003437165203409069\n",
      "Avg  Fitness:  0.002006504818899641\n",
      "Generation:  332\n",
      "Best Fitness:  0.00481041198556563\n",
      "Avg  Fitness:  0.0039733841447416275\n",
      "Generation:  333\n",
      "Best Fitness:  0.0038760930933435\n",
      "Avg  Fitness:  0.002490358843159203\n",
      "Generation:  334\n",
      "Best Fitness:  0.0028535510051218103\n",
      "Avg  Fitness:  0.0018047704992918533\n",
      "Generation:  335\n",
      "Best Fitness:  0.0075278705233711015\n",
      "Avg  Fitness:  0.0023879270546339187\n",
      "Generation:  336\n",
      "Best Fitness:  0.003558915945603431\n",
      "Avg  Fitness:  0.002300562187722916\n",
      "Generation:  337\n",
      "Best Fitness:  0.0034872496921839794\n",
      "Avg  Fitness:  0.00249830619370407\n",
      "Generation:  338\n",
      "Best Fitness:  0.00331495577028654\n",
      "Avg  Fitness:  0.0029700111477763192\n",
      "Generation:  339\n",
      "Best Fitness:  0.011979210582079368\n",
      "Avg  Fitness:  0.010787598512060228\n",
      "Generation:  340\n",
      "Best Fitness:  0.028788629271475078\n",
      "Avg  Fitness:  0.0037491284367916614\n",
      "Generation:  341\n",
      "Best Fitness:  0.002867946705801512\n",
      "Avg  Fitness:  0.002652218197780383\n",
      "Generation:  342\n",
      "Best Fitness:  0.003734822917585373\n",
      "Avg  Fitness:  0.002893809536002675\n",
      "Generation:  343\n",
      "Best Fitness:  0.0062029967193773\n",
      "Avg  Fitness:  0.0019973594329653547\n",
      "Generation:  344\n",
      "Best Fitness:  0.004956869561417503\n",
      "Avg  Fitness:  0.0038034656830725154\n",
      "Generation:  345\n",
      "Best Fitness:  0.005904172306898471\n",
      "Avg  Fitness:  0.0049757749823183305\n",
      "Generation:  346\n",
      "Best Fitness:  0.00877273716139507\n",
      "Avg  Fitness:  0.004999112102784962\n",
      "Generation:  347\n",
      "Best Fitness:  0.005822901832542849\n",
      "Avg  Fitness:  0.0024572989360985274\n",
      "Generation:  348\n",
      "Best Fitness:  0.009020846665951184\n",
      "Avg  Fitness:  0.0032845035593116\n",
      "Generation:  349\n",
      "Best Fitness:  0.003912534764938566\n",
      "Avg  Fitness:  0.0026801631570759574\n",
      "Generation:  350\n",
      "Best Fitness:  0.0030426881319738463\n",
      "Avg  Fitness:  0.002601094909223632\n",
      "Generation:  351\n",
      "Best Fitness:  0.0072300448666706724\n",
      "Avg  Fitness:  0.005820275257473016\n",
      "Generation:  352\n",
      "Best Fitness:  0.003698320561065659\n",
      "Avg  Fitness:  0.0033741393459353875\n",
      "Generation:  353\n",
      "Best Fitness:  0.004125595092832098\n",
      "Avg  Fitness:  0.0037526316501831714\n",
      "Generation:  354\n",
      "Best Fitness:  0.008084062825773789\n",
      "Avg  Fitness:  0.007351057486604685\n",
      "Generation:  355\n",
      "Best Fitness:  0.00239141402494948\n",
      "Avg  Fitness:  0.00220894014352856\n",
      "Generation:  356\n",
      "Best Fitness:  0.0024664894970911583\n",
      "Avg  Fitness:  0.002299902181953397\n",
      "Generation:  357\n",
      "Best Fitness:  0.0023231311787740034\n",
      "Avg  Fitness:  0.0022663226428955414\n",
      "Generation:  358\n",
      "Best Fitness:  0.008697732277298474\n",
      "Avg  Fitness:  0.008388926758086453\n",
      "Generation:  359\n",
      "Best Fitness:  0.00421863956924089\n",
      "Avg  Fitness:  0.0034158655438202134\n",
      "Generation:  360\n",
      "Best Fitness:  0.003711843812234438\n",
      "Avg  Fitness:  0.003322547121482958\n",
      "Generation:  361\n",
      "Best Fitness:  0.010345454308068417\n",
      "Avg  Fitness:  0.009081077983098658\n",
      "Generation:  362\n",
      "Best Fitness:  0.004797977745180603\n",
      "Avg  Fitness:  0.00448793559175883\n",
      "Generation:  363\n",
      "Best Fitness:  0.0029731182259923926\n",
      "Avg  Fitness:  0.0017319369952935248\n",
      "Generation:  364\n",
      "Best Fitness:  0.002674026752465983\n",
      "Avg  Fitness:  0.0025703390315728407\n",
      "Generation:  365\n",
      "Best Fitness:  0.002752052369629775\n",
      "Avg  Fitness:  0.00265513214834018\n",
      "Generation:  366\n",
      "Best Fitness:  0.00239857886054457\n",
      "Avg  Fitness:  0.002326117090399254\n",
      "Generation:  367\n",
      "Best Fitness:  0.00986088766951985\n",
      "Avg  Fitness:  0.002646742106955758\n",
      "Generation:  368\n",
      "Best Fitness:  0.011010611055238947\n",
      "Avg  Fitness:  0.007137058068560655\n",
      "Generation:  369\n",
      "Best Fitness:  0.0025799586404284093\n",
      "Avg  Fitness:  0.002409728912007797\n",
      "Generation:  370\n",
      "Best Fitness:  0.009071023620756477\n",
      "Avg  Fitness:  0.008044080223033755\n",
      "Generation:  371\n",
      "Best Fitness:  0.01177734817726751\n",
      "Avg  Fitness:  0.00964559637680059\n",
      "Generation:  372\n",
      "Best Fitness:  0.010146173619254588\n",
      "Avg  Fitness:  0.00856048669513085\n",
      "Generation:  373\n",
      "Best Fitness:  0.004634355535993151\n",
      "Avg  Fitness:  0.004227303951167915\n",
      "Generation:  374\n",
      "Best Fitness:  0.007086899876969183\n",
      "Avg  Fitness:  0.006228119559180001\n",
      "Generation:  375\n",
      "Best Fitness:  0.005818019730660264\n",
      "Avg  Fitness:  0.00554569810520652\n",
      "Generation:  376\n",
      "Best Fitness:  0.004265728901716012\n",
      "Avg  Fitness:  0.00375171250769315\n",
      "Generation:  377\n",
      "Best Fitness:  0.003533217986940098\n",
      "Avg  Fitness:  0.003179775272554285\n",
      "Generation:  378\n",
      "Best Fitness:  0.005027201131935258\n",
      "Avg  Fitness:  0.004547109669667852\n",
      "Generation:  379\n",
      "Best Fitness:  0.007257942708421644\n",
      "Avg  Fitness:  0.006469588202721343\n",
      "Generation:  380\n",
      "Best Fitness:  0.006074779837619442\n",
      "Avg  Fitness:  0.0056657569459745765\n",
      "Generation:  381\n",
      "Best Fitness:  0.009088935499133288\n",
      "Avg  Fitness:  0.0076243492823088385\n",
      "Generation:  382\n",
      "Best Fitness:  0.006632418372180518\n",
      "Avg  Fitness:  0.00552936454188877\n",
      "Generation:  383\n",
      "Best Fitness:  0.005723029958724113\n",
      "Avg  Fitness:  0.0051438319741155085\n",
      "Generation:  384\n",
      "Best Fitness:  0.0017660044150110375\n",
      "Avg  Fitness:  0.0016853413340978433\n",
      "Generation:  385\n",
      "Best Fitness:  0.006416362775312911\n",
      "Avg  Fitness:  0.0043702050967800886\n",
      "Generation:  386\n",
      "Best Fitness:  0.0024184542516387723\n",
      "Avg  Fitness:  0.0022096703039178973\n",
      "Generation:  387\n",
      "Best Fitness:  0.004419028824616351\n",
      "Avg  Fitness:  0.00395917126028476\n",
      "Generation:  388\n",
      "Best Fitness:  0.003601835010330655\n",
      "Avg  Fitness:  0.003426933915080261\n",
      "Generation:  389\n",
      "Best Fitness:  0.009274649431719066\n",
      "Avg  Fitness:  0.008076289454229786\n",
      "Generation:  390\n",
      "Best Fitness:  0.00540092251471311\n",
      "Avg  Fitness:  0.004819335618760783\n",
      "Generation:  391\n",
      "Best Fitness:  0.007183640064444909\n",
      "Avg  Fitness:  0.006822464354482641\n",
      "Generation:  392\n",
      "Best Fitness:  0.006600767311972703\n",
      "Avg  Fitness:  0.0060850592696301145\n",
      "Generation:  393\n",
      "Best Fitness:  0.005477759859909863\n",
      "Avg  Fitness:  0.00523795097757315\n",
      "Generation:  394\n",
      "Best Fitness:  0.007481585212135844\n",
      "Avg  Fitness:  0.002223025557415745\n",
      "Generation:  395\n",
      "Best Fitness:  0.0034657068625815485\n",
      "Avg  Fitness:  0.003261010145206392\n",
      "Generation:  396\n",
      "Best Fitness:  0.0047052054774198646\n",
      "Avg  Fitness:  0.004335636093083707\n",
      "Generation:  397\n",
      "Best Fitness:  0.005005918194411523\n",
      "Avg  Fitness:  0.004742893263754163\n",
      "Generation:  398\n",
      "Best Fitness:  0.010388867001670205\n",
      "Avg  Fitness:  0.008856575446490563\n",
      "Generation:  399\n",
      "Best Fitness:  0.013087623523705658\n",
      "Avg  Fitness:  0.0016577211539742789\n",
      "Generation:  400\n",
      "Best Fitness:  0.010564222523697037\n",
      "Avg  Fitness:  0.007401233954403271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearGPClassifier(reg_init='z')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = 96\n",
    "MIN_LENGTH = 1\n",
    "Programlib.NUM_WREG = 8\n",
    "Programlib.NUM_CREG = 3\n",
    "clf = LinearGPClassifier()\n",
    "#clf.fit(X,y,num_generations=400, POP_SIZE=75, verbose=2)\n",
    "draw_cv_pr_curve(clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe902c6-7523-4455-b6c6-2bab04c29568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f91d560-a226-4ad0-935d-9e22e601c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Programlib\n",
    "Programlib.NUM_WREG\n",
    "Programlib.NUM_WREG = 12\n",
    "Programlib.NUM_WREG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1341c64-35d1-4670-8761-eb8ffed56af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH  = 75\n",
    "MIN_LENGTH  = 1\n",
    "classifiers = [LinearGPClassifier(inputs=len(X.values[0])) for i in range(NUM_CLASSIFIERS)]\n",
    "datasets    = [data.iloc[spl[i][1]] for i in range(NUM_CLASSIFIERS)]\n",
    "\n",
    "for i in range(len(classifiers)):\n",
    "    X_i = datasets[i].iloc[:,:-1]\n",
    "    y_i = datasets[i].iloc[:,-1]\n",
    "    classifiers[i].fit(X_i, y_i, POP_SIZE = 50, num_generations = 45, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b19ea-7ff7-4c6d-ad02-0d1865decb09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ddde6c-a05d-4dce-8b3b-d152185fb53e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d8c20-63b4-4914-b95a-71968a9abee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f123e-a8fd-43a7-af6e-a27b4ae0b6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633628a4-542b-422a-b587-477763cd99d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_classifier = LinearGPClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957b411c-1e07-4251-a210-2b0aa5071b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for row in X.values:\n",
    "    row_predictions = []\n",
    "    for classifier in classifiers:\n",
    "        p = classifier.predict_proba(X)[0][1]\n",
    "        #display(p)\n",
    "        row_predictions.append(p)\n",
    "    predictions.append(np.array(row_predictions))\n",
    "predictions = np.array(predictions)\n",
    "display(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d9535-85e4-4e7a-955d-765938c7f72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c6bfbf-2f9d-4ca0-8428-cd08204f1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, cases, y_train, labels = train_test_split(X, y, stratify=y, test_size=0.01)\n",
    "test_program = Program(reg_init='z', IS=['Add', 'Sub', 'Mul', 'Div', 'Mean', 'Copy', \\\n",
    "                                 'Sqrt', 'Sqr', 'Max', 'Min', 'Exp', 'Log', 'Lt', 'Gte', \\\n",
    "                                 'Eq', 'Neq', 'And', 'Or', 'Not', 'If'])\n",
    "f_first_layer(test_program, cases, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e83a92-fc2b-44ce-bf32-5e0f5172d111",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers[0].predict_proba(np.array([X.iloc[0].values]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18287a-96b5-4116-97b3-ad2a36062d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c531e19-66ed-44be-b246-bc2acf5ea397",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearGPClassifier()\n",
    "#clf.fit([np.array([random.choice([1,2,3,4,5,6,7,8,9]) for j in range(10)]) for i in range(500)], [random.choice([0,1]) for i in range(500)])\n",
    "cross_val_score(LinearGPClassifier(), [np.array([random.choice([1,2,3,4,5,6,7,8,9]) for j in range(10)]) for i in range(500)], [random.choice([0,1]) for i in range(500)], scoring='average_precision', cv=3)\n",
    "#clf.decision_function([np.array([random.choice([1,2,3,4,5,6,7,8,9]) for j in range(10)]) for i in range(500)])\n",
    "#clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f702d779-ccc2-48be-a6df-3493f8e2a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fro\n",
    "clf2 = xgb.XGBClassifier(use_label_encoder=False)\n",
    "clf2.fit(X,y)\n",
    "clf2.decision_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4dd887-0399-43d7-9470-ffc91d22a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[0.25155683432854725, 0.7484431656714527], [0.9995038644737649, 0.0004961355262350964], [0.8397295074775288, 0.16027049252247114], [0.9571216515275255, 0.042878348472474556], [0.9561817156213857, 0.04381828437861435], [0.3890264227058262, 0.6109735772941738], [0.11423491789163753, 0.8857650821083625], [0.29034530487939825, 0.7096546951206018], [0.9490910975123195, 0.050908902487680534], [0.18048031956347532, 0.8195196804365247], [0.8618528065103092, 0.13814719348969087], [0.32943759062580613, 0.6705624093741939], [0.7858389064188289, 0.21416109358117105], [0.18311559540360733, 0.8168844045963927], [0.0, 1.0], [0.9857410382330677, 0.014258961766932362], [0.9976240105918863, 0.002375989408113731], [0.8527152243111868, 0.14728477568881312], [0.17665886638896655, 0.8233411336110334], [0.9993818502114806, 0.0006181497885194008], [0.9533997103100513, 0.04660028968994869], [0.9882316591262795, 0.011768340873720489], [0.9290131531217625, 0.07098684687823752], [0.8048364117842723, 0.19516358821572768], [0.998534498782661, 0.001465501217338981], [0.7297445817560237, 0.2702554182439763], [0.9986324807045233, 0.0013675192954767496], [0.20007641357734973, 0.7999235864226503], [0.8101257611284022, 0.1898742388715978], [0.4657359335313661, 0.5342640664686339], [0.5843023869787718, 0.41569761302122826], [0.12177123950111446, 0.8782287604988855], [0.2739580652705661, 0.7260419347294339], [0.745771982965773, 0.25422801703422704], [0.3707893339268825, 0.6292106660731175], [0.7819715248484316, 0.21802847515156848], [0.2673155650752661, 0.7326844349247339], [0.6153789345618264, 0.3846210654381737], [0.9749219494897887, 0.02507805051021129], [0.7146838988034223, 0.28531610119657763], [0.42034520537861053, 0.5796547946213895], [0.9040696756802635, 0.09593032431973647], [0.40311183637955394, 0.596888163620446], [0.29174141686360877, 0.7082585831363912], [0.05975676382439343, 0.9402432361756066], [0.969487659367107, 0.030512340632893008], [0.9852763486823204, 0.01472365131767957], [0.9970801440394406, 0.002919855960559433], [0.9604201349725066, 0.03957986502749347], [0.9117043515795105, 0.08829564842048954], [0.9125523001616476, 0.08744769983835246], [0.3744534164065124, 0.6255465835934876], [0.5396820252265264, 0.46031797477347364], [0.7196992613544181, 0.2803007386455819], [0.45557506762741584, 0.5444249323725842], [0.9845008927150585, 0.01549910728494153], [0.8626600742958991, 0.13733992570410092], [0.11845887484614792, 0.8815411251538521], [0.6185753193671362, 0.3814246806328639], [0.8921150922357226, 0.10788490776427734], [0.7652895858418363, 0.2347104141581637], [0.9955291050249918, 0.004470894975008201], [0.7308899876960528, 0.26911001230394715], [0.9428207625115936, 0.057179237488406316], [0.9325513280803577, 0.06744867191964228], [0.9740747212896361, 0.02592527871036392], [0.9315593756444078, 0.06844062435559217], [0.7519684430907867, 0.24803155690921333], [0.8275730685243752, 0.17242693147562482], [0.7235288031884717, 0.2764711968115284], [0.20454728396949862, 0.7954527160305014], [0.700271785522504, 0.299728214477496], [0.9623696147972254, 0.03763038520277458], [0.9999691769298805, 3.0823070119449e-05], [0.46686478156125977, 0.5331352184387402], [0.8708178024026236, 0.1291821975973763], [0.9860474893566507, 0.013952510643349327], [0.5775831795254887, 0.4224168204745114], [0.043362356884214304, 0.9566376431157857], [0.5458588299804183, 0.4541411700195817], [0.990080990447599, 0.009919009552401032], [0.9815380976371963, 0.018461902362803706], [0.40788049431544693, 0.5921195056845531], [0.9904886664904183, 0.009511333509581719], [0.780116437794335, 0.219883562205665], [0.7323390815234183, 0.2676609184765817], [0.9135302198850809, 0.0864697801149191], [0.40839741625763126, 0.5916025837423687], [0.4126906266043263, 0.5873093733956737], [0.9923973874411122, 0.007602612558887865], [0.5245004301905156, 0.47549956980948443], [0.31848714034644365, 0.6815128596535563], [0.9666613179617194, 0.033338682038280554], [0.19640831003667625, 0.8035916899633238], [0.20215617284793508, 0.7978438271520649], [0.3423695449788793, 0.6576304550211207], [0.05980435830402597, 0.940195641695974], [0.9409613681215415, 0.05903863187845843], [0.9987812705098181, 0.001218729490181865], [0.9797077728272912, 0.020292227172708885], [0.7689629874481042, 0.2310370125518958], [0.41460808904478685, 0.5853919109552131], [0.04335622350813739, 0.9566437764918626], [0.8336600859516179, 0.16633991404838216], [0.0369966802038566, 0.9630033197961434], [0.899901709234466, 0.100098290765534], [0.08996276946089188, 0.9100372305391081], [0.41353383961848833, 0.5864661603815117], [0.5359638302185268, 0.46403616978147316], [0.6147558661766837, 0.3852441338233163], [0.5948700693130651, 0.40512993068693487], [0.8019257190798491, 0.19807428092015092], [0.9297898019454043, 0.07021019805459577], [0.8790745631956695, 0.12092543680433056], [0.9942035160306236, 0.005796483969376426], [0.7337872376069072, 0.2662127623930928], [0.9304141648639239, 0.06958583513607612], [0.9978770318599849, 0.002122968140015071], [0.13535671044682895, 0.864643289553171], [0.1160701457467378, 0.8839298542532622], [0.7214962995131198, 0.27850370048688017], [0.35792427257731574, 0.6420757274226843], [0.46949650873173077, 0.5305034912682692], [0.292738715213795, 0.707261284786205], [0.9935585756680334, 0.006441424331966545], [0.9999376201234359, 6.23798765640888e-05], [0.2836546853476797, 0.7163453146523203], [0.5410334460199824, 0.4589665539800177], [0.6400029838589456, 0.35999701614105445], [0.9770321371052916, 0.022967862894708473], [0.32666287342448586, 0.6733371265755141], [0.952199911738073, 0.047800088261927054], [0.7921327935583171, 0.20786720644168286], [0.9940360060044781, 0.005963993995521995], [0.7765704044552544, 0.22342959554474556], [0.9070607409545772, 0.09293925904542276], [0.25478799082208625, 0.7452120091779137], [0.2432763855810992, 0.7567236144189008], [0.9192330186713926, 0.08076698132860737], [0.873068040990796, 0.126931959009204], [0.10674532048771501, 0.893254679512285], [0.8863659544170639, 0.11363404558293612], [0.7929722061480141, 0.20702779385198591], [0.8837204818775084, 0.11627951812249161], [0.3655587812910668, 0.6344412187089332], [0.18682655723831088, 0.8131734427616891], [0.4742020555760509, 0.5257979444239491], [0.8866169229985809, 0.11338307700141911], [0.6907386307713608, 0.3092613692286392], [0.7104510011128258, 0.28954899888717417], [0.042789852740213075, 0.9572101472597869], [0.9242179386582757, 0.07578206134172423], [0.8321799192802213, 0.1678200807197787], [0.9554439581455361, 0.044556041854463976], [0.7389900279171729, 0.26100997208282706], [0.8187072673119218, 0.18129273268807827], [0.999703948531792, 0.000296051468208021], [0.900288202229145, 0.09971179777085505], [0.8398328565831754, 0.16016714341682456], [0.6778485410729811, 0.32215145892701896], [0.22856011655951647, 0.7714398834404835], [0.9537999977163786, 0.04620000228362145], [0.9767940152630324, 0.023205984736967552], [0.7551444951191117, 0.24485550488088823], [0.8828983937449062, 0.11710160625509379], [0.707558390465428, 0.292441609534572], [0.12910938165670782, 0.8708906183432922], [0.5647232969561643, 0.4352767030438357], [0.44799596235846495, 0.552004037641535], [0.4668192579220385, 0.5331807420779615], [0.15319784745075304, 0.846802152549247], [0.9046692541819847, 0.09533074581801526], [0.3658341027706081, 0.6341658972293919], [0.4407905860874559, 0.5592094139125441], [0.9620491354942634, 0.037950864505736634], [0.7406454699857016, 0.2593545300142984], [0.8609290982846097, 0.1390709017153903], [0.5674049057552677, 0.43259509424473225], [0.04892031814057307, 0.9510796818594269], [0.9550740574490056, 0.044925942550994406], [0.4372110960668131, 0.5627889039331869], [0.8917090936125882, 0.10829090638741175], [0.11933035344204312, 0.8806696465579569], [0.14874023006215276, 0.8512597699378472], [0.32682984123402903, 0.673170158765971], [0.6854988663916037, 0.3145011336083962], [0.9582994206686016, 0.04170057933139835], [0.9991308293234716, 0.0008691706765284072], [0.6042739416450127, 0.3957260583549873], [0.7675475381388897, 0.2324524618611104], [0.7893417440057068, 0.2106582559942932], [0.8357457158708426, 0.1642542841291574], [0.9439722906838025, 0.056027709316197515], [0.981939915055938, 0.018060084944062017], [0.9331407346963569, 0.06685926530364311], [0.24400477602421822, 0.7559952239757818], [0.8869937703485211, 0.1130062296514789], [0.9770081045441807, 0.0229918954558193], [0.42931285339510783, 0.5706871466048922], [0.9653073440884024, 0.0346926559115977], [0.9490335513884118, 0.05096644861158822], [0.7795751824403319, 0.22042481755966817], [0.6194897025140362, 0.3805102974859637], [0.18755938853327336, 0.8124406114667266], [0.9334119053522617, 0.06658809464773828], [0.8558074367380369, 0.14419256326196309], [0.8016114713419509, 0.19838852865804918], [0.9486603582275333, 0.05133964177246672], [0.9966763007408165, 0.0033236992591834796], [0.32702491680032875, 0.6729750831996713], [0.41459183328713833, 0.5854081667128617], [0.8748835151154419, 0.12511648488455807], [0.8073278028172153, 0.19267219718278467], [0.6712062373899191, 0.32879376261008086], [0.5717476578316955, 0.42825234216830455], [0.1832759007469047, 0.8167240992530953], [0.5171358636273563, 0.4828641363726437], [0.807323931642203, 0.1926760683577971], [0.6445160475975517, 0.35548395240244834], [0.9637731931336512, 0.03622680686634887], [0.3635857633256657, 0.6364142366743343], [0.9984416939494153, 0.0015583060505847078], [0.1622116637872003, 0.8377883362127997], [0.9616580208676542, 0.038341979132345806], [0.3688793408963066, 0.6311206591036934], [0.10787860968305485, 0.8921213903169452], [0.8204148424026421, 0.179585157597358], [0.4605796106934267, 0.5394203893065733], [0.8120284424550539, 0.18797155754494604], [0.8599330678826314, 0.1400669321173686], [0.9306317716256457, 0.06936822837435432], [0.998158234720619, 0.0018417652793809671], [0.661420454570373, 0.33857954542962704], [0.9226675888651402, 0.07733241113485981], [0.31527564958289045, 0.6847243504171096], [0.43775318484012804, 0.562246815159872], [0.9947209854640146, 0.005279014535985413], [0.9260905348741529, 0.07390946512584702], [0.4157126055432977, 0.5842873944567023], [0.7833394420072892, 0.21666055799271083], [0.8474397773514781, 0.15256022264852187], [0.9978052005153862, 0.002194799484613767], [0.42992892489408885, 0.5700710751059112], [0.6837963138248779, 0.3162036861751221], [0.9138171354147618, 0.08618286458523819], [0.7769650302323219, 0.22303496976767814], [0.967901369333936, 0.03209863066606397], [0.3985932313333793, 0.6014067686666207], [0.25570606329517076, 0.7442939367048292], [0.9111570263131337, 0.08884297368686633], [0.8007344537921374, 0.1992655462078626], [0.6237299749509284, 0.37627002504907153], [0.33233453911563904, 0.667665460884361], [0.5066032265350906, 0.49339677346490945], [0.9502492132157697, 0.04975078678423032], [0.9999632074026953, 3.67925973047334e-05], [0.7811477115914991, 0.2188522884085009], [0.3424175317065351, 0.6575824682934649], [0.8043548770412574, 0.19564512295874262], [0.7395017206505021, 0.26049827934949793], [0.36428323723872746, 0.6357167627612725], [0.9482818915401426, 0.05171810845985738], [0.12609129658611695, 0.873908703413883], [0.17453923303567775, 0.8254607669643222], [0.9185685571749641, 0.08143144282503588], [0.3682302768647765, 0.6317697231352235], [0.0996229956005501, 0.9003770043994499], [0.9876158964879659, 0.012384103512034094], [0.6715200179133025, 0.32847998208669743], [0.3789237424155053, 0.6210762575844947], [0.9738158729377295, 0.026184127062270506], [0.9339873330436685, 0.06601266695633144], [0.893161390441797, 0.10683860955820297], [0.9458216632789981, 0.054178336721001925], [0.98080570069826, 0.019194299301739996], [0.24067330216834126, 0.7593266978316587], [0.6879702532196212, 0.31202974678037876], [0.8287822399267948, 0.17121776007320527], [0.9997241065576674, 0.0002758934423325327], [0.9535494218986406, 0.0464505781013594], [0.9861069493501603, 0.013893050649839709], [0.9766371048213827, 0.023362895178617362], [0.96529723902815, 0.03470276097185001], [0.8167065222782681, 0.18329347772173193], [0.27265264169157655, 0.7273473583084235], [0.006398476779606699, 0.9936015232203933], [0.6939144738860993, 0.3060855261139007], [0.5797581158598055, 0.4202418841401945], [0.5978452590229806, 0.4021547409770194], [0.8538626761057518, 0.14613732389424816], [0.2658912824064935, 0.7341087175935065], [0.22533642803262977, 0.7746635719673702], [0.9624065021739433, 0.03759349782605672], [0.7558360247983666, 0.2441639752016334], [0.9987261719395536, 0.0012738280604464246], [0.8823251243213192, 0.11767487567868082], [0.7867472844679786, 0.21325271553202144], [0.5223544102893913, 0.47764558971060866], [0.941216163324586, 0.05878383667541402], [0.9992428684345924, 0.0007571315654075275], [0.19595775645929525, 0.8040422435407047], [0.8773693402164479, 0.12263065978355217], [0.18025120625627356, 0.8197487937437264], [0.9121832573849465, 0.0878167426150535], [0.7387363315309114, 0.2612636684690886], [0.9416932597233055, 0.05830674027669442], [0.5412986025054896, 0.4587013974945105], [0.94034567663314, 0.05965432336686], [0.07126275326128217, 0.9287372467387178], [0.23712554936212704, 0.762874450637873], [0.9748847758394896, 0.02511522416051042], [0.9977749460495898, 0.0022250539504101823], [0.988024051979488, 0.011975948020512054], [0.990039596001116, 0.009960403998884024], [0.5294679035100933, 0.4705320964899067], [0.1497319635874591, 0.8502680364125409], [0.9751734362295029, 0.024826563770497027], [0.2798173467023689, 0.7201826532976311], [0.9932614417708887, 0.006738558229111329], [0.9825642018891844, 0.01743579811081568], [0.6334383904559404, 0.36656160954405953], [0.11779634306153552, 0.8822036569384645], [0.0423452538973782, 0.9576547461026218], [0.7093490421703573, 0.2906509578296427], [0.99853643195727, 0.0014635680427299107], [0.9948772910350722, 0.0051227089649277845], [0.998262635517103, 0.0017373644828969981], [0.14412261035167362, 0.8558773896483264], [0.10474190120080351, 0.8952580987991965], [0.3921185332037729, 0.6078814667962271], [0.579690956577193, 0.420309043422807], [0.5155174125316839, 0.4844825874683161], [0.415320848658399, 0.584679151341601], [0.8832383325043951, 0.11676166749560486], [0.10606186885633484, 0.8939381311436652], [0.6094299401123857, 0.39057005988761434], [0.995528839665228, 0.0044711603347719715], [0.6426137368778225, 0.35738626312217747], [0.9746440376330557, 0.0253559623669443], [0.8060769397797691, 0.19392306022023084], [0.5249231448816418, 0.47507685511835823], [0.9159789537459604, 0.08402104625403958], [0.10154053450566136, 0.8984594654943386], [0.22365595754326884, 0.7763440424567312], [0.8552724552821089, 0.14472754471789104], [0.9890142309586194, 0.010985769041380563], [0.45774915524632687, 0.5422508447536731], [0.7099637374200536, 0.29003626257994647], [0.04145764214853176, 0.9585423578514682], [0.9732988753962913, 0.0267011246037087], [0.9571523413619762, 0.04284765863802376], [0.0433732353728451, 0.9566267646271549], [0.017924319132841426, 0.9820756808671586], [0.16887356882527904, 0.831126431174721], [0.9913222532203609, 0.008677746779639105], [0.2697761387961852, 0.7302238612038148], [0.6026384212358775, 0.3973615787641224], [0.03239155725552778, 0.9676084427444722], [0.2297710163265606, 0.7702289836734394], [0.05759643759402122, 0.9424035624059788], [0.6247516877237604, 0.3752483122762396], [0.5853743626961914, 0.41462563730380864], [0.9070049600159472, 0.09299503998405283], [0.5622522962510156, 0.4377477037489844], [0.9732303088762149, 0.026769691123785105], [0.9233846274479905, 0.07661537255200948], [0.8901881300179445, 0.10981186998205553], [0.7193377172616904, 0.2806622827383097], [0.5211513228632966, 0.4788486771367034], [0.8479750255127727, 0.1520249744872273], [0.4582162040356391, 0.5417837959643609], [0.9979446478989362, 0.002055352101063744], [0.06647032958511079, 0.9335296704148892], [0.5356499493235953, 0.4643500506764047], [0.42035493295655724, 0.5796450670434428], [0.9987289038542134, 0.0012710961457865235], [0.23836645242181365, 0.7616335475781864], [0.6855386225160749, 0.31446137748392516], [0.6928130278996808, 0.3071869721003192], [0.7212880524708141, 0.278711947529186], [0.07001019091494542, 0.9299898090850546], [0.9820740743900465, 0.01792592560995359], [0.98968825570626, 0.010311744293740025], [1.0, 0.0], [0.09920794052489179, 0.9007920594751082], [0.2843308717684514, 0.7156691282315486], [0.7419159866534017, 0.2580840133465982], [0.6044292028090891, 0.3955707971909109], [0.028024742038780248, 0.9719752579612198], [0.9988254265220778, 0.001174573477922126], [0.996329167753892, 0.003670832246107991], [0.4616222695968466, 0.5383777304031534], [0.9846630772400877, 0.015336922759912247], [0.9980560390669146, 0.0019439609330853108], [0.8707734078809445, 0.1292265921190555], [0.8416384065946262, 0.15836159340537384], [0.7971751642382385, 0.2028248357617615], [0.7928630631542916, 0.2071369368457084], [0.8039821261990638, 0.1960178738009362], [0.7187723419990293, 0.28122765800097066], [0.8483409527886197, 0.15165904721138032], [0.45814687626672046, 0.5418531237332795], [0.8939451720927752, 0.10605482790722472], [0.0016287500688686452, 0.9983712499311314], [0.6058128489942103, 0.3941871510057897], [0.9660539041276975, 0.0339460958723025], [0.9323117477043069, 0.06768825229569313], [0.995556595231475, 0.004443404768524966], [0.9725127862980874, 0.027487213701912615], [0.9680273749110448, 0.03197262508895511], [0.9081201186710939, 0.09187988132890607], [0.9527794054056834, 0.0472205945943166], [0.01604887028215507, 0.9839511297178449], [0.8801315144725516, 0.11986848552744847], [0.6083299142179941, 0.39167008578200585], [0.9935603251704082, 0.006439674829591825], [0.5045348680906027, 0.49546513190939734], [0.7015858861944162, 0.2984141138055838], [0.9798785641515221, 0.02012143584847791], [0.88079825236395, 0.11920174763604992], [0.9895564045073552, 0.01044359549264484], [0.1311272450758174, 0.8688727549241826], [0.8402962854213549, 0.15970371457864505], [0.9941653465076601, 0.0058346534923399565], [0.7740810678844559, 0.22591893211554417], [0.31894227203427417, 0.6810577279657258], [0.6091993617268294, 0.3908006382731705], [0.7563742630665652, 0.24362573693343478], [0.24352699391922428, 0.7564730060807757], [0.6531245477504048, 0.3468754522495951], [0.7286353496455704, 0.27136465035442964], [0.6456161318174618, 0.35438386818253825], [0.9957930295857587, 0.00420697041424138], [0.8548894938646227, 0.14511050613537735], [0.9804798233849706, 0.01952017661502935], [0.9740417092177958, 0.02595829078220423], [0.40904929673263435, 0.5909507032673657], [0.9925978026160088, 0.007402197383991145], [0.2746321238635322, 0.7253678761364678], [0.7487629658638797, 0.2512370341361203], [0.3012340657942534, 0.6987659342057466], [0.41043397651732794, 0.5895660234826721], [0.7878850850585022, 0.21211491494149784], [0.5250731700884885, 0.4749268299115114], [0.9998325137694523, 0.00016748623054778505], [0.9460481540700528, 0.05395184592994721], [0.9963856529761081, 0.0036143470238918004], [0.30092179421076093, 0.6990782057892391], [0.7678247054866578, 0.23217529451334223], [0.7892329443994146, 0.2107670556005854], [0.8690111924894188, 0.13098880751058123], [0.218782250583749, 0.781217749416251], [0.7338891713010876, 0.2661108286989125], [0.64841460087472, 0.35158539912528003], [0.5530798499092199, 0.4469201500907801], [0.5507516223710998, 0.4492483776289003], [0.7915743427749218, 0.20842565722507828], [0.9966864869227434, 0.00331351307725662], [0.06160507401723525, 0.9383949259827647], [0.8924210643081205, 0.10757893569187946], [0.8601490448075486, 0.13985095519245144], [0.3647252377689326, 0.6352747622310674], [0.7610836210473513, 0.23891637895264872], [0.06356408396078284, 0.9364359160392172], [0.8485098519083758, 0.15149014809162423], [0.962280271048231, 0.03771972895176895], [0.9931324954409986, 0.006867504559001414], [0.6276372692311885, 0.3723627307688115], [0.796770159349195, 0.20322984065080493], [0.20918310978917165, 0.7908168902108283], [0.6508222323963525, 0.3491777676036475], [0.7955162097623321, 0.20448379023766788], [0.44381960689365596, 0.556180393106344], [0.9334826631800546, 0.06651733681994541], [0.9995555527701813, 0.0004444472298186953], [0.07409599759145946, 0.9259040024085405], [0.992702044739011, 0.0072979552609889215], [0.7682232735617186, 0.23177672643828137], [0.5509908570753398, 0.4490091429246602], [0.33975805773838863, 0.6602419422616114], [0.22141148490765095, 0.778588515092349], [0.6550321664867702, 0.3449678335132298], [0.6504399027041926, 0.3495600972958074], [0.13193707850628045, 0.8680629214937196], [0.4872669238719307, 0.5127330761280693], [0.9881246719551856, 0.01187532804481436], [0.4113662496718743, 0.5886337503281257], [0.6876438080453081, 0.3123561919546919], [0.8239409501135122, 0.17605904988648785], [0.835540302364923, 0.16445969763507692], [0.6100158652887, 0.38998413471130006], [0.7947244488377256, 0.20527555116227436], [0.8449708782754599, 0.15502912172454014], [0.08187226090749078, 0.9181277390925092], [0.65795995007023, 0.34204004992977005], [0.1528034401862024, 0.8471965598137976], [0.6433514648184497, 0.3566485351815503], [0.5286613555651289, 0.4713386444348711], [0.25996428582645215, 0.7400357141735479], [0.20195577447579394, 0.7980442255242061]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d351d-6f0f-4cde-afcf-cb03510da250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X,y)\n",
    "lr.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35147e9-4d36-45ee-824b-3699df089d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clf.decision_function(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7edd30-ab3b-4abb-8f10-b5e0cbe3572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2e104f-ecc0-4c60-acfb-a6a1a05f20e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "check_estimator(LinearGPClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6290b50-8509-4add-af56-b9b1fd039b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.empty(0).reshape(0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab7a8eb-f887-483e-8d5e-a66449bdbddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d234fe67-02d0-4b0f-a2b5-948b1bcd0e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe89b6-6ab6-4afd-9d3a-73c54633a9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "MAX_LENGTH   = 64\n",
    "MIN_LENGTH   = 1\n",
    "clf = LinearGPClassifier()\n",
    "clf.fit(X,y,num_generations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf39545-db2d-45a5-be14-db94272ac150",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.population_[0].get_effective_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bb3327-67bb-4355-bc69-207e5ce60357",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X)\n",
    "#type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7dcae-3083-4805-85cb-5760cc7cf2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "MAX_LENGTH   = 64\n",
    "MIN_LENGTH   = 1\n",
    "POP_SIZE     = 20\n",
    "XOVER_P      = 0.09\n",
    "ELITE_K      = int(POP_SIZE*0.1)#50\n",
    "population   = [Program(reg_init='z') for i in range(POP_SIZE)] # Start with a population of 500 random programs\n",
    "best_fitness = -1                     # Start this very high since even a random program will beat it\n",
    "best_prog    = None                       # Keep track of the best program\n",
    "tolerance    = 1                          # Stop searching when the best program has a fitness less than 1 (we want to minimize error)\n",
    "generation   = 1\n",
    "\n",
    "best_fitnesses_generation = []\n",
    "avg_fitnesses_generation  = []\n",
    "\n",
    "#TODO: Consider a fitness function that encourages putting DIFFERENT functions of the input in different registers. 10_000\n",
    "#      is too big for a population and XOVER and mutation probabilities should be lowered.\n",
    "\n",
    "\n",
    "\n",
    "#logfile = open('log3.txt', 'w+')\n",
    "\n",
    "while (generation < 25):\n",
    "    X_train, cases, y_train, labels = train_test_split(X, y, stratify=y, test_size=0.01)\n",
    "    if (generation % 100 < 10):\n",
    "        dumpfile='./saved_generations_fraud/generation'+str(generation)+'.pkl'\n",
    "        with open(dumpfile, 'wb+') as du:\n",
    "            pickle.dump(population, du)\n",
    "    print('Generation: ', generation)\n",
    "    this_gen_fitness= []\n",
    "    for individual in population:\n",
    "        ind_fit = f1(individual, cases, labels)\n",
    "        this_gen_fitness.append(ind_fit)\n",
    "        if ind_fit > best_fitness:\n",
    "            print('----------NEW BEST FITNESS---------------')\n",
    "            print(ind_fit)\n",
    "            print(individual)\n",
    "            print('---------- EFFECTIVE PROGRAM ---------------')\n",
    "            individual.print_effective_program()\n",
    "            best_fitness = ind_fit\n",
    "            best_prog    = individual\n",
    "    best_fitnesses_generation.append(max(this_gen_fitness))\n",
    "    best_program_this_gen = population[np.argmax(this_gen_fitness)]\n",
    "    avg_fitnesses_generation.append(sum(this_gen_fitness)/len(this_gen_fitness))\n",
    "    # logfile.write('-----------------GENERATION ' + str(generation) + '---------------------------\\n')\n",
    "    # logfile.write(\"Fitnesses: [\" + \", \".join(str(item) for item in this_gen_fitness) + \"]\\n\")\n",
    "    # logfile.write('best fitness: ' +  str(best_fitnesses_generation[-1]) + \"\\n\")\n",
    "    # logfile.write('avg fitnesses: ' +  str(avg_fitnesses_generation[-1]) + \"\\n\")\n",
    "    print('Best Fitness: ', best_fitnesses_generation[-1])\n",
    "    print('Avg  Fitness: ', avg_fitnesses_generation[-1])\n",
    "    elite = [best_program_this_gen]#Selection.elite_selection(population, f1, CASES, k=int(POP_SIZE/10))\n",
    "    #tournament = Selection.tournament_selection(population, f1, CASES, pop_fitnesses=this_gen_fitness, next_gen_size=int((POP_SIZE*8)/10))\n",
    "   # randos     = [Program(reg_init='z', IS=['Add', 'Sub', 'Mul', 'Div', 'Mean', 'Copy', \\\n",
    "                                 # 'Sqrt', 'Sqr', 'Max', 'Min', 'Exp', 'Log', 'Lt', 'Gte', \\\n",
    "                                 # 'Eq', 'Neq', 'And', 'Or', 'Not', 'If']) for i in range(int(POP_SIZE/10))]\n",
    "        \n",
    "    next_gen=elite\n",
    "    while(len(next_gen) < POP_SIZE): #Fill the next gen with tournament selection up to 90%\n",
    "        r1 = random.randint(0, POP_SIZE-1)\n",
    "        r2 = random.randint(0, POP_SIZE-1)\n",
    "        if this_gen_fitness[r1] < this_gen_fitness[r2]:\n",
    "            next_gen.append(population[r1]._clone())\n",
    "        else:\n",
    "            next_gen.append(population[r2]._clone())\n",
    "            \n",
    "    # while(len(next_gen) < POP_SIZE):              #Fill the rest of the population with random programs\n",
    "    #     next_gen.append(Program(reg_init='z'))\n",
    "\n",
    "    for program in range(len(next_gen)):\n",
    "        mut(next_gen[program])\n",
    "        if random.random() < XOVER_P:\n",
    "            program2 = random.randint(0, len(next_gen)-1)\n",
    "            XOver(next_gen[program], next_gen[program2])\n",
    "            #logfile.write('Crossed ' + str(program) + ' and ' + str(program2) +'\\n')\n",
    "    generation += 1\n",
    "    population = next_gen\n",
    "    for program in population:\n",
    "        program.reset(new_gen=True)\n",
    "    \n",
    "            \n",
    "#logfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92443e4-0b09-4f0e-b04c-7a40c411db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7fdbe3-4f6c-4985-b4a5-c3fd80acca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_cv_pr_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a39ff9-14a2-4fc6-b3df-f24fbed759b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    print(sum(spl[i][0] == spl[i+1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e27972-6a72-4ff5-99bf-fe77b78ce092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76766aa2-36cc-4aa7-ba56-1b845bb043eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=10)\n",
    "for train, test in cv.split(X,y):\n",
    "    print(train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6ac01-90fe-4dcc-b31c-1418c7629bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof = ProfileReport(data) \n",
    "prof.to_file(output_file='output.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6752553-cac9-4940-b0f5-c518c3660e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a023109-656a-42df-8c1f-de11d4790211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  1\n",
      "[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 49982.0, -0.630110783194858, 0.692333248777891, -0.322161867819743, -2.18583158685984, 2.75971744114757, 2.94018448518302, 0.563657967318824, 0.811433062425241, -0.666525063577608, -0.576883078118525, -0.160720440372171, -0.231110022314643, -0.205396094827889, 0.410244125809745, 0.445766822043713, 0.567161736862317, -0.923636977675983, -0.45530985129445, 0.146386599725918, 0.223853799938259, -0.44848740282879, -1.49937164250269, 0.0200181261941607, 0.993663234987354, -0.0485420705673582, 0.512316895494987, 0.206047786195779, 0.145908404953964, 20.98]\n",
      "Or(9,0,0)\n",
      "\n",
      "[-0.6629111891642557, 0.0, 5553.611111111111, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 49982.0, -0.630110783194858, 0.692333248777891, -0.322161867819743, -2.18583158685984, 2.75971744114757, 2.94018448518302, 0.563657967318824, 0.811433062425241, -0.666525063577608, -0.576883078118525, -0.160720440372171, -0.231110022314643, -0.205396094827889, 0.410244125809745, 0.445766822043713, 0.567161736862317, -0.923636977675983, -0.45530985129445, 0.146386599725918, 0.223853799938259, -0.44848740282879, -1.49937164250269, 0.0200181261941607, 0.993663234987354, -0.0485420705673582, 0.512316895494987, 0.206047786195779, 0.145908404953964, 20.98]\n",
      "Mean(3,11,2)\n",
      "Sin(2,2,0)\n",
      "\n",
      "[223.56654490330166, 223.56654490330166, 0.0, 0.0, 0.36787944117144233, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 49982.0, -0.630110783194858, 0.692333248777891, -0.322161867819743, -2.18583158685984, 2.75971744114757, 2.94018448518302, 0.563657967318824, 0.811433062425241, -0.666525063577608, -0.576883078118525, -0.160720440372171, -0.231110022314643, -0.205396094827889, 0.410244125809745, 0.445766822043713, 0.567161736862317, -0.923636977675983, -0.45530985129445, 0.146386599725918, 0.223853799938259, -0.44848740282879, -1.49937164250269, 0.0200181261941607, 0.993663234987354, -0.0485420705673582, 0.512316895494987, 0.206047786195779, 0.145908404953964, 20.98]\n",
      "Exp(8,4,4)\n",
      "Sqrt(11,6,1)\n",
      "And(4,1,0)\n",
      "\n",
      "Generation:  2\n",
      "[0.7008129893959945, 0.0, 14332.722222222223, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 128994.0, -2.52485781461691, 3.55952792608604, -4.65843626315176, -0.280522958294178, 0.148759025800718, -2.95660026559362, 0.177275693425173, 0.195317946255935, 0.350727556983228, -1.05344602061992, 0.324767223703156, -0.485346668416095, -1.16486239196416, -2.87419291146152, 0.304694788164594, 1.08643297676534, 3.7328414905533, 1.49087586831627, -1.27096126885466, -0.0597461006125822, 0.686587630984263, -0.255092526985777, 0.205405304865672, 0.0751319534797114, 0.140918305927273, 0.0751380467212592, 0.103934117615914, -0.12797232955741, 0.91]\n",
      "Mean(3,11,2)\n",
      "Sin(2,2,0)\n",
      "\n",
      "Generation:  3\n",
      "[-0.8790211438997919, 0.0, 16674.5, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 150070.0, -0.263947055699462, 1.11969985488523, -0.639393996020637, -0.880566581198262, 1.19411960817965, -0.31069283978617, 0.962087036398426, -0.0888803033431148, 0.386664377614638, 0.195361925453401, 0.608261421077392, -0.0942712454254826, -0.875175806325676, -1.0275513600734, -0.748959849841511, 0.574590245469413, -0.117623468829519, 0.464547064038526, 0.178892590937878, 0.294947642440701, -0.448080854117751, -0.893010046137473, 0.0046778566623062, 0.0625545154897021, -0.347536387787768, 0.10650956381887, 0.274116803322294, -0.0362629693327005, 7.99]\n",
      "Mean(3,11,2)\n",
      "Sin(2,2,0)\n",
      "\n",
      "Generation:  4\n",
      "Generation:  5\n",
      "Generation:  6\n",
      "Generation:  7\n",
      "Generation:  8\n",
      "Generation:  9\n",
      "Generation:  10\n",
      "Generation:  11\n",
      "Generation:  12\n",
      "Generation:  13\n",
      "Generation:  14\n",
      "Generation:  15\n",
      "Generation:  16\n",
      "Generation:  17\n",
      "Generation:  18\n",
      "Generation:  19\n",
      "Generation:  20\n",
      "Generation:  21\n",
      "Generation:  22\n",
      "Generation:  23\n",
      "Generation:  24\n",
      "Generation:  25\n",
      "Generation:  26\n",
      "Generation:  27\n",
      "Generation:  28\n",
      "Generation:  29\n",
      "Generation:  30\n",
      "Generation:  31\n",
      "Generation:  32\n",
      "Generation:  33\n",
      "Generation:  34\n",
      "Generation:  35\n",
      "Generation:  36\n",
      "Generation:  37\n",
      "[352.29107283608533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 124109.0, -0.258391246511748, 0.273087627880728, 0.171431364256102, -2.20868753891195, 0.0197868347165622, -0.691829379268524, 0.253709771894109, 0.100536318086616, -0.794622125432749, -0.178992421139052, 0.733016574543952, -0.978016715728851, -1.37844388114596, -1.27139965587405, -1.09554050011982, 1.96455240499486, 0.619003823729693, -0.66239018502201, 0.238105558440816, 0.0161905060868142, -0.297104692173645, -0.968251960793141, 0.164098389477471, -0.608682202732423, -0.527671478886913, -0.791554780493737, 0.169044865584338, 0.108336177011568, 24.99]\n",
      "Sqrt(11,8,0)\n",
      "\n",
      "Generation:  38\n",
      "[267.70879701645964, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 71668.0, -0.729041937225476, 1.21667804242526, 1.94145636677607, 2.17058081334774, -0.512722241742069, 0.168023913074679, 0.026399891054304, 0.541461559324521, -0.875019279948196, -0.178952818809967, -1.13558758527313, 0.466256533000369, 0.945776463869807, -0.316998059427963, -0.780193977387154, 0.439891002579023, -0.22952792345074, -0.0419489612147777, -0.741697020622136, -0.0997902239870329, 0.208290042701949, 0.605331996471555, -0.136756261757112, 0.428979166751472, 0.162428964791138, 0.107844662546344, -0.0726732451361845, -0.0250692728191089, 30.42]\n",
      "Sqrt(11,8,0)\n",
      "\n",
      "Generation:  39\n",
      "Generation:  40\n",
      "Generation:  41\n",
      "Generation:  42\n",
      "[-105942.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 105942.0, -1.23851693030091, 1.37014195527837, 1.96436608650974, -0.410080753810923, 0.187161896450844, 0.769250509836225, -0.426906448906617, -2.40960624569898, 1.52974657570736, -0.752922864460414, 1.57712718955943, -1.99825413943126, 1.28222956720018, 1.56525091024854, -0.042851129448778, 0.0110590332915076, 0.165082985189281, 1.13134812860889, 0.865461330011529, -0.457085300080932, 2.00118892608022, -1.17643442382252, -0.0352159977713511, -0.619431870483533, 0.316860329614108, -0.697163304149831, 0.030304298711958, -0.141259354677293, 1.51]\n",
      "Div(11,8,0)\n",
      "\n",
      "Generation:  43\n",
      "Generation:  44\n",
      "Generation:  45\n",
      "Generation:  46\n",
      "Generation:  47\n",
      "Generation:  48\n",
      "Generation:  49\n",
      "Generation:  50\n",
      "Generation:  51\n",
      "Generation:  52\n",
      "Generation:  53\n",
      "Generation:  54\n",
      "Generation:  55\n",
      "Generation:  56\n",
      "Generation:  57\n",
      "Generation:  58\n",
      "Generation:  59\n",
      "Generation:  60\n",
      "Generation:  61\n",
      "Generation:  62\n",
      "Generation:  63\n",
      "Generation:  64\n",
      "Generation:  65\n",
      "Generation:  66\n",
      "Generation:  67\n",
      "Generation:  68\n",
      "Generation:  69\n",
      "Generation:  70\n",
      "Generation:  71\n",
      "Generation:  72\n",
      "Generation:  73\n",
      "Generation:  74\n",
      "Generation:  75\n",
      "Generation:  76\n",
      "Generation:  77\n",
      "Generation:  78\n",
      "Generation:  79\n",
      "Generation:  80\n",
      "Generation:  81\n",
      "Generation:  82\n",
      "Generation:  83\n",
      "Generation:  84\n",
      "Generation:  85\n",
      "Generation:  86\n",
      "Generation:  87\n",
      "Generation:  88\n",
      "Generation:  89\n",
      "Generation:  90\n",
      "[-159137.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 159137.0, -0.59289211366075, 0.770378730162656, -0.198638909334615, -0.862522413770982, 0.864270719588545, -0.34008864929094, 0.482418190356215, 0.431080830295559, -0.555154098645029, -0.702568484511673, -0.458856726592306, 0.242394639646811, -0.51648367944438, 0.793224703980915, -0.989615718077679, 0.484728930445338, -0.782310313661596, 0.254792472773721, 0.747032590077855, -0.19556805608521, -0.137353937673985, -0.602937369187713, -0.0654058116798118, -1.05062438086691, -0.621649800660754, 0.159358385053646, -0.0116896021599474, 0.0616943827263544, 1.98]\n",
      "Div(11,8,0)\n",
      "\n",
      "Generation:  91\n",
      "Generation:  92\n",
      "Generation:  93\n",
      "Generation:  94\n",
      "Generation:  95\n",
      "Generation:  96\n",
      "Generation:  97\n",
      "Generation:  98\n",
      "Generation:  99\n",
      "Generation:  100\n",
      "Generation:  101\n",
      "Generation:  102\n",
      "Generation:  103\n",
      "Generation:  104\n",
      "Generation:  105\n",
      "Generation:  106\n",
      "Generation:  107\n",
      "Generation:  108\n",
      "Generation:  109\n",
      "Generation:  110\n",
      "Generation:  111\n",
      "Generation:  112\n",
      "Generation:  113\n",
      "Generation:  114\n",
      "Generation:  115\n",
      "Generation:  116\n",
      "Generation:  117\n",
      "Generation:  118\n",
      "Generation:  119\n",
      "Generation:  120\n",
      "Generation:  121\n",
      "Generation:  122\n",
      "Generation:  123\n",
      "Generation:  124\n",
      "Generation:  125\n",
      "Generation:  126\n",
      "Generation:  127\n",
      "Generation:  128\n",
      "[-100850.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 100850.0, 1.89098125052964, -0.511744331831603, 0.143543210957879, 0.388295625107928, -0.741717880800336, 0.156131334763227, -1.007738026376, 0.0824147602868155, 2.30669684837474, -0.264496882129814, 1.66351261967756, -1.13227747317793, 2.60975895371355, 1.13901388361634, -0.68482541741591, 0.831777920058269, -0.259659624642453, 0.717705719845751, -0.188456039379117, -0.0938511587064735, -0.0534969603299952, 0.195839669007388, 0.249779032966323, -0.405848493479101, -0.606937109339663, 0.442993353448882, -0.0402866389984725, -0.0513025606990217, 39.0]\n",
      "Div(11,8,0)\n",
      "\n",
      "Generation:  129\n",
      "Generation:  130\n",
      "Generation:  131\n",
      "Generation:  132\n",
      "Generation:  133\n",
      "Generation:  134\n",
      "Generation:  135\n",
      "Generation:  136\n",
      "Generation:  137\n",
      "Generation:  138\n",
      "Generation:  139\n",
      "Generation:  140\n",
      "Generation:  141\n",
      "Generation:  142\n",
      "Generation:  143\n",
      "Generation:  144\n",
      "Generation:  145\n",
      "Generation:  146\n",
      "Generation:  147\n",
      "Generation:  148\n",
      "Generation:  149\n",
      "Generation:  150\n",
      "Generation:  151\n",
      "[-119801.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 119801.0, 2.03911550164895, 0.548359358813657, -2.38030472365601, 0.775442622917321, 0.519606362661455, -1.73851528237988, 0.331490927214751, -0.360442552448414, 0.574172520273709, -1.19459141829449, 0.171159407552368, -0.448408482829849, -0.471466730112445, -2.43974655994939, 1.16663436948959, 0.733543118801055, 1.85163814553202, 1.23618230827009, -0.767286170897159, -0.188701886663511, 0.0982148256692335, 0.472202299182925, -0.035461474955756, -0.239882104958254, 0.227829498614859, -0.0960112795563472, 0.0203768356353839, 0.0063241938676708, 1.0]\n",
      "Div(11,8,0)\n",
      "\n",
      "Generation:  152\n",
      "Generation:  153\n",
      "Generation:  154\n",
      "Generation:  155\n",
      "Generation:  156\n",
      "Generation:  157\n",
      "Generation:  158\n",
      "Generation:  159\n",
      "Generation:  160\n",
      "Generation:  161\n",
      "Generation:  162\n",
      "Generation:  163\n",
      "Generation:  164\n",
      "Generation:  165\n",
      "Generation:  166\n",
      "Generation:  167\n",
      "Generation:  168\n",
      "Generation:  169\n",
      "Generation:  170\n",
      "Generation:  171\n",
      "Generation:  172\n",
      "Generation:  173\n",
      "Generation:  174\n",
      "Generation:  175\n",
      "Generation:  176\n",
      "Generation:  177\n",
      "Generation:  178\n",
      "Generation:  179\n",
      "Generation:  180\n",
      "Generation:  181\n",
      "Generation:  182\n",
      "Generation:  183\n",
      "Generation:  184\n",
      "Generation:  185\n",
      "Generation:  186\n",
      "Generation:  187\n",
      "Generation:  188\n",
      "Generation:  189\n",
      "Generation:  190\n",
      "Generation:  191\n",
      "Generation:  192\n",
      "Generation:  193\n",
      "Generation:  194\n",
      "Generation:  195\n",
      "Generation:  196\n",
      "Generation:  197\n",
      "Generation:  198\n",
      "Generation:  199\n",
      "Generation:  200\n",
      "Generation:  201\n",
      "Generation:  202\n",
      "Generation:  203\n",
      "Generation:  204\n",
      "Generation:  205\n",
      "Generation:  206\n",
      "Generation:  207\n",
      "Generation:  208\n",
      "Generation:  209\n",
      "Generation:  210\n",
      "Generation:  211\n",
      "Generation:  212\n",
      "Generation:  213\n",
      "Generation:  214\n",
      "Generation:  215\n",
      "Generation:  216\n",
      "Generation:  217\n",
      "Generation:  218\n",
      "Generation:  219\n",
      "Generation:  220\n",
      "Generation:  221\n",
      "Generation:  222\n",
      "Generation:  223\n",
      "Generation:  224\n",
      "Generation:  225\n",
      "Generation:  226\n",
      "Generation:  227\n",
      "Generation:  228\n",
      "Generation:  229\n",
      "Generation:  230\n",
      "Generation:  231\n",
      "Generation:  232\n",
      "Generation:  233\n",
      "Generation:  234\n",
      "Generation:  235\n",
      "Generation:  236\n",
      "Generation:  237\n",
      "Generation:  238\n",
      "Generation:  239\n",
      "Generation:  240\n",
      "Generation:  241\n",
      "Generation:  242\n",
      "Generation:  243\n",
      "Generation:  244\n",
      "Generation:  245\n",
      "Generation:  246\n",
      "Generation:  247\n",
      "Generation:  248\n",
      "Generation:  249\n",
      "Generation:  250\n",
      "Generation:  251\n",
      "Generation:  252\n",
      "Generation:  253\n",
      "Generation:  254\n",
      "Generation:  255\n",
      "Generation:  256\n",
      "[-77968.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 77968.0, 1.42207056113268, -1.2147802111159, 0.810254581359268, -1.41362360269412, -1.84173402319304, -0.482911346065057, -1.36528659741808, 0.0289478656951181, -1.78604090776897, 1.60172916590918, 1.10122321885483, -0.469118103726333, 0.035231226893307, -0.204029499861697, 0.193825797451592, 0.0718660173334829, 0.149288698330141, 0.864616141137059, -0.153520600954445, -0.302964828861436, -0.0039627925251162, 0.304865807316711, -0.0225029797042526, 0.322040604605579, 0.288548758725629, -0.121305582832481, 0.0416921622618072, 0.0191729359730398, 29.6]\n",
      "Div(11,8,0)\n",
      "\n",
      "Generation:  257\n",
      "Generation:  258\n",
      "Generation:  259\n",
      "Generation:  260\n",
      "Generation:  261\n",
      "Generation:  262\n",
      "Generation:  263\n",
      "Generation:  264\n",
      "Generation:  265\n",
      "Generation:  266\n",
      "Generation:  267\n",
      "Generation:  268\n",
      "Generation:  269\n",
      "Generation:  270\n",
      "Generation:  271\n",
      "Generation:  272\n",
      "Generation:  273\n",
      "Generation:  274\n",
      "Generation:  275\n",
      "Generation:  276\n",
      "Generation:  277\n",
      "Generation:  278\n",
      "Generation:  279\n",
      "Generation:  280\n",
      "Generation:  281\n",
      "Generation:  282\n",
      "Generation:  283\n",
      "Generation:  284\n",
      "Generation:  285\n",
      "Generation:  286\n",
      "Generation:  287\n",
      "Generation:  288\n",
      "Generation:  289\n",
      "Generation:  290\n",
      "Generation:  291\n",
      "Generation:  292\n",
      "Generation:  293\n",
      "Generation:  294\n",
      "Generation:  295\n",
      "Generation:  296\n",
      "Generation:  297\n",
      "Generation:  298\n",
      "Generation:  299\n",
      "Generation:  300\n",
      "Generation:  301\n",
      "Generation:  302\n",
      "Generation:  303\n",
      "Generation:  304\n",
      "Generation:  305\n",
      "Generation:  306\n",
      "Generation:  307\n",
      "Generation:  308\n",
      "Generation:  309\n",
      "Generation:  310\n",
      "Generation:  311\n",
      "Generation:  312\n",
      "Generation:  313\n",
      "Generation:  314\n",
      "Generation:  315\n",
      "Generation:  316\n",
      "Generation:  317\n",
      "Generation:  318\n",
      "Generation:  319\n",
      "Generation:  320\n",
      "Generation:  321\n",
      "Generation:  322\n",
      "Generation:  323\n",
      "Generation:  324\n",
      "Generation:  325\n",
      "Generation:  326\n",
      "Generation:  327\n",
      "Generation:  328\n",
      "Generation:  329\n",
      "Generation:  330\n",
      "Generation:  331\n",
      "Generation:  332\n",
      "Generation:  333\n",
      "Generation:  334\n",
      "Generation:  335\n",
      "Generation:  336\n",
      "Generation:  337\n",
      "Generation:  338\n",
      "Generation:  339\n",
      "Generation:  340\n",
      "Generation:  341\n",
      "Generation:  342\n",
      "Generation:  343\n",
      "Generation:  344\n",
      "Generation:  345\n",
      "Generation:  346\n",
      "Generation:  347\n",
      "Generation:  348\n",
      "Generation:  349\n",
      "Generation:  350\n",
      "Generation:  351\n",
      "Generation:  352\n",
      "Generation:  353\n",
      "Generation:  354\n",
      "Generation:  355\n",
      "Generation:  356\n",
      "Generation:  357\n",
      "Generation:  358\n",
      "Generation:  359\n",
      "Generation:  360\n",
      "Generation:  361\n",
      "Generation:  362\n",
      "Generation:  363\n",
      "Generation:  364\n",
      "Generation:  365\n",
      "Generation:  366\n",
      "Generation:  367\n",
      "Generation:  368\n",
      "Generation:  369\n",
      "Generation:  370\n",
      "Generation:  371\n",
      "Generation:  372\n",
      "Generation:  373\n",
      "Generation:  374\n",
      "Generation:  375\n",
      "Generation:  376\n",
      "Generation:  377\n",
      "Generation:  378\n",
      "Generation:  379\n",
      "Generation:  380\n",
      "Generation:  381\n",
      "Generation:  382\n",
      "Generation:  383\n",
      "Generation:  384\n",
      "Generation:  385\n",
      "Generation:  386\n",
      "Generation:  387\n",
      "Generation:  388\n",
      "Generation:  389\n",
      "Generation:  390\n",
      "Generation:  391\n",
      "Generation:  392\n",
      "Generation:  393\n",
      "Generation:  394\n",
      "Generation:  395\n",
      "Generation:  396\n",
      "Generation:  397\n",
      "Generation:  398\n",
      "Generation:  399\n",
      "Generation:  400\n",
      "Generation:  401\n",
      "Generation:  402\n",
      "Generation:  403\n",
      "Generation:  404\n",
      "Generation:  405\n",
      "Generation:  406\n",
      "Generation:  407\n",
      "Generation:  408\n",
      "Generation:  409\n",
      "Generation:  410\n",
      "Generation:  411\n",
      "Generation:  412\n",
      "Generation:  413\n",
      "Generation:  414\n",
      "Generation:  415\n",
      "Generation:  416\n",
      "Generation:  417\n",
      "Generation:  418\n",
      "Generation:  419\n",
      "Generation:  420\n",
      "Generation:  421\n",
      "Generation:  422\n",
      "Generation:  423\n",
      "Generation:  424\n",
      "Generation:  425\n",
      "Generation:  426\n",
      "Generation:  427\n",
      "Generation:  428\n",
      "Generation:  429\n",
      "Generation:  430\n",
      "Generation:  431\n",
      "Generation:  432\n",
      "Generation:  433\n",
      "Generation:  434\n",
      "Generation:  435\n",
      "Generation:  436\n",
      "Generation:  437\n",
      "Generation:  438\n",
      "Generation:  439\n",
      "Generation:  440\n",
      "Generation:  441\n",
      "Generation:  442\n",
      "Generation:  443\n",
      "Generation:  444\n",
      "Generation:  445\n",
      "Generation:  446\n",
      "Generation:  447\n",
      "Generation:  448\n",
      "Generation:  449\n",
      "Generation:  450\n",
      "Generation:  451\n",
      "Generation:  452\n",
      "Generation:  453\n",
      "Generation:  454\n",
      "Generation:  455\n",
      "Generation:  456\n",
      "Generation:  457\n",
      "Generation:  458\n",
      "Generation:  459\n",
      "Generation:  460\n",
      "Generation:  461\n",
      "Generation:  462\n",
      "Generation:  463\n",
      "Generation:  464\n",
      "Generation:  465\n",
      "Generation:  466\n",
      "Generation:  467\n",
      "Generation:  468\n",
      "Generation:  469\n",
      "Generation:  470\n",
      "Generation:  471\n",
      "Generation:  472\n",
      "Generation:  473\n",
      "Generation:  474\n",
      "Generation:  475\n",
      "Generation:  476\n",
      "Generation:  477\n",
      "Generation:  478\n",
      "Generation:  479\n",
      "Generation:  480\n",
      "Generation:  481\n",
      "Generation:  482\n",
      "Generation:  483\n",
      "Generation:  484\n",
      "Generation:  485\n",
      "Generation:  486\n",
      "Generation:  487\n",
      "Generation:  488\n",
      "Generation:  489\n",
      "Generation:  490\n",
      "Generation:  491\n",
      "Generation:  492\n",
      "Generation:  493\n",
      "Generation:  494\n",
      "Generation:  495\n",
      "Generation:  496\n",
      "Generation:  497\n",
      "Generation:  498\n",
      "Generation:  499\n",
      "Generation:  500\n",
      "Generation:  1\n",
      "[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 76686.0, 1.1344067170605, -0.152363418644002, 0.75520299568978, 0.392481990011095, -0.684137519250901, -0.219545386357259, -0.416224603681302, 0.0964483660986151, 0.172512995101221, 0.0570309512134252, 1.0742700594689, 0.73487637112151, 0.0357878869925793, 0.25882734773294, 0.662589131799507, 0.885764685963097, -0.893865628230584, 0.47212526393443, 0.112845514471254, 0.0119762350550479, -0.0387550470486631, -0.237845043931808, 0.0111727418149842, 0.0216359303705482, 0.133708804446919, 0.318382129962704, -0.0269309234667544, 0.0178966116690727, 44.98]\n",
      "Neq(8,0,0)\n",
      "\n",
      "[76686.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 76686.0, 1.1344067170605, -0.152363418644002, 0.75520299568978, 0.392481990011095, -0.684137519250901, -0.219545386357259, -0.416224603681302, 0.0964483660986151, 0.172512995101221, 0.0570309512134252, 1.0742700594689, 0.73487637112151, 0.0357878869925793, 0.25882734773294, 0.662589131799507, 0.885764685963097, -0.893865628230584, 0.47212526393443, 0.112845514471254, 0.0119762350550479, -0.0387550470486631, -0.237845043931808, 0.0111727418149842, 0.0216359303705482, 0.133708804446919, 0.318382129962704, -0.0269309234667544, 0.0178966116690727, 44.98]\n",
      "Copy(11,8,0)\n",
      "\n",
      "Generation:  2\n",
      "[-74099.5, 0.5, 0.0, 0.0, 0.0, 272.21315177632397, -74099.5, 0.8936478509869139, -1.0, 1.0, 0.5, 74100.0, 1.0857982203845, -0.146481243933281, 1.13738491677051, 1.10163410192804, -0.579804378608319, 0.783539071200756, -0.746221385159956, 0.398286138808294, 0.652903196564198, -0.0770756737687923, 0.83336213429845, 1.13906374478414, -0.0294313541803363, -0.150721020711131, -0.244103275332666, 0.0787188857563573, -0.33883517548645, 0.0934976023904112, -0.157072780116027, -0.140734582988156, -0.0035814217906839, 0.190577212700372, -0.0294098276791777, -0.298068405426592, 0.35453890224423, -0.376810946405619, 0.0841978577830167, 0.0217561278387144, 11.5]\n",
      "Div(10,9,1)\n",
      "Sqrt(11,9,5)\n",
      "Sin(5,8,7)\n",
      "Sub(1,11,6)\n",
      "Min(6,8,0)\n",
      "\n",
      "Generation:  3\n",
      "Generation:  4\n",
      "Generation:  5\n",
      "Generation:  6\n",
      "[-163379.5205744614, 0.479425538604203, 0.0, 0.0, 0.0, 0.0, -163379.5205744614, 0.0, -1.0, 1.0, 0.5, 163380.0, 1.60036856572599, -0.545799598426502, -1.94359366837075, 0.457066120905466, 0.160001781320794, -0.649794946721082, 0.339490983285715, -0.200870167734523, 0.624019468018782, -0.854091331311027, 1.16798324553601, 1.36621539302724, 0.741247186485957, -1.53974544428592, -0.818812139264545, 0.241704116126956, 0.835198617080062, 0.51941642296205, 0.372672687538341, 0.302523568109685, -0.085298015911199, -0.420804315361664, -0.0512266158819478, -0.488545290008457, -0.109798823803771, -0.107157900836865, -0.0384023928902596, 0.005099082091298, 212.36]\n",
      "Mul(3,3,4)\n",
      "Mean(5,0,7)\n",
      "Sin(10,9,1)\n",
      "Sub(1,11,6)\n",
      "Min(6,8,0)\n",
      "\n",
      "Generation:  7\n",
      "Generation:  8\n",
      "Generation:  9\n",
      "Generation:  10\n",
      "Generation:  11\n",
      "Generation:  12\n",
      "[-0.08013888361213128, 0.7117228383573231, 0.0, 0.0, 0.0, 0.0, -155295.28827716166, 0.0, -1.0, 1.0, 0.5, 155296.0, -0.461529850340035, -0.114868572242679, 0.9460329179997, -1.04218376350702, -0.923726574282403, -0.677484063028037, -0.707766943680548, 0.460377115473907, -0.914187571743674, 0.0538495557475767, -1.10234620986066, -1.11765702182867, -0.651580335121315, 0.0740576457381478, 0.53210093753276, 0.897251732156365, 0.732612497485329, -1.13916614602403, 1.55356064522927, 0.112406030156373, 0.13865458091558, 0.112768789964651, 0.189262348860349, -0.050666933439557, -1.01593701589773, -0.382981976305664, 0.0980401115010905, 0.0908480129186887, 20.97]\n",
      "Copy(11,8,0)\n",
      "Sin(0,3,1)\n",
      "Sub(1,11,6)\n",
      "Sin(6,8,0)\n",
      "\n",
      "Generation:  13\n",
      "Generation:  14\n",
      "Generation:  15\n",
      "Generation:  16\n",
      "Generation:  17\n",
      "Generation:  18\n",
      "Generation:  19\n",
      "Generation:  20\n",
      "Generation:  21\n",
      "Generation:  22\n",
      "Generation:  23\n",
      "Generation:  24\n",
      "Generation:  25\n",
      "Generation:  26\n",
      "Generation:  27\n",
      "Generation:  28\n",
      "Generation:  29\n",
      "Generation:  30\n",
      "Generation:  31\n",
      "Generation:  32\n",
      "Generation:  33\n",
      "Generation:  34\n",
      "Generation:  35\n",
      "Generation:  36\n",
      "Generation:  37\n",
      "Generation:  38\n",
      "Generation:  39\n",
      "Generation:  40\n",
      "Generation:  41\n",
      "Generation:  42\n",
      "Generation:  43\n",
      "Generation:  44\n",
      "Generation:  45\n",
      "Generation:  46\n",
      "Generation:  47\n",
      "Generation:  48\n",
      "Generation:  49\n",
      "Generation:  50\n",
      "Generation:  51\n",
      "[-28718.0, 0.0, 0.0, 0.0, 0.0, 0.0, -28718.0, 0.0, -1.0, 1.0, 0.5, 28718.0, 0.941625686564232, -0.905944799149134, 0.899453446007551, 0.742129524480986, -1.33704750684726, -0.0666046571881063, -0.58414351229081, 0.0190511610831789, -0.542451956381439, 0.571850263961489, -0.34687609656337, 0.415487030458344, 0.597628386991921, -0.2563844082303, 0.515666685701299, -1.83663510884381, 0.318302713612351, 0.849527015505756, -1.6461347634126, -0.258665259153395, -0.098943602609585, 0.0716736169049148, -0.118748679544238, 0.397366864879008, 0.366306772872548, -0.229839045501869, 0.0692645516903708, 0.0608394472070436, 158.0]\n",
      "Sqr(7,1,1)\n",
      "Add(5,2,4)\n",
      "And(4,8,7)\n",
      "Sub(1,11,6)\n",
      "Min(6,8,0)\n",
      "\n",
      "Generation:  52\n",
      "Generation:  53\n",
      "Generation:  54\n",
      "Generation:  55\n",
      "Generation:  56\n",
      "Generation:  57\n",
      "Generation:  58\n",
      "Generation:  59\n",
      "Generation:  60\n",
      "Generation:  61\n",
      "Generation:  62\n",
      "Generation:  63\n",
      "Generation:  64\n",
      "Generation:  65\n",
      "Generation:  66\n",
      "Generation:  67\n",
      "Generation:  68\n",
      "Generation:  69\n",
      "Generation:  70\n",
      "Generation:  71\n",
      "Generation:  72\n",
      "Generation:  73\n",
      "Generation:  74\n",
      "Generation:  75\n",
      "Generation:  76\n",
      "Generation:  77\n",
      "Generation:  78\n",
      "Generation:  79\n",
      "[-112968.29289321881, 0.7071067811865476, 0.0, 0.0, 0.0, 0.0, -112968.29289321881, 0.0, -1.0, 1.0, 0.5, 112969.0, -0.837556385549654, 0.556235474101475, 1.35199453709338, 0.0624772122932835, 1.00074668219529, -0.963512458682764, 0.744881396641868, -0.197956629289141, -0.850990036641389, -0.603698513471718, -0.54781267838016, 0.53534882316002, 1.11950068118252, 0.0595161150879334, 0.381764917337068, -0.706712169741188, 0.138156513532649, -0.778727612499645, 1.21355210159766, 0.285547911627909, -0.216078428958937, -0.67439893251128, -0.185050135427723, 0.0332058817247675, 0.393665013146231, 0.501031218204705, -0.020842133048868, 0.0648566260743742, 6.44]\n",
      "Log(7,0,7)\n",
      "Sqrt(10,3,1)\n",
      "Sub(1,11,6)\n",
      "Min(6,8,0)\n",
      "\n",
      "Generation:  80\n",
      "Generation:  81\n",
      "Generation:  82\n",
      "Generation:  83\n",
      "Generation:  84\n",
      "Generation:  85\n",
      "Generation:  86\n",
      "Generation:  87\n",
      "Generation:  88\n",
      "Generation:  89\n",
      "Generation:  90\n",
      "Generation:  91\n",
      "Generation:  92\n",
      "Generation:  93\n",
      "Generation:  94\n",
      "Generation:  95\n",
      "Generation:  96\n",
      "Generation:  97\n",
      "Generation:  98\n",
      "Generation:  99\n",
      "Generation:  100\n",
      "Generation:  101\n",
      "Generation:  102\n",
      "Generation:  103\n",
      "Generation:  104\n",
      "Generation:  105\n",
      "Generation:  106\n",
      "Generation:  107\n",
      "Generation:  108\n",
      "Generation:  109\n",
      "Generation:  110\n",
      "Generation:  111\n",
      "Generation:  112\n",
      "Generation:  113\n",
      "Generation:  114\n",
      "Generation:  115\n",
      "Generation:  116\n",
      "Generation:  117\n",
      "Generation:  118\n",
      "Generation:  119\n",
      "Generation:  120\n",
      "Generation:  121\n",
      "Generation:  122\n",
      "Generation:  123\n",
      "Generation:  124\n",
      "Generation:  125\n",
      "Generation:  126\n",
      "Generation:  127\n",
      "Generation:  128\n",
      "[-10521.5, 0.5, 0.0, 0.0, 0.0, 0.0, -10521.5, 0.0, -1.0, 1.0, 0.5, 10522.0, -2.16818142269719, -0.55838149748535, 3.59090079443469, -0.466456286966178, 0.194533788046814, 1.43901489466946, -0.638701377301096, 0.0912164353422951, 3.53814217011026, 0.136272906125416, 1.95649735327859, -1.23720538657786, 1.6318813162221, -0.738192045411053, -2.6489670308968, -0.918877406783463, 0.879083834052963, -0.504181540305394, -0.641911547134633, 0.309736481796612, -0.249841317438866, 0.83293166031305, -0.3541740995236, -0.212842788996539, 0.229146898100503, 1.09781153668763, -0.442648631758717, 0.0626923867207605, 2.21]\n",
      "Sqrt(3,3,7)\n",
      "Add(10,3,1)\n",
      "Sub(1,11,6)\n",
      "Min(6,8,0)\n",
      "\n",
      "Generation:  129\n",
      "Generation:  130\n",
      "Generation:  131\n",
      "Generation:  132\n",
      "Generation:  133\n",
      "Generation:  134\n",
      "Generation:  135\n",
      "Generation:  136\n",
      "Generation:  137\n",
      "Generation:  138\n",
      "Generation:  139\n",
      "Generation:  140\n",
      "Generation:  141\n",
      "Generation:  142\n",
      "Generation:  143\n",
      "Generation:  144\n",
      "Generation:  145\n",
      "Generation:  146\n",
      "Generation:  147\n",
      "Generation:  148\n",
      "Generation:  149\n",
      "Generation:  150\n",
      "Generation:  151\n",
      "Generation:  152\n",
      "Generation:  153\n",
      "Generation:  154\n",
      "Generation:  155\n",
      "Generation:  156\n",
      "Generation:  157\n",
      "Generation:  158\n",
      "Generation:  159\n",
      "Generation:  160\n",
      "Generation:  161\n",
      "Generation:  162\n",
      "Generation:  163\n",
      "Generation:  164\n",
      "Generation:  165\n",
      "Generation:  166\n",
      "Generation:  167\n",
      "Generation:  168\n",
      "Generation:  169\n",
      "Generation:  170\n",
      "Generation:  171\n",
      "Generation:  172\n",
      "Generation:  173\n",
      "Generation:  174\n",
      "Generation:  175\n",
      "Generation:  176\n",
      "Generation:  177\n",
      "Generation:  178\n",
      "Generation:  179\n",
      "Generation:  180\n",
      "Generation:  181\n",
      "Generation:  182\n",
      "Generation:  183\n",
      "Generation:  184\n",
      "Generation:  185\n",
      "Generation:  186\n",
      "Generation:  187\n",
      "Generation:  188\n",
      "Generation:  189\n",
      "Generation:  190\n",
      "Generation:  191\n",
      "Generation:  192\n",
      "Generation:  193\n",
      "Generation:  194\n",
      "Generation:  195\n",
      "Generation:  196\n",
      "Generation:  197\n",
      "Generation:  198\n",
      "Generation:  199\n",
      "Generation:  200\n",
      "Generation:  201\n",
      "Generation:  202\n",
      "Generation:  203\n",
      "Generation:  204\n",
      "Generation:  205\n",
      "Generation:  206\n",
      "Generation:  207\n",
      "Generation:  208\n",
      "Generation:  209\n",
      "Generation:  210\n",
      "Generation:  211\n",
      "Generation:  212\n",
      "Generation:  213\n",
      "Generation:  214\n",
      "Generation:  215\n",
      "Generation:  216\n",
      "Generation:  217\n",
      "Generation:  218\n",
      "Generation:  219\n",
      "Generation:  220\n",
      "Generation:  221\n",
      "Generation:  222\n",
      "Generation:  223\n",
      "Generation:  224\n",
      "Generation:  225\n",
      "Generation:  226\n",
      "Generation:  227\n",
      "Generation:  228\n",
      "Generation:  229\n",
      "Generation:  230\n",
      "Generation:  231\n",
      "Generation:  232\n",
      "Generation:  233\n",
      "Generation:  234\n",
      "Generation:  235\n",
      "Generation:  236\n",
      "Generation:  237\n",
      "Generation:  238\n",
      "Generation:  239\n",
      "Generation:  240\n",
      "Generation:  241\n",
      "Generation:  242\n",
      "Generation:  243\n",
      "Generation:  244\n",
      "Generation:  245\n",
      "Generation:  246\n",
      "Generation:  247\n",
      "Generation:  248\n",
      "Generation:  249\n",
      "Generation:  250\n",
      "Generation:  251\n",
      "Generation:  252\n",
      "Generation:  253\n",
      "Generation:  254\n",
      "Generation:  255\n",
      "Generation:  256\n",
      "Generation:  257\n",
      "Generation:  258\n",
      "Generation:  259\n",
      "Generation:  260\n",
      "Generation:  261\n",
      "Generation:  262\n",
      "Generation:  263\n",
      "Generation:  264\n",
      "Generation:  265\n",
      "Generation:  266\n",
      "Generation:  267\n",
      "Generation:  268\n",
      "Generation:  269\n",
      "Generation:  270\n",
      "Generation:  271\n",
      "Generation:  272\n",
      "Generation:  273\n",
      "Generation:  274\n",
      "Generation:  275\n",
      "Generation:  276\n",
      "Generation:  277\n",
      "Generation:  278\n",
      "Generation:  279\n",
      "Generation:  280\n",
      "Generation:  281\n",
      "Generation:  282\n",
      "Generation:  283\n",
      "Generation:  284\n",
      "Generation:  285\n",
      "Generation:  286\n",
      "Generation:  287\n",
      "Generation:  288\n",
      "Generation:  289\n",
      "Generation:  290\n",
      "Generation:  291\n",
      "Generation:  292\n",
      "Generation:  293\n",
      "Generation:  294\n",
      "Generation:  295\n",
      "Generation:  296\n",
      "Generation:  297\n",
      "Generation:  298\n",
      "Generation:  299\n",
      "Generation:  300\n",
      "Generation:  301\n",
      "Generation:  302\n",
      "Generation:  303\n",
      "Generation:  304\n",
      "Generation:  305\n",
      "Generation:  306\n",
      "Generation:  307\n",
      "Generation:  308\n",
      "Generation:  309\n",
      "Generation:  310\n",
      "Generation:  311\n",
      "Generation:  312\n",
      "Generation:  313\n",
      "Generation:  314\n",
      "Generation:  315\n",
      "Generation:  316\n",
      "Generation:  317\n",
      "Generation:  318\n",
      "Generation:  319\n",
      "Generation:  320\n",
      "Generation:  321\n",
      "Generation:  322\n",
      "Generation:  323\n",
      "Generation:  324\n",
      "Generation:  325\n",
      "Generation:  326\n",
      "Generation:  327\n",
      "Generation:  328\n",
      "Generation:  329\n",
      "Generation:  330\n",
      "Generation:  331\n",
      "Generation:  332\n",
      "Generation:  333\n",
      "Generation:  334\n",
      "Generation:  335\n",
      "Generation:  336\n",
      "Generation:  337\n",
      "Generation:  338\n",
      "Generation:  339\n",
      "Generation:  340\n",
      "Generation:  341\n",
      "Generation:  342\n",
      "Generation:  343\n",
      "Generation:  344\n",
      "Generation:  345\n",
      "Generation:  346\n",
      "Generation:  347\n",
      "Generation:  348\n",
      "Generation:  349\n",
      "Generation:  350\n",
      "Generation:  351\n",
      "Generation:  352\n",
      "Generation:  353\n",
      "Generation:  354\n",
      "Generation:  355\n",
      "Generation:  356\n",
      "Generation:  357\n",
      "Generation:  358\n",
      "Generation:  359\n",
      "Generation:  360\n",
      "Generation:  361\n",
      "Generation:  362\n",
      "Generation:  363\n",
      "Generation:  364\n",
      "Generation:  365\n",
      "Generation:  366\n",
      "Generation:  367\n",
      "Generation:  368\n",
      "Generation:  369\n",
      "Generation:  370\n",
      "Generation:  371\n",
      "Generation:  372\n",
      "Generation:  373\n",
      "Generation:  374\n",
      "Generation:  375\n",
      "Generation:  376\n",
      "Generation:  377\n",
      "Generation:  378\n",
      "Generation:  379\n",
      "Generation:  380\n",
      "Generation:  381\n",
      "Generation:  382\n",
      "Generation:  383\n",
      "Generation:  384\n",
      "Generation:  385\n",
      "Generation:  386\n",
      "Generation:  387\n",
      "Generation:  388\n",
      "Generation:  389\n",
      "Generation:  390\n",
      "Generation:  391\n",
      "Generation:  392\n",
      "Generation:  393\n",
      "Generation:  394\n",
      "Generation:  395\n",
      "Generation:  396\n",
      "Generation:  397\n",
      "Generation:  398\n",
      "Generation:  399\n",
      "Generation:  400\n",
      "Generation:  401\n",
      "Generation:  402\n",
      "Generation:  403\n",
      "Generation:  404\n",
      "Generation:  405\n",
      "Generation:  406\n",
      "Generation:  407\n",
      "Generation:  408\n",
      "Generation:  409\n",
      "Generation:  410\n",
      "Generation:  411\n",
      "Generation:  412\n",
      "Generation:  413\n",
      "Generation:  414\n",
      "Generation:  415\n",
      "Generation:  416\n",
      "Generation:  417\n",
      "Generation:  418\n",
      "Generation:  419\n",
      "Generation:  420\n",
      "Generation:  421\n",
      "Generation:  422\n",
      "Generation:  423\n",
      "Generation:  424\n",
      "Generation:  425\n",
      "Generation:  426\n",
      "Generation:  427\n",
      "Generation:  428\n",
      "Generation:  429\n",
      "Generation:  430\n",
      "Generation:  431\n",
      "Generation:  432\n",
      "Generation:  433\n",
      "Generation:  434\n",
      "Generation:  435\n",
      "Generation:  436\n",
      "Generation:  437\n",
      "Generation:  438\n",
      "Generation:  439\n",
      "Generation:  440\n",
      "Generation:  441\n",
      "Generation:  442\n",
      "Generation:  443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  445\n",
      "Generation:  446\n",
      "Generation:  447\n",
      "Generation:  448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:62: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return (self._saturate(register_set[self.op1] / register_set[self.op2]), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  452\n",
      "Generation:  453\n",
      "Generation:  454\n",
      "Generation:  455\n",
      "Generation:  456\n",
      "Generation:  457\n",
      "Generation:  458\n",
      "Generation:  459\n",
      "Generation:  460\n",
      "Generation:  461\n",
      "Generation:  462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  463\n",
      "Generation:  464\n",
      "Generation:  465\n",
      "Generation:  466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:62: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return (self._saturate(register_set[self.op1] / register_set[self.op2]), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  467\n",
      "Generation:  468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:62: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return (self._saturate(register_set[self.op1] / register_set[self.op2]), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  469\n",
      "Generation:  470\n",
      "Generation:  471\n",
      "Generation:  472\n",
      "Generation:  473\n",
      "Generation:  474\n",
      "Generation:  475\n",
      "Generation:  476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:62: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return (self._saturate(register_set[self.op1] / register_set[self.op2]), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  477\n",
      "Generation:  478\n",
      "Generation:  479\n",
      "Generation:  480\n",
      "Generation:  481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:62: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return (self._saturate(register_set[self.op1] / register_set[self.op2]), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:62: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return (self._saturate(register_set[self.op1] / register_set[self.op2]), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  483\n",
      "Generation:  484\n",
      "Generation:  485\n",
      "Generation:  486\n",
      "Generation:  487\n",
      "Generation:  488\n",
      "Generation:  489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  490\n",
      "Generation:  491\n",
      "Generation:  492\n",
      "Generation:  493\n",
      "Generation:  494\n",
      "Generation:  495\n",
      "Generation:  496\n",
      "Generation:  497\n",
      "Generation:  498\n",
      "Generation:  499\n",
      "Generation:  500\n",
      "Generation:  1\n",
      "[-1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 71972.0, 1.29399321440103, -1.23557121628205, 0.285374293596513, -1.25120443474076, -1.55733328375307, -0.910243648863863, -0.72044899448556, -0.12136857469613, -1.87451914572247, 1.38693490424636, -0.0761351963366209, -1.38472530289395, -1.10996954682825, 0.230188444167171, 0.899780437936315, -0.738002048912616, 1.04638861057209, -0.93607937296588, -0.474350019658017, -0.296889768476051, -0.562144924879169, -1.55403092278352, 0.303840830633338, 0.281881750793562, -0.111882486208564, -0.636578415201566, 0.0084691185723373, 0.0364476057180747, 99.95]\n",
      "Sin(6,9,7)\n",
      "Min(5,8,0)\n",
      "\n",
      "[-71971.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 71972.0, 1.29399321440103, -1.23557121628205, 0.285374293596513, -1.25120443474076, -1.55733328375307, -0.910243648863863, -0.72044899448556, -0.12136857469613, -1.87451914572247, 1.38693490424636, -0.0761351963366209, -1.38472530289395, -1.10996954682825, 0.230188444167171, 0.899780437936315, -0.738002048912616, 1.04638861057209, -0.93607937296588, -0.474350019658017, -0.296889768476051, -0.562144924879169, -1.55403092278352, 0.303840830633338, 0.281881750793562, -0.111882486208564, -0.636578415201566, 0.0084691185723373, 0.0364476057180747, 99.95]\n",
      "Sub(10,11,0)\n",
      "\n",
      "Generation:  2\n",
      "[-141049.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 141050.0, 2.07307253639755, -0.380567923255683, -1.5179862594975, 0.404982453480475, 0.0606188313913962, -0.55097977705581, -0.0265115493998686, -0.158574806568979, 1.10991929378901, 0.0682185973067526, -1.98997570838334, -0.643455352411853, -1.50521577274641, 0.321659696767552, -0.396134471625885, -0.291227217875232, -0.168332511903218, -0.207421155859827, 0.349821034352693, -0.291002785692985, 0.0725858686348982, 0.436467569058014, -0.0790848270368641, -0.724584782598292, 0.235000050942449, 0.779647145724403, -0.0783845861453775, -0.0810324588223229, 17.52]\n",
      "Sub(10,11,0)\n",
      "\n",
      "Generation:  3\n",
      "Generation:  4\n",
      "Generation:  5\n",
      "Generation:  6\n",
      "Generation:  7\n",
      "Generation:  8\n",
      "Generation:  9\n",
      "Generation:  10\n",
      "Generation:  11\n",
      "Generation:  12\n",
      "Generation:  13\n",
      "[-26755.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 26756.0, -0.952054883874585, 1.14900223054326, 0.0310341379054624, 1.27327272124303, 0.387543053815945, -1.32113210057107, 0.464109255414707, 0.0447095023009586, -0.187964097923794, -0.36894586946498, -0.144273738853683, -0.764272486556056, -0.817523843641326, -1.2657537713955, 1.33749953784789, 0.355398482250739, 1.18972191329463, 0.924297209947587, 0.126833778749643, -0.14038753968783, -0.0662469263183737, 0.153175911642807, 0.292474776235785, 0.214150602369574, -0.0389114014736561, -0.338271374916069, -0.130484787558118, -0.186793690113777, 1.0]\n",
      "Sub(10,11,0)\n",
      "\n",
      "Generation:  14\n",
      "[10.828956039676049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.828956039676049, -1.0, 1.0, 0.5, 50461.0, 1.14618816114855, 0.398537707145029, 0.635945537260293, 1.31068618708798, -0.299731712895834, -0.686599715840811, 0.156787560996559, -0.169489516296943, -0.245844961242787, -0.0181737162943296, 0.495300495538451, 1.10439177455963, 1.19406965381103, 0.136972714845739, 1.01069454761004, -0.357667319070967, -0.0726237641985889, -0.847589344199663, -1.04405536255065, -0.101589050390601, 0.150044252019389, 0.568060490172436, -0.033025884130527, 0.645078265458418, 0.570959447267923, -0.288427338762127, 0.0430096576254922, 0.0274128680878719, 8.36]\n",
      "Sub(10,11,0)\n",
      "Sub(0,5,7)\n",
      "Not(7,4,6)\n",
      "Sin(6,10,5)\n",
      "Log(11,8,7)\n",
      "Max(5,9,0)\n",
      "\n",
      "Generation:  15\n",
      "Generation:  16\n",
      "Generation:  17\n",
      "Generation:  18\n",
      "Generation:  19\n",
      "Generation:  20\n",
      "[-32566.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 32567.0, -0.898904635555521, 1.06567731292579, 0.262551935637097, -0.32348905858634, -0.720074507888939, -0.83882433114771, 1.14350408214634, 0.186171698647251, -0.107719466309867, -0.661512634681485, -1.14152564725371, -0.350029038553811, -1.14034183846674, 0.510062757099919, -0.594757895503663, 0.0207108329555091, -0.110820877416139, -0.364673099540656, -0.504593084352824, -0.217054256388342, 0.0753114450145278, 0.243757635580363, 0.0473415383719432, 0.404714945639717, -0.375755702836088, 0.288083636516296, 0.139013152581625, 0.132966599183332, 135.92]\n",
      "Sub(10,11,0)\n",
      "\n",
      "Generation:  21\n",
      "Generation:  22\n",
      "Generation:  23\n",
      "Generation:  24\n",
      "Generation:  25\n",
      "Generation:  26\n",
      "Generation:  27\n",
      "Generation:  28\n",
      "Generation:  29\n",
      "Generation:  30\n",
      "Generation:  31\n",
      "Generation:  32\n",
      "Generation:  33\n",
      "Generation:  34\n",
      "Generation:  35\n",
      "Generation:  36\n",
      "Generation:  37\n",
      "Generation:  38\n",
      "Generation:  39\n",
      "Generation:  40\n",
      "Generation:  41\n",
      "Generation:  42\n",
      "Generation:  43\n",
      "Generation:  44\n",
      "Generation:  45\n",
      "Generation:  46\n",
      "Generation:  47\n",
      "Generation:  48\n",
      "Generation:  49\n",
      "Generation:  50\n",
      "Generation:  51\n",
      "Generation:  52\n",
      "Generation:  53\n",
      "Generation:  54\n",
      "Generation:  55\n",
      "Generation:  56\n",
      "Generation:  57\n",
      "Generation:  58\n",
      "[0.49804875026322065, 1.0, 0.9960975005264413, 0.5, 0.0, 1.0, 0.5, -25583.648149169654, -1.0, 1.0, 0.5, 127932.0, 0.0168650719904868, 0.780158198614606, 0.145877112272178, -0.799332061390601, 0.692021949842966, -0.501515658050637, 0.861073371632165, 0.0014311949261683, -0.252852054897354, -0.242478670072838, 0.438841828553057, 0.632747379108443, -0.16501289866711, 0.32199285949864, -1.00980490298749, 0.167567432678884, -0.73869509847875, -0.19321475982597, 0.285764717235108, -0.0330878665771328, -0.23142109220724, -0.522016306838517, 0.0133710824257679, -0.477053514810188, -0.486925063802597, 0.151292056721364, 0.245374969930366, 0.0823446936101941, 0.99]\n",
      "Mul(8,11,4)\n",
      "Sin(7,9,2)\n",
      "Or(10,4,7)\n",
      "Min(9,10,6)\n",
      "Eq(2,7,4)\n",
      "Add(6,4,1)\n",
      "Or(1,4,3)\n",
      "Log(11,7,7)\n",
      "Sub(10,11,0)\n",
      "Copy(9,11,5)\n",
      "Sqr(5,8,1)\n",
      "Or(7,2,2)\n",
      "Mean(4,0,7)\n",
      "Min(9,10,6)\n",
      "Sin(7,9,2)\n",
      "Mul(6,2,0)\n",
      "\n",
      "Generation:  59\n",
      "Generation:  60\n",
      "Generation:  61\n",
      "Generation:  62\n",
      "Generation:  63\n",
      "Generation:  64\n",
      "Generation:  65\n",
      "Generation:  66\n",
      "Generation:  67\n",
      "Generation:  68\n",
      "Generation:  69\n",
      "Generation:  70\n",
      "Generation:  71\n",
      "Generation:  72\n",
      "Generation:  73\n",
      "Generation:  74\n",
      "Generation:  75\n",
      "Generation:  76\n",
      "[0.44832200567970115, 0.0, 0.8966440113594023, 0.0, 0.0, 0.0, 0.5, 2.0294621358924045, -1.0, 1.0, 0.5, 69377.0, 1.07567635919744, -0.281543625771467, 1.31731850174263, 0.752709471980858, -0.977419353730556, 0.361756102325203, -0.81556527378324, 0.359193920183536, 0.695107764818884, -0.108676831868102, 1.32331219992882, 1.13590931817953, -0.371944977416, -0.114054740830135, -0.233782462444042, 0.0200867482881131, -0.0562356831120968, -0.222204593199741, -0.185419307675078, -0.140786183688986, -0.0005453643267059, 0.171275020833416, 0.0743252844347594, 0.252556685391156, 0.113666519970155, 0.317260124206797, 0.0276179646269061, 0.0155411905010375, 11.5]\n",
      "Sin(6,10,5)\n",
      "Sqr(5,8,1)\n",
      "Div(3,1,2)\n",
      "Log(11,7,7)\n",
      "Div(3,7,3)\n",
      "Max(1,4,5)\n",
      "Sqr(5,8,1)\n",
      "Or(7,2,2)\n",
      "And(10,8,0)\n",
      "Mean(4,0,7)\n",
      "Min(9,10,6)\n",
      "Sin(7,9,2)\n",
      "Mul(6,2,0)\n",
      "\n",
      "[0.06318200846660518, -21.294621358924044, 0.12636401693321037, 0.0, 22.294621358924044, 1.0, 0.5, 0.12670275052901395, -1.0, 1.0, 0.5, 69377.0, 1.07567635919744, -0.281543625771467, 1.31731850174263, 0.752709471980858, -0.977419353730556, 0.361756102325203, -0.81556527378324, 0.359193920183536, 0.695107764818884, -0.108676831868102, 1.32331219992882, 1.13590931817953, -0.371944977416, -0.114054740830135, -0.233782462444042, 0.0200867482881131, -0.0562356831120968, -0.222204593199741, -0.185419307675078, -0.140786183688986, -0.0005453643267059, 0.171275020833416, 0.0743252844347594, 0.252556685391156, 0.113666519970155, 0.317260124206797, 0.0276179646269061, 0.0155411905010375, 11.5]\n",
      "Sqr(5,8,1)\n",
      "Div(3,1,2)\n",
      "Not(0,3,1)\n",
      "Log(11,7,7)\n",
      "Div(10,11,0)\n",
      "Div(3,7,3)\n",
      "Max(1,4,5)\n",
      "Sqr(5,8,1)\n",
      "Or(7,2,2)\n",
      "Add(2,7,4)\n",
      "Mean(4,0,7)\n",
      "Min(9,9,6)\n",
      "Sin(7,9,2)\n",
      "Mul(6,2,0)\n",
      "Sub(6,4,1)\n",
      "Sin(11,7,2)\n",
      "Mean(4,0,7)\n",
      "Min(9,10,6)\n",
      "Sin(7,9,2)\n",
      "Mul(6,2,0)\n",
      "\n",
      "Generation:  77\n",
      "Generation:  78\n",
      "Generation:  79\n",
      "Generation:  80\n",
      "Generation:  81\n",
      "Generation:  82\n",
      "Generation:  83\n",
      "Generation:  84\n",
      "Generation:  85\n",
      "Generation:  86\n",
      "Generation:  87\n",
      "Generation:  88\n",
      "Generation:  89\n",
      "Generation:  90\n",
      "Generation:  91\n",
      "[0.4171008596657082, 0.005625, 0.8342017193314164, 0.0, -0.1, 0.075, 0.5, 2.154909035520819, -1.0, 1.0, 0.5, 142760.0, 1.79180273461504, -0.163625784291395, -2.29540804448361, 1.026397495084, 0.966209730213912, -0.369313442991133, 0.806456690503685, -0.258861633528643, -0.363402033760969, 0.473802334538182, 0.51414796075946, 0.520646163584431, -0.392539451042142, 1.06505173681962, -0.343232824799856, -0.0818093798517183, -0.801145462945206, 0.235273891515637, -0.0246262468584488, 0.0273866942156346, 0.223870505158639, 0.363600614726391, -0.159069190268216, 0.217845141661613, 0.44654111535647, -0.518388069637682, -0.0666214374678507, -0.0480876367053007, 144.0]\n",
      "Log(11,7,7)\n",
      "Div(3,7,3)\n",
      "Min(9,10,6)\n",
      "Log(3,8,7)\n",
      "Sin(7,9,2)\n",
      "Add(7,3,4)\n",
      "Sub(6,4,1)\n",
      "And(10,8,0)\n",
      "Mean(4,0,7)\n",
      "Add(7,3,2)\n",
      "Min(9,10,6)\n",
      "Copy(2,6,4)\n",
      "Mean(4,7,5)\n",
      "Log(11,7,7)\n",
      "Div(3,7,3)\n",
      "Sqr(5,8,1)\n",
      "Or(7,2,2)\n",
      "And(10,8,0)\n",
      "Mean(4,0,7)\n",
      "Min(9,10,6)\n",
      "Sin(7,9,2)\n",
      "Mul(6,2,0)\n",
      "\n",
      "[-0.11422655820272251, -11.368920177604096, -0.22845311640544502, 0.0, 11.868920177604096, 0.0, 0.5, -0.22845311640544502, -1.0, 1.0, 0.5, 142760.0, 1.79180273461504, -0.163625784291395, -2.29540804448361, 1.026397495084, 0.966209730213912, -0.369313442991133, 0.806456690503685, -0.258861633528643, -0.363402033760969, 0.473802334538182, 0.51414796075946, 0.520646163584431, -0.392539451042142, 1.06505173681962, -0.343232824799856, -0.0818093798517183, -0.801145462945206, 0.235273891515637, -0.0246262468584488, 0.0273866942156346, 0.223870505158639, 0.363600614726391, -0.159069190268216, 0.217845141661613, 0.44654111535647, -0.518388069637682, -0.0666214374678507, -0.0480876367053007, 144.0]\n",
      "Log(11,7,7)\n",
      "Div(3,7,3)\n",
      "Min(9,10,6)\n",
      "Sin(7,9,2)\n",
      "Add(7,3,4)\n",
      "Sub(6,4,1)\n",
      "And(10,8,0)\n",
      "Mean(4,0,7)\n",
      "Min(9,10,6)\n",
      "Log(7,9,2)\n",
      "Mul(6,2,0)\n",
      "\n",
      "Generation:  92\n",
      "Generation:  93\n",
      "Generation:  94\n",
      "Generation:  95\n",
      "Generation:  96\n",
      "Generation:  97\n",
      "[-0.14970313101100693, 0.14946188735719557, -0.29940626202201387, 0.0, 0.35053811264280443, 0.0, 0.5, -0.29940626202201387, -1.0, 1.0, 0.5, 55188.0, 0.987940726799409, 0.161435977572335, 0.878433477245026, 2.50214947813005, -0.0459232506183304, 1.05616493660079, -0.35099736090514, 0.480509908754932, -0.385093658884416, 0.626972684455463, 1.08192543682732, 0.466766202406962, -1.30854107467699, 0.36557727107901, -0.705535899181723, -0.0647943797944037, 0.135554282997901, -0.953299852807839, -1.01534130750163, -0.257303796427591, -0.183879939353502, -0.480690683544382, 0.13138191761104, -0.334977908481654, 0.154620671786575, -0.136617376230485, 0.0278149797076695, 0.010508666431163, 22.8]\n",
      "Sin(11,6,4)\n",
      "Log(11,7,7)\n",
      "Div(3,7,3)\n",
      "Min(9,10,6)\n",
      "Sin(7,9,2)\n",
      "Sub(6,4,1)\n",
      "And(10,8,0)\n",
      "Mean(4,0,7)\n",
      "Min(9,10,6)\n",
      "Log(7,9,2)\n",
      "Mul(6,2,0)\n",
      "\n",
      "Generation:  98\n",
      "Generation:  99\n",
      "Generation:  100\n",
      "Generation:  101\n",
      "Generation:  102\n",
      "[0.39664785094862176, 0.0, 0.7932957018972435, 0.0, 0.3021863790438281, 0.20054659476095704, 0.5, 2.2253894718680187, -1.0, 1.0, 0.5, 136593.0, -0.751263351355178, 0.694619191732257, -0.22512462489857, -0.148458069446486, 2.87577575285931, 4.41522801846535, 0.328787635913066, 1.11781645926297, -0.181434434409543, -1.33279270068042, -0.529775464513845, -0.254193336750737, -0.631834675803823, -1.40654300442263, -0.820055019252187, -0.63965817446489, 1.07079191656404, 0.500931521374821, 0.072204849799532, 0.135034992353934, 0.212348194004641, 0.78082910577073, -0.602637769092174, 0.656069103267145, 1.25926831236471, -0.0205725054667618, 0.102610293666826, 0.0792326667478591, 57.5]\n",
      "Min(9,10,6)\n",
      "Sin(11,6,4)\n",
      "Mean(4,7,5)\n",
      "Log(11,7,7)\n",
      "Div(10,11,0)\n",
      "Div(3,5,3)\n",
      "Or(7,2,2)\n",
      "Not(0,3,1)\n",
      "And(10,8,0)\n",
      "Mean(4,0,7)\n",
      "Min(9,10,6)\n",
      "Sin(7,9,2)\n",
      "Mul(6,2,0)\n",
      "\n",
      "Generation:  103\n",
      "Generation:  104\n",
      "Generation:  105\n",
      "Generation:  106\n",
      "Generation:  107\n",
      "Generation:  108\n",
      "Generation:  109\n",
      "Generation:  110\n",
      "Generation:  111\n",
      "Generation:  112\n",
      "Generation:  113\n",
      "Generation:  114\n",
      "Generation:  115\n",
      "Generation:  116\n",
      "Generation:  117\n",
      "Generation:  118\n",
      "Generation:  119\n",
      "Generation:  120\n",
      "Generation:  121\n",
      "Generation:  122\n",
      "Generation:  123\n",
      "Generation:  124\n",
      "Generation:  125\n",
      "Generation:  126\n",
      "Generation:  127\n",
      "Generation:  128\n",
      "Generation:  129\n",
      "Generation:  130\n",
      "Generation:  131\n",
      "[0.4952299219783654, 0.4804530139182014, 0.9904598439567308, 0.0, -0.6931471805599453, 0.0, 0.5, 1.7090378824071188, -1.0, 1.0, 0.5, 17290.0, 0.890523708432275, -0.759219674595812, 0.855745495340511, -0.438365023204276, -1.03682495179352, -0.198534228805737, -0.49185581481074, -0.0787911812758578, 2.95496788620103, -1.59286422458877, 0.878587151092754, -1.12595598712714, 2.7186725860811, 1.10193739413134, 0.698819370220504, -0.809894052642663, 0.798705433564649, 0.0764662966145383, -0.149635911459115, 0.206062625768782, 0.126097578279533, 0.604539237817823, -0.248036857161627, 0.101117252936395, 0.505351058416357, -0.561343438284754, 0.0606610927720156, 0.0540860298756448, 165.6]\n",
      "Min(9,10,6)\n",
      "Log(6,4,4)\n",
      "Log(11,7,7)\n",
      "Div(3,7,3)\n",
      "Sqr(4,8,1)\n",
      "Or(7,2,2)\n",
      "And(10,8,0)\n",
      "Mean(4,0,7)\n",
      "Min(9,10,6)\n",
      "Sin(7,9,2)\n",
      "Mul(6,2,0)\n",
      "\n",
      "Generation:  132\n",
      "Generation:  133\n",
      "Generation:  134\n",
      "Generation:  135\n",
      "Generation:  136\n",
      "Generation:  137\n",
      "Generation:  138\n",
      "Generation:  139\n",
      "Generation:  140\n",
      "Generation:  141\n",
      "Generation:  142\n",
      "Generation:  143\n",
      "Generation:  144\n",
      "Generation:  145\n",
      "Generation:  146\n",
      "Generation:  147\n",
      "Generation:  148\n",
      "Generation:  149\n",
      "Generation:  150\n",
      "Generation:  151\n",
      "Generation:  152\n",
      "Generation:  153\n",
      "Generation:  154\n",
      "Generation:  155\n",
      "Generation:  156\n",
      "Generation:  157\n",
      "Generation:  158\n",
      "Generation:  159\n",
      "Generation:  160\n",
      "Generation:  161\n",
      "Generation:  162\n",
      "Generation:  163\n",
      "Generation:  164\n",
      "Generation:  165\n",
      "Generation:  166\n",
      "Generation:  167\n",
      "Generation:  168\n",
      "Generation:  169\n",
      "Generation:  170\n",
      "Generation:  171\n",
      "Generation:  172\n",
      "Generation:  173\n",
      "Generation:  174\n",
      "Generation:  175\n",
      "Generation:  176\n",
      "Generation:  177\n",
      "Generation:  178\n",
      "Generation:  179\n",
      "Generation:  180\n",
      "Generation:  181\n",
      "Generation:  182\n",
      "Generation:  183\n",
      "Generation:  184\n",
      "Generation:  185\n",
      "Generation:  186\n",
      "Generation:  187\n",
      "Generation:  188\n",
      "Generation:  189\n",
      "Generation:  190\n",
      "Generation:  191\n",
      "Generation:  192\n",
      "Generation:  193\n",
      "Generation:  194\n",
      "Generation:  195\n",
      "Generation:  196\n",
      "Generation:  197\n",
      "Generation:  198\n",
      "Generation:  199\n",
      "Generation:  200\n",
      "Generation:  201\n",
      "Generation:  202\n",
      "Generation:  203\n",
      "Generation:  204\n",
      "Generation:  205\n",
      "Generation:  206\n",
      "Generation:  207\n",
      "Generation:  208\n",
      "Generation:  209\n",
      "Generation:  210\n",
      "Generation:  211\n",
      "Generation:  212\n",
      "Generation:  213\n",
      "Generation:  214\n",
      "Generation:  215\n",
      "Generation:  216\n",
      "Generation:  217\n",
      "Generation:  218\n",
      "Generation:  219\n",
      "Generation:  220\n",
      "Generation:  221\n",
      "Generation:  222\n",
      "Generation:  223\n",
      "Generation:  224\n",
      "Generation:  225\n",
      "Generation:  226\n",
      "Generation:  227\n",
      "Generation:  228\n",
      "Generation:  229\n",
      "Generation:  230\n",
      "Generation:  231\n",
      "Generation:  232\n",
      "Generation:  233\n",
      "Generation:  234\n",
      "Generation:  235\n",
      "Generation:  236\n",
      "Generation:  237\n",
      "Generation:  238\n",
      "Generation:  239\n",
      "Generation:  240\n",
      "Generation:  241\n",
      "Generation:  242\n",
      "Generation:  243\n",
      "Generation:  244\n",
      "Generation:  245\n",
      "Generation:  246\n",
      "Generation:  247\n",
      "Generation:  248\n",
      "Generation:  249\n",
      "Generation:  250\n",
      "Generation:  251\n",
      "Generation:  252\n",
      "Generation:  253\n",
      "Generation:  254\n",
      "Generation:  255\n",
      "Generation:  256\n",
      "Generation:  257\n",
      "Generation:  258\n",
      "Generation:  259\n",
      "Generation:  260\n",
      "Generation:  261\n",
      "Generation:  262\n",
      "Generation:  263\n",
      "Generation:  264\n",
      "Generation:  265\n",
      "Generation:  266\n",
      "Generation:  267\n",
      "Generation:  268\n",
      "Generation:  269\n",
      "Generation:  270\n",
      "Generation:  271\n",
      "Generation:  272\n",
      "Generation:  273\n",
      "Generation:  274\n",
      "Generation:  275\n",
      "Generation:  276\n",
      "Generation:  277\n",
      "Generation:  278\n",
      "Generation:  279\n",
      "Generation:  280\n",
      "Generation:  281\n",
      "Generation:  282\n",
      "Generation:  283\n",
      "Generation:  284\n",
      "Generation:  285\n",
      "Generation:  286\n",
      "Generation:  287\n",
      "Generation:  288\n",
      "Generation:  289\n",
      "Generation:  290\n",
      "Generation:  291\n",
      "Generation:  292\n",
      "Generation:  293\n",
      "Generation:  294\n",
      "Generation:  295\n",
      "Generation:  296\n",
      "Generation:  297\n",
      "Generation:  298\n",
      "Generation:  299\n",
      "Generation:  300\n",
      "Generation:  301\n",
      "Generation:  302\n",
      "Generation:  303\n",
      "Generation:  304\n",
      "Generation:  305\n",
      "Generation:  306\n",
      "Generation:  307\n",
      "Generation:  308\n",
      "Generation:  309\n",
      "Generation:  310\n",
      "Generation:  311\n",
      "Generation:  312\n",
      "Generation:  313\n",
      "Generation:  314\n",
      "Generation:  315\n",
      "Generation:  316\n",
      "Generation:  317\n",
      "Generation:  318\n",
      "Generation:  319\n",
      "Generation:  320\n",
      "Generation:  321\n",
      "Generation:  322\n",
      "Generation:  323\n",
      "Generation:  324\n",
      "Generation:  325\n",
      "Generation:  326\n",
      "Generation:  327\n",
      "Generation:  328\n",
      "Generation:  329\n",
      "Generation:  330\n",
      "Generation:  331\n",
      "Generation:  332\n",
      "Generation:  333\n",
      "Generation:  334\n",
      "Generation:  335\n",
      "[0.08930901692454762, 0.0, 0.17861803384909525, 0.0, 0.0, 0.0, 0.5, 28.094752164124923, -1.0, 1.0, 0.5, 137916.0, -0.0133755774936621, 0.344715389625648, 0.256493431090332, -0.855345134491482, 0.782908416949717, 0.851861144623389, 0.178258157838116, 0.292102717291271, 0.701072165025031, -0.583289661372226, -0.68462078349097, 0.178916057279686, -0.626986931553246, -0.0926942807348811, -1.06879981058054, 0.173381318484132, -0.941437898897629, 0.98127373719041, 0.427170038849067, -0.193479111402311, 0.26007489087724, 0.87727733084934, -0.322917546662651, -0.387013459361688, -0.0421004373950217, -0.178863879259508, -0.0464726942537619, 0.0377938742375551, 6.17]\n",
      "Not(8,6,3)\n",
      "Add(7,3,5)\n",
      "Add(5,9,1)\n",
      "Gte(1,9,7)\n",
      "Min(9,10,6)\n",
      "Sin(7,9,2)\n",
      "Mul(6,2,0)\n",
      "Log(11,7,7)\n",
      "Sqr(7,2,2)\n",
      "Not(0,3,1)\n",
      "Mean(4,0,7)\n",
      "Min(9,10,6)\n",
      "Sin(7,9,2)\n",
      "Mul(6,2,0)\n",
      "\n",
      "[373.70863145441, 11.834400083156758, 31.57816440448795, 23.668800166313517, 0.0, 0.0, 11.834400083156758, 7.909364238174432, -1.0, 1.0, 0.5, 137916.0, -0.0133755774936621, 0.344715389625648, 0.256493431090332, -0.855345134491482, 0.782908416949717, 0.851861144623389, 0.178258157838116, 0.292102717291271, 0.701072165025031, -0.583289661372226, -0.68462078349097, 0.178916057279686, -0.626986931553246, -0.0926942807348811, -1.06879981058054, 0.173381318484132, -0.941437898897629, 0.98127373719041, 0.427170038849067, -0.193479111402311, 0.26007489087724, 0.87727733084934, -0.322917546662651, -0.387013459361688, -0.0421004373950217, -0.178863879259508, -0.0464726942537619, 0.0377938742375551, 6.17]\n",
      "Log(11,7,7)\n",
      "Sub(7,7,4)\n",
      "Max(8,6,6)\n",
      "Sin(7,9,2)\n",
      "Mul(6,2,0)\n",
      "Log(11,7,7)\n",
      "Or(7,2,2)\n",
      "Sub(6,4,1)\n",
      "Add(1,2,3)\n",
      "Sub(4,0,7)\n",
      "Add(7,3,2)\n",
      "Mul(6,2,0)\n",
      "\n",
      "Generation:  336\n",
      "Generation:  337\n",
      "Generation:  338\n",
      "Generation:  339\n",
      "Generation:  340\n",
      "Generation:  341\n",
      "Generation:  342\n",
      "Generation:  343\n",
      "Generation:  344\n",
      "Generation:  345\n",
      "Generation:  346\n",
      "Generation:  347\n",
      "Generation:  348\n",
      "Generation:  349\n",
      "Generation:  350\n",
      "Generation:  351\n",
      "Generation:  352\n",
      "Generation:  353\n",
      "Generation:  354\n",
      "Generation:  355\n",
      "Generation:  356\n",
      "Generation:  357\n",
      "Generation:  358\n",
      "Generation:  359\n",
      "Generation:  360\n",
      "Generation:  361\n",
      "Generation:  362\n",
      "Generation:  363\n",
      "Generation:  364\n",
      "Generation:  365\n",
      "Generation:  366\n",
      "Generation:  367\n",
      "Generation:  368\n",
      "Generation:  369\n",
      "Generation:  370\n",
      "Generation:  371\n",
      "Generation:  372\n",
      "Generation:  373\n",
      "Generation:  374\n",
      "Generation:  375\n",
      "Generation:  376\n",
      "Generation:  377\n",
      "Generation:  378\n",
      "Generation:  379\n",
      "Generation:  380\n",
      "Generation:  381\n",
      "Generation:  382\n",
      "Generation:  383\n",
      "Generation:  384\n",
      "Generation:  385\n",
      "Generation:  386\n",
      "Generation:  387\n",
      "Generation:  388\n",
      "Generation:  389\n",
      "Generation:  390\n",
      "Generation:  391\n",
      "Generation:  392\n",
      "Generation:  393\n",
      "Generation:  394\n",
      "Generation:  395\n",
      "Generation:  396\n",
      "Generation:  397\n",
      "Generation:  398\n",
      "Generation:  399\n",
      "Generation:  400\n",
      "Generation:  401\n",
      "Generation:  402\n",
      "Generation:  403\n",
      "Generation:  404\n",
      "Generation:  405\n",
      "Generation:  406\n",
      "Generation:  407\n",
      "Generation:  408\n",
      "Generation:  409\n",
      "Generation:  410\n",
      "Generation:  411\n",
      "Generation:  412\n",
      "Generation:  413\n",
      "Generation:  414\n",
      "Generation:  415\n",
      "Generation:  416\n",
      "Generation:  417\n",
      "Generation:  418\n",
      "Generation:  419\n",
      "Generation:  420\n",
      "Generation:  421\n",
      "Generation:  422\n",
      "Generation:  423\n",
      "Generation:  424\n",
      "Generation:  425\n",
      "Generation:  426\n",
      "Generation:  427\n",
      "Generation:  428\n",
      "Generation:  429\n",
      "Generation:  430\n",
      "Generation:  431\n",
      "Generation:  432\n",
      "Generation:  433\n",
      "Generation:  434\n",
      "Generation:  435\n",
      "Generation:  436\n",
      "Generation:  437\n",
      "Generation:  438\n",
      "Generation:  439\n",
      "Generation:  440\n",
      "Generation:  441\n",
      "Generation:  442\n",
      "Generation:  443\n",
      "Generation:  444\n",
      "Generation:  445\n",
      "Generation:  446\n",
      "Generation:  447\n",
      "Generation:  448\n",
      "Generation:  449\n",
      "Generation:  450\n",
      "Generation:  451\n",
      "Generation:  452\n",
      "Generation:  453\n",
      "Generation:  454\n",
      "Generation:  455\n",
      "Generation:  456\n",
      "Generation:  457\n",
      "Generation:  458\n",
      "Generation:  459\n",
      "Generation:  460\n",
      "Generation:  461\n",
      "Generation:  462\n",
      "Generation:  463\n",
      "Generation:  464\n",
      "Generation:  465\n",
      "Generation:  466\n",
      "Generation:  467\n",
      "Generation:  468\n",
      "Generation:  469\n",
      "Generation:  470\n",
      "Generation:  471\n",
      "Generation:  472\n",
      "Generation:  473\n",
      "Generation:  474\n",
      "Generation:  475\n",
      "Generation:  476\n",
      "Generation:  477\n",
      "Generation:  478\n",
      "Generation:  479\n",
      "Generation:  480\n",
      "Generation:  481\n",
      "Generation:  482\n",
      "Generation:  483\n",
      "Generation:  484\n",
      "Generation:  485\n",
      "Generation:  486\n",
      "Generation:  487\n",
      "Generation:  488\n",
      "Generation:  489\n",
      "Generation:  490\n",
      "Generation:  491\n",
      "Generation:  492\n",
      "Generation:  493\n",
      "Generation:  494\n",
      "Generation:  495\n",
      "Generation:  496\n",
      "Generation:  497\n",
      "Generation:  498\n",
      "Generation:  499\n",
      "Generation:  500\n",
      "Generation:  1\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 919.0, 0.904289463743519, -0.538055260410103, 0.39605803442056, 0.500679802458296, -0.864473245499103, -0.65719889527551, 0.0272307766245696, -0.0294730226819784, 0.265447158980849, -0.110990680576296, 1.2542495941642, 0.704630696835016, -0.967460980110652, 0.466523755933469, -0.303978534196397, 0.0947022459907592, -0.198326033211889, -0.124616107606201, 0.401377294875365, 0.157346245975617, -0.0994597455975855, -0.597579489269958, -0.0486661442244073, 0.551823921849876, 0.182934211359254, 0.402176323701105, -0.0813574985755738, 0.027251829658255, 158.0]\n",
      "\n",
      "[919.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 919.0, 0.904289463743519, -0.538055260410103, 0.39605803442056, 0.500679802458296, -0.864473245499103, -0.65719889527551, 0.0272307766245696, -0.0294730226819784, 0.265447158980849, -0.110990680576296, 1.2542495941642, 0.704630696835016, -0.967460980110652, 0.466523755933469, -0.303978534196397, 0.0947022459907592, -0.198326033211889, -0.124616107606201, 0.401377294875365, 0.157346245975617, -0.0994597455975855, -0.597579489269958, -0.0486661442244073, 0.551823921849876, 0.182934211359254, 0.402176323701105, -0.0813574985755738, 0.027251829658255, 158.0]\n",
      "Div(11,9,0)\n",
      "\n",
      "Generation:  2\n",
      "Generation:  3\n",
      "Generation:  4\n",
      "Generation:  5\n",
      "Generation:  6\n",
      "Generation:  7\n",
      "Generation:  8\n",
      "Generation:  9\n",
      "Generation:  10\n",
      "Generation:  11\n",
      "Generation:  12\n",
      "Generation:  13\n",
      "Generation:  14\n",
      "Generation:  15\n",
      "Generation:  16\n",
      "Generation:  17\n",
      "Generation:  18\n",
      "Generation:  19\n",
      "Generation:  20\n",
      "Generation:  21\n",
      "Generation:  22\n",
      "Generation:  23\n",
      "Generation:  24\n",
      "Generation:  25\n",
      "Generation:  26\n",
      "[-0.0891561996399722, 0.0, 0.0, 11.21626991771934, 0.0, 0.0, -1.0, 0.0, -1.0, 1.0, 0.5, 74329.0, -1.24249916753332, 0.18397170503581, 1.3821629221576, -1.10527611635046, -0.969099224533242, -0.49446914984581, 0.662540166172614, -0.494303982298783, -0.468604723536012, 2.20276139427994, 1.72927991268005, -0.801365895217973, -0.850516156986393, -0.602403539632439, 0.484348602226336, 0.832392091465958, 0.0180671013862018, -0.903306311993982, 1.58380164211378, 0.487852023755495, -0.241290538275223, 0.113149254747383, 0.214482242122248, 0.49791633331709, -0.672985823850884, -0.613289804689179, -0.404120457632691, -0.474791326409163, 100.0]\n",
      "Add(9,11,0)\n",
      "Sqrt(8,4,6)\n",
      "Log(0,3,3)\n",
      "Div(6,3,0)\n",
      "\n",
      "Generation:  27\n",
      "[0.08307360447067887, 0.0, 0.0, 12.037517889968933, 0.0, 0.0, 1.0, 0.0, -1.0, 1.0, 0.5, 168976.0, -1.33814752362267, 0.145267429057438, -3.0712455824181, -0.175918421609628, 0.156127225932399, -1.26485082249939, 1.74022421672631, 0.321044408999997, -0.752164601787838, -0.277345270223664, -0.320909072520661, -0.292581605518818, -2.02182481922195, 1.92874764151535, -1.40235868824249, -0.0966538635235031, -0.286038629975983, 0.449259560895433, 0.154862609951377, -0.461800076866743, 0.539340120077574, 1.55671013188694, 0.553652979819922, -0.282971449382927, -1.05674597159022, 0.402706276136803, 0.360380401141294, -0.157277667272547, 208.05]\n",
      "Add(9,11,0)\n",
      "Exp(3,6,6)\n",
      "Log(0,3,3)\n",
      "Div(6,3,0)\n",
      "\n",
      "Generation:  28\n",
      "Generation:  29\n",
      "Generation:  30\n",
      "Generation:  31\n",
      "Generation:  32\n",
      "Generation:  33\n",
      "Generation:  34\n",
      "Generation:  35\n",
      "Generation:  36\n",
      "Generation:  37\n",
      "Generation:  38\n",
      "Generation:  39\n",
      "Generation:  40\n",
      "Generation:  41\n",
      "[-0.48883710534569547, 0.0, 0.0, -0.9807470287370333, 0.0, 0.0, 0.479425538604203, 0.0, -1.0, 1.0, 0.5, 73957.0, -0.893444107664955, 0.346707578097543, 1.88968634774467, -0.174823966650307, 0.340038339251861, -0.837689223764173, 0.77925734549832, -0.24026195507721, 0.276465165050787, -0.505497601126681, -0.993047196566429, -0.48861714595093, -1.2520284164602, -0.238885968872294, -0.642392941524578, -0.189473912055019, -0.24694608520908, -0.502207035993704, -0.210030865983421, -0.173179949921383, -0.210675821917343, -0.365080273001261, 0.0199284687009103, 0.379523096003721, 0.0624128317109927, 0.0985002143615696, -0.235638845936994, -0.133450701218257, 1.0]\n",
      "Add(9,11,0)\n",
      "Sin(0,3,3)\n",
      "Sin(10,5,6)\n",
      "Div(6,3,0)\n",
      "\n",
      "Generation:  42\n",
      "Generation:  43\n",
      "Generation:  44\n",
      "Generation:  45\n",
      "Generation:  46\n",
      "Generation:  47\n",
      "Generation:  48\n",
      "Generation:  49\n",
      "Generation:  50\n",
      "Generation:  51\n",
      "Generation:  52\n",
      "Generation:  53\n",
      "Generation:  54\n",
      "Generation:  55\n",
      "Generation:  56\n",
      "Generation:  57\n",
      "Generation:  58\n",
      "[-19349.0, 0.0, 0.0, 0.0, -19349.0, -19349.0, 0.0, 0.0, -1.0, 1.0, 0.5, 19349.0, 0.463598779158506, -1.04301853145962, 2.09550519541751, 3.32604813489693, -1.3866625750721, 1.88633276746832, -1.21437716927036, 0.626074777794766, 2.24500261412841, -0.107636068180823, 1.10392924221143, -1.57191678457931, 0.847204242576553, 0.558763442263547, -2.99017057006833, 0.049388445662557, 0.889252993118404, 0.364742013088838, -0.895277161416764, 0.241097258355634, 0.140335154973121, 0.518077225117353, -0.305530065844135, 0.0084162034007976, 0.229396060566862, 0.199050570052824, 0.0274056269466176, 0.0631852394238556, 261.63]\n",
      "Mul(8,11,4)\n",
      "Log(4,3,5)\n",
      "Min(4,6,0)\n",
      "\n",
      "Generation:  59\n",
      "Generation:  60\n",
      "Generation:  61\n",
      "Generation:  62\n",
      "Generation:  63\n",
      "Generation:  64\n",
      "Generation:  65\n",
      "Generation:  66\n",
      "Generation:  67\n",
      "Generation:  68\n",
      "Generation:  69\n",
      "Generation:  70\n",
      "Generation:  71\n",
      "Generation:  72\n",
      "Generation:  73\n",
      "Generation:  74\n",
      "Generation:  75\n",
      "Generation:  76\n",
      "Generation:  77\n",
      "Generation:  78\n",
      "Generation:  79\n",
      "Generation:  80\n",
      "Generation:  81\n",
      "Generation:  82\n",
      "Generation:  83\n",
      "Generation:  84\n",
      "Generation:  85\n",
      "Generation:  86\n",
      "Generation:  87\n",
      "Generation:  88\n",
      "[0.05069241743473363, 0.0, 0.0, 9.457539467741153, 0.0, 51220.0, 0.479425538604203, 0.0, -1.0, 1.0, 0.5, 51220.0, 0.850087233932037, -1.46129702358995, -0.604113395181656, -0.385588460726415, -0.798379926856554, -0.472978970381324, 0.193440525236803, -0.336163131771925, -0.804176021463048, 0.471593697906497, -1.45486276420842, -0.206142729374471, 0.524717313639078, 0.0079788738030675, 0.0384180600894514, -1.51946867668401, 0.0095593304440339, 0.911864675455148, -0.250242693744163, 0.185015471004196, -0.309617575900297, -0.9938718149867, -0.391381634395445, -0.387632173992929, 0.434216476756706, 1.10963859608958, -0.12880933067381, 0.0451879664341911, 324.98]\n",
      "Sin(10,5,6)\n",
      "Add(9,11,0)\n",
      "Log(0,3,3)\n",
      "Div(6,3,0)\n",
      "Sin(4,2,4)\n",
      "Log(0,3,3)\n",
      "Sin(10,5,6)\n",
      "Max(1,1,0)\n",
      "Log(0,11,7)\n",
      "Max(4,11,5)\n",
      "Mean(6,3,0)\n",
      "Log(0,3,3)\n",
      "Sin(10,5,6)\n",
      "Div(6,3,0)\n",
      "\n",
      "Generation:  89\n",
      "Generation:  90\n",
      "Generation:  91\n",
      "Generation:  92\n",
      "Generation:  93\n",
      "Generation:  94\n",
      "Generation:  95\n",
      "Generation:  96\n",
      "Generation:  97\n",
      "Generation:  98\n",
      "Generation:  99\n",
      "Generation:  100\n",
      "Generation:  101\n",
      "Generation:  102\n",
      "Generation:  103\n",
      "Generation:  104\n",
      "Generation:  105\n",
      "Generation:  106\n",
      "Generation:  107\n",
      "Generation:  108\n",
      "Generation:  109\n",
      "Generation:  110\n",
      "Generation:  111\n",
      "Generation:  112\n",
      "Generation:  113\n",
      "Generation:  114\n",
      "Generation:  115\n",
      "Generation:  116\n",
      "Generation:  117\n",
      "Generation:  118\n",
      "Generation:  119\n",
      "Generation:  120\n",
      "Generation:  121\n",
      "Generation:  122\n",
      "Generation:  123\n",
      "Generation:  124\n",
      "Generation:  125\n",
      "Generation:  126\n",
      "Generation:  127\n",
      "Generation:  128\n",
      "Generation:  129\n",
      "Generation:  130\n",
      "Generation:  131\n",
      "Generation:  132\n",
      "Generation:  133\n",
      "Generation:  134\n",
      "Generation:  135\n",
      "Generation:  136\n",
      "Generation:  137\n",
      "Generation:  138\n",
      "Generation:  139\n",
      "Generation:  140\n",
      "Generation:  141\n",
      "Generation:  142\n",
      "Generation:  143\n",
      "Generation:  144\n",
      "Generation:  145\n",
      "Generation:  146\n",
      "Generation:  147\n",
      "Generation:  148\n",
      "Generation:  149\n",
      "Generation:  150\n",
      "Generation:  151\n",
      "Generation:  152\n",
      "Generation:  153\n",
      "Generation:  154\n",
      "Generation:  155\n",
      "Generation:  156\n",
      "[0.09515185633516046, 0.0, 1.0, 10.509516456280428, 1.0, 146639.0, 1.0, 1.1401890360004385, -1.0, 1.0, 0.5, 146639.0, -1.75971183029743, 1.53644997356474, -0.285420227966492, -0.930989585659862, 0.227305859567324, -1.12954827291207, 0.705497094214442, 0.347748260324811, -0.475596180487845, -1.18872252921635, 1.02848502149414, 0.398064227740112, -0.529652340198782, -0.349730375268804, -0.345704057984997, 0.751787354100708, 0.168853762735062, 1.08564438242016, -0.794165056482191, -0.424840666597407, 0.300849523981783, 0.571106444414236, -0.633374253192045, 0.0038838376062968, 1.1241286457474, -0.15936853102007, -1.02581174463093, -0.406064968446666, 35.85]\n",
      "Add(9,11,0)\n",
      "Log(9,7,4)\n",
      "Log(0,11,7)\n",
      "Sin(10,5,6)\n",
      "Log(4,6,3)\n",
      "Max(4,11,5)\n",
      "Mean(6,3,0)\n",
      "Sqr(8,8,4)\n",
      "Lt(4,7,6)\n",
      "Neq(8,6,2)\n",
      "Log(0,3,3)\n",
      "Sub(9,2,5)\n",
      "Mean(6,3,0)\n",
      "Sin(10,5,6)\n",
      "Log(0,11,7)\n",
      "Max(4,11,5)\n",
      "Mean(6,3,0)\n",
      "Mul(4,4,4)\n",
      "Log(0,3,3)\n",
      "Lt(4,7,6)\n",
      "Div(6,3,0)\n",
      "\n",
      "Generation:  157\n",
      "Generation:  158\n",
      "Generation:  159\n",
      "Generation:  160\n",
      "Generation:  161\n",
      "Generation:  162\n",
      "Generation:  163\n",
      "Generation:  164\n",
      "Generation:  165\n",
      "Generation:  166\n",
      "Generation:  167\n",
      "Generation:  168\n",
      "Generation:  169\n",
      "Generation:  170\n",
      "Generation:  171\n",
      "Generation:  172\n",
      "Generation:  173\n",
      "Generation:  174\n",
      "Generation:  175\n",
      "Generation:  176\n",
      "Generation:  177\n",
      "Generation:  178\n",
      "Generation:  179\n",
      "Generation:  180\n",
      "Generation:  181\n",
      "[-0.695832329738945, 0.0, 0.0, 1.643830192639704, 0.0, 67906.0, -1.143830192639704, -0.7351666863853142, -1.0, 1.0, 0.5, 67906.0, 1.23879866801103, -1.4294565699543, 0.640028371158442, -1.20833080857382, -1.84912327286557, -0.618336663699473, -1.06310186641639, -0.0485372609528645, -1.64339544122372, 1.35649804393976, -0.0948418306367026, -1.12634066963855, -0.381151665685388, -0.146407648955055, 1.05509419829163, -0.57345991337938, 0.886221553328362, -0.410202421758462, -0.79576122963432, -0.20957579281461, -0.151852564245279, -0.303577887920134, 0.0858198469899122, 0.358066106003204, 0.0686380247846248, -0.280574029343676, 0.0331218579994252, 0.0429855491894556, 117.0]\n",
      "Max(4,11,5)\n",
      "Mul(4,4,4)\n",
      "Copy(4,10,2)\n",
      "Sin(10,5,6)\n",
      "Log(9,7,4)\n",
      "Or(6,6,0)\n",
      "Log(0,11,7)\n",
      "Div(7,10,3)\n",
      "Max(4,11,5)\n",
      "Mean(9,2,0)\n",
      "Log(0,3,3)\n",
      "Div(6,3,0)\n",
      "Eq(10,3,1)\n",
      "Mean(3,0,3)\n",
      "Sqr(3,11,0)\n",
      "Log(0,3,3)\n",
      "Sub(10,3,6)\n",
      "Div(6,3,0)\n",
      "\n",
      "Generation:  182\n",
      "Generation:  183\n",
      "Generation:  184\n",
      "Generation:  185\n",
      "Generation:  186\n",
      "Generation:  187\n",
      "Generation:  188\n",
      "Generation:  189\n",
      "Generation:  190\n",
      "Generation:  191\n",
      "Generation:  192\n",
      "Generation:  193\n",
      "Generation:  194\n",
      "Generation:  195\n",
      "Generation:  196\n",
      "Generation:  197\n",
      "Generation:  198\n",
      "Generation:  199\n",
      "Generation:  200\n",
      "Generation:  201\n",
      "Generation:  202\n",
      "Generation:  203\n",
      "Generation:  204\n",
      "Generation:  205\n",
      "Generation:  206\n",
      "Generation:  207\n",
      "Generation:  208\n",
      "Generation:  209\n",
      "Generation:  210\n",
      "Generation:  211\n",
      "Generation:  212\n",
      "Generation:  213\n",
      "Generation:  214\n",
      "Generation:  215\n",
      "Generation:  216\n",
      "Generation:  217\n",
      "Generation:  218\n",
      "[476131946.0067445, 0.0, 0.0, 21.002581498403277, 2.718281828459045, 462268132.8155266, 10000000000, 0.0, -1.0, 1.0, 0.5, 151789.0, -1.13338419978218, -0.0755396933234025, 1.69557627904196, -1.09335899239569, -0.250542675938789, -0.224424217453187, 0.920927139904774, 0.0148743278554196, -1.20856249358932, -0.443415072921862, -1.69204745431547, -0.836566154475275, -0.603770968955109, -0.0988208034051158, -0.679211350458666, -0.909500273347331, -0.738650345081265, 1.43236432226288, -2.46403061533906, -0.367600397297894, -0.241058545008411, -0.564345509664794, -0.0499917079737515, -0.290247123243406, 0.831990387598026, -0.719386452939702, 0.0020079295821354, 0.0688169515720427, 148.0]\n",
      "Add(8,11,0)\n",
      "Mul(4,4,4)\n",
      "Sin(10,5,6)\n",
      "Log(0,11,7)\n",
      "Max(7,10,3)\n",
      "Max(4,11,5)\n",
      "Mean(6,3,0)\n",
      "Log(10,11,3)\n",
      "Mean(3,5,6)\n",
      "Exp(9,4,4)\n",
      "Log(0,3,3)\n",
      "Div(6,3,0)\n",
      "Not(3,0,1)\n",
      "Add(11,0,5)\n",
      "Mean(3,0,3)\n",
      "Exp(3,6,6)\n",
      "Log(0,3,3)\n",
      "Mean(6,3,0)\n",
      "Log(0,3,3)\n",
      "Not(4,4,7)\n",
      "Div(6,3,0)\n",
      "Eq(10,3,1)\n",
      "Add(11,0,5)\n",
      "Mean(3,0,3)\n",
      "Exp(3,6,6)\n",
      "Mean(9,2,0)\n",
      "Log(0,3,3)\n",
      "Div(6,3,0)\n",
      "\n",
      "[461993708.6386206, 0.0, 0.0, 21.64531640369625, 0.0, 10000000000, 10000000000, 1.0, -1.0, 1.0, 0.5, 151789.0, -1.13338419978218, -0.0755396933234025, 1.69557627904196, -1.09335899239569, -0.250542675938789, -0.224424217453187, 0.920927139904774, 0.0148743278554196, -1.20856249358932, -0.443415072921862, -1.69204745431547, -0.836566154475275, -0.603770968955109, -0.0988208034051158, -0.679211350458666, -0.909500273347331, -0.738650345081265, 1.43236432226288, -2.46403061533906, -0.367600397297894, -0.241058545008411, -0.564345509664794, -0.0499917079737515, -0.290247123243406, 0.831990387598026, -0.719386452939702, 0.0020079295821354, 0.0688169515720427, 148.0]\n",
      "Add(8,11,0)\n",
      "Mul(4,4,4)\n",
      "Sin(10,5,6)\n",
      "Log(0,11,7)\n",
      "Max(7,10,3)\n",
      "Max(4,11,5)\n",
      "Mean(6,3,0)\n",
      "Mean(3,5,6)\n",
      "Log(0,3,3)\n",
      "Div(6,3,0)\n",
      "Not(3,0,1)\n",
      "Add(11,0,5)\n",
      "Mean(3,0,3)\n",
      "Exp(3,6,6)\n",
      "Log(0,3,3)\n",
      "Mean(6,3,0)\n",
      "Log(0,3,3)\n",
      "Exp(11,4,5)\n",
      "Not(4,4,7)\n",
      "Div(6,3,0)\n",
      "Eq(10,3,1)\n",
      "Mean(3,0,3)\n",
      "Exp(3,6,6)\n",
      "Mean(9,2,0)\n",
      "Log(0,3,3)\n",
      "Div(6,3,0)\n",
      "\n",
      "Generation:  219\n",
      "Generation:  220\n",
      "Generation:  221\n",
      "Generation:  222\n",
      "Generation:  223\n",
      "Generation:  224\n",
      "Generation:  225\n",
      "Generation:  226\n",
      "Generation:  227\n",
      "Generation:  228\n",
      "Generation:  229\n",
      "Generation:  230\n",
      "Generation:  231\n",
      "Generation:  232\n",
      "Generation:  233\n",
      "[0.19773780303907773, 0.0, 0.0, 1.2643004835579872, 0.0, 116363.27865247955, 0.25, 1.6168066722416745, -1.0, 1.0, 0.5, 116364.0, 2.03018602880589, 0.168626853019699, -1.92064857012978, 0.326140130534895, 0.48740693788222, -0.934401961015292, 0.20188100909537, -0.240896724752175, 0.307865333740183, -0.358433466821746, 1.01212700680951, 0.755403283279931, 0.328194373047678, -0.729538209092883, 0.206773575941997, 0.556332077484125, 0.0337755740738379, 1.15609606916608, 0.0158215440402382, -0.137262045297552, 0.219578949250282, 0.773859439995574, -0.10523564711532, -0.555574542139101, 0.294905763114516, -0.0934287549046717, -0.0050272336916666, -0.0415730527952527, 12.34]\n",
      "Eq(10,3,1)\n",
      "Mean(3,0,3)\n",
      "Sqr(3,11,0)\n",
      "Log(0,3,3)\n",
      "Mul(2,3,4)\n",
      "Log(10,11,3)\n",
      "Exp(3,6,6)\n",
      "Div(6,3,0)\n",
      "Add(11,0,5)\n",
      "Log(10,11,3)\n",
      "Sqr(3,11,0)\n",
      "Sqr(10,10,6)\n",
      "Exp(0,11,7)\n",
      "Mean(9,2,0)\n",
      "Log(0,3,3)\n",
      "Div(6,3,0)\n",
      "Eq(10,3,1)\n",
      "Mean(3,0,3)\n",
      "Sqr(3,11,0)\n",
      "Log(0,3,3)\n",
      "Not(3,0,1)\n",
      "Mean(3,0,3)\n",
      "Sqr(3,11,0)\n",
      "Sqr(10,10,6)\n",
      "Log(0,3,3)\n",
      "Div(6,3,0)\n",
      "\n",
      "Generation:  234\n",
      "Generation:  235\n",
      "Generation:  236\n",
      "Generation:  237\n",
      "Generation:  238\n",
      "Generation:  239\n",
      "Generation:  240\n",
      "Generation:  241\n",
      "Generation:  242\n",
      "Generation:  243\n",
      "Generation:  244\n",
      "Generation:  245\n",
      "Generation:  246\n",
      "Generation:  247\n",
      "Generation:  248\n",
      "Generation:  249\n",
      "Generation:  250\n",
      "Generation:  251\n",
      "Generation:  252\n",
      "Generation:  253\n",
      "Generation:  254\n",
      "Generation:  255\n",
      "Generation:  256\n",
      "Generation:  257\n",
      "Generation:  258\n",
      "Generation:  259\n",
      "Generation:  260\n",
      "Generation:  261\n",
      "Generation:  262\n",
      "Generation:  263\n",
      "Generation:  264\n",
      "Generation:  265\n",
      "Generation:  266\n",
      "Generation:  267\n",
      "Generation:  268\n",
      "Generation:  269\n",
      "Generation:  270\n",
      "Generation:  271\n",
      "Generation:  272\n",
      "Generation:  273\n",
      "Generation:  274\n",
      "Generation:  275\n",
      "Generation:  276\n",
      "Generation:  277\n",
      "Generation:  278\n",
      "Generation:  279\n",
      "Generation:  280\n",
      "Generation:  281\n",
      "Generation:  282\n",
      "Generation:  283\n",
      "Generation:  284\n",
      "Generation:  285\n",
      "Generation:  286\n",
      "Generation:  287\n",
      "Generation:  288\n",
      "Generation:  289\n",
      "Generation:  290\n",
      "Generation:  291\n",
      "Generation:  292\n",
      "Generation:  293\n",
      "Generation:  294\n",
      "Generation:  295\n",
      "Generation:  296\n",
      "Generation:  297\n",
      "Generation:  298\n",
      "Generation:  299\n",
      "Generation:  300\n",
      "Generation:  301\n",
      "Generation:  302\n",
      "Generation:  303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  310\n",
      "Generation:  311\n",
      "Generation:  312\n",
      "Generation:  313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  314\n",
      "Generation:  315\n",
      "Generation:  316\n",
      "Generation:  317\n",
      "Generation:  318\n",
      "Generation:  319\n",
      "Generation:  320\n",
      "Generation:  321\n",
      "Generation:  322\n",
      "Generation:  323\n",
      "Generation:  324\n",
      "Generation:  325\n",
      "Generation:  326\n",
      "Generation:  327\n",
      "Generation:  328\n",
      "Generation:  329\n",
      "Generation:  330\n",
      "Generation:  331\n",
      "Generation:  332\n",
      "Generation:  333\n",
      "Generation:  334\n",
      "Generation:  335\n",
      "Generation:  336\n",
      "Generation:  337\n",
      "Generation:  338\n",
      "Generation:  339\n",
      "Generation:  340\n",
      "Generation:  341\n",
      "Generation:  342\n",
      "Generation:  343\n",
      "Generation:  344\n",
      "Generation:  345\n",
      "Generation:  346\n",
      "Generation:  347\n",
      "Generation:  348\n",
      "Generation:  349\n",
      "Generation:  350\n",
      "Generation:  351\n",
      "Generation:  352\n",
      "Generation:  353\n",
      "Generation:  354\n",
      "Generation:  355\n",
      "Generation:  356\n",
      "Generation:  357\n",
      "Generation:  358\n",
      "Generation:  359\n",
      "Generation:  360\n",
      "Generation:  361\n",
      "Generation:  362\n",
      "Generation:  363\n",
      "Generation:  364\n",
      "Generation:  365\n",
      "Generation:  366\n",
      "Generation:  367\n",
      "Generation:  368\n",
      "Generation:  369\n",
      "Generation:  370\n",
      "Generation:  371\n",
      "Generation:  372\n",
      "Generation:  373\n",
      "Generation:  374\n",
      "Generation:  375\n",
      "Generation:  376\n",
      "Generation:  377\n",
      "Generation:  378\n",
      "Generation:  379\n",
      "Generation:  380\n",
      "Generation:  381\n",
      "Generation:  382\n",
      "Generation:  383\n",
      "Generation:  384\n",
      "Generation:  385\n",
      "Generation:  386\n",
      "Generation:  387\n",
      "Generation:  388\n",
      "Generation:  389\n",
      "Generation:  390\n",
      "Generation:  391\n",
      "Generation:  392\n",
      "Generation:  393\n",
      "Generation:  394\n",
      "Generation:  395\n",
      "Generation:  396\n",
      "Generation:  397\n",
      "Generation:  398\n",
      "Generation:  399\n",
      "Generation:  400\n",
      "Generation:  401\n",
      "Generation:  402\n",
      "Generation:  403\n",
      "Generation:  404\n",
      "Generation:  405\n",
      "Generation:  406\n",
      "Generation:  407\n",
      "Generation:  408\n",
      "Generation:  409\n",
      "Generation:  410\n",
      "Generation:  411\n",
      "Generation:  412\n",
      "Generation:  413\n",
      "Generation:  414\n",
      "Generation:  415\n",
      "Generation:  416\n",
      "Generation:  417\n",
      "Generation:  418\n",
      "Generation:  419\n",
      "Generation:  420\n",
      "Generation:  421\n",
      "Generation:  422\n",
      "Generation:  423\n",
      "Generation:  424\n",
      "Generation:  425\n",
      "Generation:  426\n",
      "Generation:  427\n",
      "Generation:  428\n",
      "Generation:  429\n",
      "Generation:  430\n",
      "Generation:  431\n",
      "Generation:  432\n",
      "[-0.013210323259785411, 0.0, 0.0, -0.013210707518028176, 0.0, 0.0, 0.5, 0.0, -1.0, 1.0, 0.5, 25342.0, 1.05082860808528, -0.172360620495568, 1.34885107527415, 1.23129365467608, -0.892720136056344, 0.464008902014735, -0.778930398781166, 0.399845478705345, 0.710512487230974, -0.0589431482099192, 1.20304550829538, 0.984741484916253, -0.742687746911931, 0.0041891270597807, -0.215148712893466, -0.0474209545304452, -0.110220212363425, -0.004180033292278, -0.352744087282903, -0.200751004017958, 0.0233247777929652, 0.238495609617027, 0.0450692888437338, 0.217246325991292, 0.263180627991013, -0.407535070491338, 0.0811274800792536, 0.0255524082123889, 9.99]\n",
      "Log(10,11,3)\n",
      "Exp(3,6,6)\n",
      "Add(10,11,3)\n",
      "Div(6,3,0)\n",
      "Log(0,3,3)\n",
      "Sin(3,11,0)\n",
      "Log(0,3,3)\n",
      "Sin(3,11,0)\n",
      "\n",
      "Generation:  433\n",
      "Generation:  434\n",
      "Generation:  435\n",
      "Generation:  436\n",
      "Generation:  437\n",
      "Generation:  438\n",
      "Generation:  439\n",
      "Generation:  440\n",
      "Generation:  441\n",
      "Generation:  442\n",
      "Generation:  443\n",
      "Generation:  444\n",
      "Generation:  445\n",
      "Generation:  446\n",
      "Generation:  447\n",
      "Generation:  448\n",
      "Generation:  449\n",
      "Generation:  450\n",
      "Generation:  451\n",
      "Generation:  452\n",
      "Generation:  453\n",
      "Generation:  454\n",
      "Generation:  455\n",
      "Generation:  456\n",
      "Generation:  457\n",
      "Generation:  458\n",
      "Generation:  459\n",
      "Generation:  460\n",
      "Generation:  461\n",
      "Generation:  462\n",
      "Generation:  463\n",
      "Generation:  464\n",
      "Generation:  465\n",
      "Generation:  466\n",
      "Generation:  467\n",
      "Generation:  468\n",
      "Generation:  469\n",
      "Generation:  470\n",
      "Generation:  471\n",
      "[-0.36158380657528416, 0.0, 0.0, -0.3699660793156038, 0.0, 12735.32671320486, 10000000000, 0.0, -1.0, 1.0, 0.5, 50942.0, -0.316300880867088, -0.0516350470897281, 1.59257031982497, -0.937029798370101, -0.41327173077414, 0.0095563896775932, 0.0745576805088336, 0.200430789127872, -1.23825825524089, 0.131848598472067, 1.02474791063214, 1.00550390326811, 0.393041142409372, -0.257126277889318, -1.51055879886776, -1.66255072801148, 0.0341798031370355, 0.641057327484451, -1.3439779957298, -0.519185403552837, -0.436633565527867, -0.732322366399603, 0.224858238238224, 0.186667769188077, -0.618028237287696, -0.0497162433500847, 0.0722817393449787, 0.0617931627297229, 31.71]\n",
      "Or(4,11,5)\n",
      "Min(3,2,4)\n",
      "Lt(4,5,3)\n",
      "Mul(2,3,4)\n",
      "Log(10,11,3)\n",
      "Mean(6,3,5)\n",
      "Mul(11,5,6)\n",
      "Add(10,6,3)\n",
      "Exp(3,6,6)\n",
      "Add(10,11,3)\n",
      "Div(6,3,0)\n",
      "Log(0,3,3)\n",
      "Sin(3,11,0)\n",
      "Log(0,3,3)\n",
      "Sin(3,11,0)\n",
      "\n",
      "Generation:  472\n",
      "Generation:  473\n",
      "Generation:  474\n",
      "Generation:  475\n",
      "Generation:  476\n",
      "Generation:  477\n",
      "Generation:  478\n",
      "Generation:  479\n",
      "Generation:  480\n",
      "Generation:  481\n",
      "Generation:  482\n",
      "Generation:  483\n",
      "Generation:  484\n",
      "Generation:  485\n",
      "Generation:  486\n",
      "Generation:  487\n",
      "Generation:  488\n",
      "Generation:  489\n",
      "Generation:  490\n",
      "Generation:  491\n",
      "Generation:  492\n",
      "Generation:  493\n",
      "Generation:  494\n",
      "Generation:  495\n",
      "Generation:  496\n",
      "Generation:  497\n",
      "Generation:  498\n",
      "Generation:  499\n",
      "Generation:  500\n",
      "Generation:  1\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 144837.0, -0.268989456512987, 0.551097231557059, 0.206860395330621, 0.166647171787457, 0.869894546343072, -0.712060834165726, 1.35186322592169, -0.428186303389548, -0.525721870056913, 0.185378416519246, 0.734430517931796, 0.0555419940365684, -1.19081915962835, 0.564335382317923, -0.767910641017481, -0.968175663045101, 0.0029044020065964, -0.0337267742784268, 1.36399978390501, -0.0475988664066673, 0.0908996767755482, 0.559836573053034, -0.0761865622406768, 0.0569561094007267, -0.361370883192971, 0.425313184172889, -0.150165962212323, -0.0740963297541695, 33.25]\n",
      "\n",
      "[144837.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 144837.0, -0.268989456512987, 0.551097231557059, 0.206860395330621, 0.166647171787457, 0.869894546343072, -0.712060834165726, 1.35186322592169, -0.428186303389548, -0.525721870056913, 0.185378416519246, 0.734430517931796, 0.0555419940365684, -1.19081915962835, 0.564335382317923, -0.767910641017481, -0.968175663045101, 0.0029044020065964, -0.0337267742784268, 1.36399978390501, -0.0475988664066673, 0.0908996767755482, 0.559836573053034, -0.0761865622406768, 0.0569561094007267, -0.361370883192971, 0.425313184172889, -0.150165962212323, -0.0740963297541695, 33.25]\n",
      "And(9,5,5)\n",
      "Add(11,5,0)\n",
      "\n",
      "Generation:  2\n",
      "Generation:  3\n",
      "Generation:  4\n",
      "[8.515448813730888, 0.0, 4991.285714285715, 0.0, 34939.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 34939.0, 1.13226392865691, -0.202075692824632, 0.653943489042509, 0.789989728439677, -0.651787734817116, 0.0264513601485061, -0.434097842680921, 0.271713425810928, 0.613219188156799, -0.0338640767454041, 1.01041059870805, 0.357684091159439, -2.00446637074798, 0.402957189583589, -0.667775281984704, -0.438244597668613, 0.294713895902181, -0.36511391919459, 0.0546889045485908, -0.275962489845674, -0.0192107169374518, 0.114506243696293, -0.0208683739625711, 0.236102330146248, 0.396491155471941, 0.444555381275775, -0.0137562191685036, -0.0053609597615843, 2.0]\n",
      "Sub(7,0,6)\n",
      "Max(5,11,4)\n",
      "Mean(7,1,2)\n",
      "Log(2,7,0)\n",
      "\n",
      "Generation:  5\n",
      "Generation:  6\n",
      "Generation:  7\n",
      "Generation:  8\n",
      "Generation:  9\n",
      "Generation:  10\n",
      "Generation:  11\n",
      "Generation:  12\n",
      "Generation:  13\n",
      "Generation:  14\n",
      "[-0.477501629624192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 77911.0, -3.73227649977127, 3.13119456376746, -0.961135646964281, -0.187914365800676, -2.04234636538574, -0.5791465890102, -1.53998258484728, 2.54949795170544, -0.374500857579453, 0.276904948518146, 0.804089613644527, 1.01801770871826, -1.03963524259418, 1.93705035472538, 0.407676740561609, 0.637491078240919, 0.633613705299429, -0.103557723280235, -0.264315634508928, -0.130246255309498, -0.0153368149548598, -0.577186261706395, 0.329662643896512, 0.151196872665335, -0.0932460151482353, 0.0379867811516024, -0.626057507801992, -0.272337523493582, 11.98]\n",
      "Sin(11,8,0)\n",
      "\n",
      "Generation:  15\n",
      "[0.7481411979693834, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 53565.0, -1.56351442082326, -0.46980680196994, 1.72795609919515, -1.43035247531884, 0.0154381068365209, 0.280742877977805, -0.12414066400136, -0.0931115164801404, -0.93579440621712, 1.05483381227416, 1.83662815347314, -0.441094704464841, -0.539341537505875, -0.3849817502831, 0.706586090872713, 0.752007407062522, 0.364031645205134, -1.66389603790371, 0.412875822738294, -0.446296019250737, 0.0355071158927261, 0.499053424333821, 0.418403139004115, -0.312794919383764, -0.423568802546613, -0.469560269752642, -0.530480077103861, -0.101518880213109, 17.5]\n",
      "Sin(11,8,0)\n",
      "\n",
      "Generation:  16\n",
      "Generation:  17\n",
      "Generation:  18\n",
      "Generation:  19\n",
      "Generation:  20\n",
      "[-0.46514181375628344, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 69703.0, -2.17323594869766, 0.59825691485958, 1.20957245587291, -0.328127172205285, 0.331068175451033, 1.09334617972661, 0.307360892645115, 0.651999205536677, 0.460821898200986, -0.252932445049838, 0.340159085815457, 0.465736478690031, -0.734063897547555, -0.127522231322176, 0.454159677037362, -1.39917399492748, 1.1729441962775, -2.67690342462987, -1.99040381957445, -0.374292242466957, -0.203961968324494, -0.0103372995169139, 0.0127676576989132, -0.609349528657771, 0.270084280179798, 0.273459545824225, -0.238770518253566, -0.214975228043662, 58.11]\n",
      "Sin(11,8,0)\n",
      "\n",
      "Generation:  21\n",
      "Generation:  22\n",
      "Generation:  23\n",
      "Generation:  24\n",
      "Generation:  25\n",
      "Generation:  26\n",
      "[0.9999393210617908, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 26592.0, 1.2804538407658, -0.340756596520131, -0.482384029623176, 0.0997588304400815, 0.16887081981372, 0.231607351741847, -0.007142041343603, -0.0667670084712115, -1.1218280936189, 0.73404964786655, -1.58448744184276, -0.513266937001185, 0.212962928976687, 0.383731287444538, 1.01695231114482, -1.37595183730842, -0.307848916527345, 0.716350241752336, -0.706144265868818, -0.417730288829464, -0.776344431532391, -1.92517108847088, 0.0245083426464295, -1.4424834607986, 0.372016442957113, -0.62931983444658, 0.029308808225572, 0.0226109006558302, 74.78]\n",
      "Sin(11,8,0)\n",
      "\n",
      "Generation:  27\n",
      "Generation:  28\n",
      "[0.7344674439957048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.5, 126274.0, -1.21962711358552, 1.0864894295131, 1.09262681821298, -0.622352682908879, 0.158508942175914, 0.25041193610958, 0.303602731381754, -1.02715163770287, 1.1797102446696, 0.239237246493671, -1.64294541443957, -0.141567662525833, 0.309130922470678, -0.719360191284789, 0.448275891781314, -0.122114910720416, -0.718381296582112, 0.582301750329078, 0.434512946769549, -0.0073621019724573, 0.893208866059655, 0.669810840306322, -0.264007251569862, -0.688654471535148, 0.183951632273452, -0.0787219256970877, 0.0329167854459836, 0.0346854053047772, 20.64]\n",
      "Sin(11,8,0)\n",
      "\n",
      "Generation:  29\n",
      "Generation:  30\n",
      "Generation:  31\n",
      "Generation:  32\n",
      "Generation:  33\n",
      "Generation:  34\n",
      "Generation:  35\n",
      "Generation:  36\n",
      "Generation:  37\n",
      "Generation:  38\n",
      "Generation:  39\n",
      "Generation:  40\n",
      "Generation:  41\n",
      "Generation:  42\n",
      "Generation:  43\n",
      "Generation:  44\n",
      "Generation:  45\n",
      "Generation:  46\n",
      "Generation:  47\n",
      "Generation:  48\n",
      "Generation:  49\n",
      "Generation:  50\n",
      "Generation:  51\n",
      "Generation:  52\n",
      "Generation:  53\n",
      "Generation:  54\n",
      "Generation:  55\n",
      "Generation:  56\n",
      "Generation:  57\n",
      "Generation:  58\n",
      "Generation:  59\n",
      "Generation:  60\n",
      "Generation:  61\n",
      "Generation:  62\n",
      "Generation:  63\n",
      "Generation:  64\n",
      "Generation:  65\n",
      "Generation:  66\n",
      "Generation:  67\n",
      "Generation:  68\n",
      "Generation:  69\n",
      "Generation:  70\n",
      "Generation:  71\n",
      "Generation:  72\n",
      "Generation:  73\n",
      "Generation:  74\n",
      "Generation:  75\n",
      "Generation:  76\n",
      "Generation:  77\n",
      "Generation:  78\n",
      "Generation:  79\n",
      "Generation:  80\n",
      "Generation:  81\n",
      "Generation:  82\n",
      "Generation:  83\n",
      "Generation:  84\n",
      "Generation:  85\n",
      "Generation:  86\n",
      "Generation:  87\n",
      "Generation:  88\n",
      "Generation:  89\n",
      "Generation:  90\n",
      "Generation:  91\n",
      "Generation:  92\n",
      "Generation:  93\n",
      "Generation:  94\n",
      "Generation:  95\n",
      "Generation:  96\n",
      "Generation:  97\n",
      "Generation:  98\n",
      "Generation:  99\n",
      "Generation:  100\n",
      "Generation:  101\n",
      "Generation:  102\n",
      "Generation:  103\n",
      "Generation:  104\n",
      "Generation:  105\n",
      "Generation:  106\n",
      "Generation:  107\n",
      "Generation:  108\n",
      "Generation:  109\n",
      "Generation:  110\n",
      "Generation:  111\n",
      "Generation:  112\n",
      "Generation:  113\n",
      "Generation:  114\n",
      "Generation:  115\n",
      "Generation:  116\n",
      "Generation:  117\n",
      "Generation:  118\n",
      "Generation:  119\n",
      "Generation:  120\n",
      "Generation:  121\n",
      "Generation:  122\n",
      "Generation:  123\n",
      "Generation:  124\n",
      "Generation:  125\n",
      "Generation:  126\n",
      "Generation:  127\n",
      "Generation:  128\n",
      "Generation:  129\n",
      "Generation:  130\n",
      "Generation:  131\n",
      "Generation:  132\n",
      "Generation:  133\n",
      "Generation:  134\n",
      "Generation:  135\n",
      "Generation:  136\n",
      "Generation:  137\n",
      "Generation:  138\n",
      "Generation:  139\n",
      "Generation:  140\n",
      "Generation:  141\n",
      "Generation:  142\n",
      "Generation:  143\n",
      "Generation:  144\n",
      "Generation:  145\n",
      "Generation:  146\n",
      "Generation:  147\n",
      "Generation:  148\n",
      "Generation:  149\n",
      "Generation:  150\n",
      "Generation:  151\n",
      "Generation:  152\n",
      "Generation:  153\n",
      "Generation:  154\n",
      "Generation:  155\n",
      "Generation:  156\n",
      "Generation:  157\n",
      "Generation:  158\n",
      "Generation:  159\n",
      "Generation:  160\n",
      "Generation:  161\n",
      "Generation:  162\n",
      "Generation:  163\n",
      "Generation:  164\n",
      "Generation:  165\n",
      "Generation:  166\n",
      "Generation:  167\n",
      "Generation:  168\n",
      "Generation:  169\n",
      "Generation:  170\n",
      "Generation:  171\n",
      "Generation:  172\n",
      "Generation:  173\n",
      "Generation:  174\n",
      "Generation:  175\n",
      "Generation:  176\n",
      "Generation:  177\n",
      "Generation:  178\n",
      "Generation:  179\n",
      "Generation:  180\n",
      "Generation:  181\n",
      "Generation:  182\n",
      "Generation:  183\n",
      "Generation:  184\n",
      "Generation:  185\n",
      "Generation:  186\n",
      "Generation:  187\n",
      "Generation:  188\n",
      "Generation:  189\n",
      "Generation:  190\n",
      "Generation:  191\n",
      "Generation:  192\n",
      "Generation:  193\n",
      "Generation:  194\n",
      "Generation:  195\n",
      "Generation:  196\n",
      "Generation:  197\n",
      "Generation:  198\n",
      "Generation:  199\n",
      "Generation:  200\n",
      "Generation:  201\n",
      "Generation:  202\n",
      "Generation:  203\n",
      "Generation:  204\n",
      "Generation:  205\n",
      "Generation:  206\n",
      "Generation:  207\n",
      "Generation:  208\n",
      "Generation:  209\n",
      "Generation:  210\n",
      "Generation:  211\n",
      "Generation:  212\n",
      "Generation:  213\n",
      "Generation:  214\n",
      "Generation:  215\n",
      "Generation:  216\n",
      "Generation:  217\n",
      "Generation:  218\n",
      "Generation:  219\n",
      "Generation:  220\n",
      "Generation:  221\n",
      "Generation:  222\n",
      "Generation:  223\n",
      "Generation:  224\n",
      "Generation:  225\n",
      "Generation:  226\n",
      "Generation:  227\n",
      "Generation:  228\n",
      "Generation:  229\n",
      "Generation:  230\n",
      "Generation:  231\n",
      "Generation:  232\n",
      "Generation:  233\n",
      "Generation:  234\n",
      "Generation:  235\n",
      "Generation:  236\n",
      "Generation:  237\n",
      "Generation:  238\n",
      "Generation:  239\n",
      "Generation:  240\n",
      "Generation:  241\n",
      "Generation:  242\n",
      "Generation:  243\n",
      "Generation:  244\n",
      "Generation:  245\n",
      "Generation:  246\n",
      "Generation:  247\n",
      "Generation:  248\n",
      "Generation:  249\n",
      "Generation:  250\n",
      "Generation:  251\n",
      "Generation:  252\n",
      "Generation:  253\n",
      "Generation:  254\n",
      "Generation:  255\n",
      "Generation:  256\n",
      "Generation:  257\n",
      "Generation:  258\n",
      "Generation:  259\n",
      "Generation:  260\n",
      "Generation:  261\n",
      "Generation:  262\n",
      "Generation:  263\n",
      "Generation:  264\n",
      "Generation:  265\n",
      "Generation:  266\n",
      "Generation:  267\n",
      "Generation:  268\n",
      "Generation:  269\n",
      "Generation:  270\n",
      "Generation:  271\n",
      "Generation:  272\n",
      "Generation:  273\n",
      "Generation:  274\n",
      "Generation:  275\n",
      "Generation:  276\n",
      "Generation:  277\n",
      "Generation:  278\n",
      "Generation:  279\n",
      "Generation:  280\n",
      "Generation:  281\n",
      "Generation:  282\n",
      "Generation:  283\n",
      "Generation:  284\n",
      "Generation:  285\n",
      "Generation:  286\n",
      "Generation:  287\n",
      "Generation:  288\n",
      "Generation:  289\n",
      "Generation:  290\n",
      "Generation:  291\n",
      "Generation:  292\n",
      "Generation:  293\n",
      "Generation:  294\n",
      "Generation:  295\n",
      "Generation:  296\n",
      "Generation:  297\n",
      "Generation:  298\n",
      "Generation:  299\n",
      "Generation:  300\n",
      "Generation:  301\n",
      "Generation:  302\n",
      "Generation:  303\n",
      "Generation:  304\n",
      "Generation:  305\n",
      "Generation:  306\n",
      "Generation:  307\n",
      "Generation:  308\n",
      "Generation:  309\n",
      "Generation:  310\n",
      "Generation:  311\n",
      "Generation:  312\n",
      "Generation:  313\n",
      "Generation:  314\n",
      "Generation:  315\n",
      "[39841.211378560154, 1111125486.7777777, 1.0, 0.0, 1587322124.1111112, 0.0, 10000000000, 0.0, -1.0, 1.0, 0.5, 129381.0, -0.458830969668875, 0.923160605348127, 0.340263786714378, -0.72187523561816, 1.11292310431438, -0.453141020000037, 0.919113057351139, 0.0809933852947428, -0.605347603930305, -1.04305059766587, 0.977978660336892, 0.0167593613431975, -1.07180321952029, -0.535926660692509, -0.737109550710679, 0.394020150407065, 0.343595064263939, 0.320230469699542, 0.163329956975936, -0.0843496696985109, -0.230361534236792, -0.760728910660636, -0.077206165757377, 0.588875925123924, -0.276220946928237, 0.151638762285174, 0.0490885304878424, 0.144508414227893, 1.98]\n",
      "Or(11,0,0)\n",
      "Sqr(5,4,5)\n",
      "Exp(11,8,6)\n",
      "Copy(9,2,2)\n",
      "Not(2,9,4)\n",
      "Eq(11,8,3)\n",
      "Mean(0,8,1)\n",
      "Mean(6,0,4)\n",
      "Sqrt(4,11,0)\n",
      "\n",
      "Generation:  316\n",
      "Generation:  317\n",
      "Generation:  318\n",
      "Generation:  319\n",
      "Generation:  320\n",
      "Generation:  321\n",
      "Generation:  322\n",
      "Generation:  323\n",
      "Generation:  324\n",
      "Generation:  325\n",
      "Generation:  326\n",
      "Generation:  327\n",
      "Generation:  328\n",
      "Generation:  329\n",
      "Generation:  330\n",
      "Generation:  331\n",
      "Generation:  332\n",
      "Generation:  333\n",
      "Generation:  334\n",
      "Generation:  335\n",
      "Generation:  336\n",
      "Generation:  337\n",
      "Generation:  338\n",
      "Generation:  339\n",
      "Generation:  340\n",
      "Generation:  341\n",
      "Generation:  342\n",
      "Generation:  343\n",
      "Generation:  344\n",
      "Generation:  345\n",
      "Generation:  346\n",
      "Generation:  347\n",
      "Generation:  348\n",
      "Generation:  349\n",
      "Generation:  350\n",
      "Generation:  351\n",
      "Generation:  352\n",
      "Generation:  353\n",
      "Generation:  354\n",
      "Generation:  355\n",
      "Generation:  356\n",
      "Generation:  357\n",
      "Generation:  358\n",
      "Generation:  359\n",
      "Generation:  360\n",
      "Generation:  361\n",
      "Generation:  362\n",
      "Generation:  363\n",
      "Generation:  364\n",
      "Generation:  365\n",
      "Generation:  366\n",
      "Generation:  367\n",
      "Generation:  368\n",
      "Generation:  369\n",
      "Generation:  370\n",
      "Generation:  371\n",
      "Generation:  372\n",
      "Generation:  373\n",
      "Generation:  374\n",
      "Generation:  375\n",
      "Generation:  376\n",
      "Generation:  377\n",
      "Generation:  378\n",
      "Generation:  379\n",
      "Generation:  380\n",
      "Generation:  381\n",
      "Generation:  382\n",
      "Generation:  383\n",
      "Generation:  384\n",
      "Generation:  385\n",
      "Generation:  386\n",
      "Generation:  387\n",
      "Generation:  388\n",
      "Generation:  389\n",
      "Generation:  390\n",
      "Generation:  391\n",
      "Generation:  392\n",
      "Generation:  393\n",
      "Generation:  394\n",
      "Generation:  395\n",
      "Generation:  396\n",
      "Generation:  397\n",
      "Generation:  398\n",
      "Generation:  399\n",
      "Generation:  400\n",
      "Generation:  401\n",
      "Generation:  402\n",
      "Generation:  403\n",
      "Generation:  404\n",
      "Generation:  405\n",
      "Generation:  406\n",
      "Generation:  407\n",
      "Generation:  408\n",
      "Generation:  409\n",
      "Generation:  410\n",
      "Generation:  411\n",
      "Generation:  412\n",
      "Generation:  413\n",
      "Generation:  414\n",
      "Generation:  415\n",
      "Generation:  416\n",
      "Generation:  417\n",
      "Generation:  418\n",
      "Generation:  419\n",
      "Generation:  420\n",
      "Generation:  421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  422\n",
      "Generation:  423\n",
      "Generation:  424\n",
      "Generation:  425\n",
      "Generation:  426\n",
      "Generation:  427\n",
      "Generation:  428\n",
      "Generation:  429\n",
      "Generation:  430\n",
      "Generation:  431\n",
      "Generation:  432\n",
      "Generation:  433\n",
      "Generation:  434\n",
      "Generation:  435\n",
      "Generation:  436\n",
      "Generation:  437\n",
      "Generation:  438\n",
      "Generation:  439\n",
      "Generation:  440\n",
      "Generation:  441\n",
      "Generation:  442\n",
      "Generation:  443\n",
      "Generation:  444\n",
      "Generation:  445\n",
      "Generation:  446\n",
      "Generation:  447\n",
      "Generation:  448\n",
      "Generation:  449\n",
      "Generation:  450\n",
      "Generation:  451\n",
      "Generation:  452\n",
      "Generation:  453\n",
      "Generation:  454\n",
      "Generation:  455\n",
      "Generation:  456\n",
      "Generation:  457\n",
      "Generation:  458\n",
      "Generation:  459\n",
      "Generation:  460\n",
      "Generation:  461\n",
      "Generation:  462\n",
      "Generation:  463\n",
      "Generation:  464\n",
      "Generation:  465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp/ipykernel_27012/274406855.py:100: RuntimeWarning: overflow encountered in double_scalars\n",
      "  ans = (self._saturate(register_set[self.op1]**2), self.dest)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  466\n",
      "Generation:  467\n",
      "Generation:  468\n",
      "Generation:  469\n",
      "Generation:  470\n",
      "Generation:  471\n",
      "Generation:  472\n",
      "Generation:  473\n",
      "Generation:  474\n",
      "Generation:  475\n",
      "Generation:  476\n",
      "Generation:  477\n",
      "Generation:  478\n",
      "Generation:  479\n",
      "Generation:  480\n",
      "Generation:  481\n",
      "Generation:  482\n",
      "Generation:  483\n",
      "Generation:  484\n",
      "Generation:  485\n",
      "Generation:  486\n",
      "Generation:  487\n",
      "Generation:  488\n",
      "Generation:  489\n",
      "Generation:  490\n",
      "Generation:  491\n",
      "Generation:  492\n",
      "Generation:  493\n",
      "Generation:  494\n",
      "Generation:  495\n",
      "Generation:  496\n",
      "Generation:  497\n",
      "Generation:  498\n",
      "Generation:  499\n",
      "Generation:  500\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_cv_pr_curve(LinearGPClassifier(), StratifiedKFold(n_splits=5) , X, y, title='PR Curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6cea39-39c2-4a52-81fa-9b452855a850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
